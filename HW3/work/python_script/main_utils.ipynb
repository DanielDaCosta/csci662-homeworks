{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldacosta/miniconda3/envs/hack/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "import argparse\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(example):\n",
    "    \n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "    \n",
    "    # Design and implement the transformation as mentioned in pdf\n",
    "    # You are free to implement any transformation but the comments at the top roughly describe\n",
    "    # how you could implement two of them --- synonym replacement and typos.\n",
    "    \n",
    "    # You should update example[\"text\"] using your transformation\n",
    "        \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE ######\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the dataset is saved\n",
    "load_directory = \"dataset/\"\n",
    "\n",
    "# Load the dataset from the specified directory\n",
    "loaded_data = load_from_disk(load_directory)\n",
    "\n",
    "# Now you can access the individual splits (train, test, unsupervised) as follows:\n",
    "train_dataset = loaded_data[\"train\"]\n",
    "test_dataset = loaded_data[\"test\"]\n",
    "unsupervised_dataset = loaded_data[\"unsupervised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement probability\n",
    "probability = 0.6\n",
    "\n",
    "# Generate a random number between 0 and 1\n",
    "random_number = random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test_dataset[15]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now, I LOVE Italian horror films. The cheesier they are, the better. However, this is not cheesy Italian. This is week-old spaghetti sauce with rotting meatballs. It is amateur hour on every level. There is no suspense, no horror, with just a few drops of blood scattered around to remind you that you are in fact watching a horror film. The \"special effects\" consist of the lights changing to red whenever the ghost (or whatever it was supposed to be) is around, and a string pulling bed sheets up and down. Oooh, can you feel the chills? The DVD quality is that of a VHS transfer (which actually helps the film more than hurts it). The dubbing is below even the lowest \"bad Italian movie\" standards and I gave it one star just because the dialogue is so hilarious! And what do we discover when she finally DOES look in the attic (in a scene that is daytime one minute and night the next)...well, I won\\'t spoil it for anyone who really wants to see, but let\\'s just say that it isn\\'t very \"novel\"!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag): # Convert POS-Tag from treebank_tag to Wordnet tags\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None  # If no match is found, return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_synonym(text: str, pct_synonyms: float = 0.4) -> str:\n",
    "    \"\"\"Replace {pct_synonyms}% of words in a sentence with their synonyms\n",
    "    :param text (str):\n",
    "    :param pct_synonyms (float): percentage of words to be replaced. Between 0 and 1\n",
    "    :return: transformed text\n",
    "    \"\"\"\n",
    "    # Tokenize sentence\n",
    "    word_list  = word_tokenize(text)\n",
    "    n_tokens = len(word_list)\n",
    "    n_synonyms = int(n_tokens*pct_synonyms)\n",
    "\n",
    "    # Perform POS tagging to help selecting synonym\n",
    "    pos_tags = pos_tag(word_list)\n",
    "    syn_dict = {}\n",
    "    for i in range(n_synonyms):\n",
    "        chosen_word = random.choice(word_list)\n",
    "        synonyms = wn.synsets(chosen_word)\n",
    "\n",
    "        # If chosen word doesn't have a synonym\n",
    "        # retry until finding a word that does.\n",
    "        max_retry = len(word_list)\n",
    "        retry = 0 \n",
    "        while (len(synonyms) == 0 and retry <= max_retry):\n",
    "            chosen_word = random.choice(word_list)\n",
    "            synonyms = wn.synsets(chosen_word)\n",
    "\n",
    "            if len(synonyms) > 0:\n",
    "                # Print the word and its POS tag\n",
    "                chosen_word_tag = [tag for word, tag in pos_tags if word == chosen_word][0]\n",
    "                chosen_word_wordnet_tag = get_wordnet_pos(chosen_word_tag)\n",
    "                \n",
    "                selected_synonym = chosen_word\n",
    "                for syn in synonyms:\n",
    "                    # Only consider synonyms that have the same POS_TAG\n",
    "                    if syn.pos() == chosen_word_wordnet_tag:\n",
    "                        for lemma in syn.lemmas():\n",
    "                            if (lemma.name() != chosen_word):\n",
    "                                # Select the first synonym that appears\n",
    "                                selected_synonym = lemma.name().replace('_', ' ') # Replace underscores for multi-word synonyms\n",
    "                                break\n",
    "                        break\n",
    "                    break\n",
    "                if selected_synonym == chosen_word:\n",
    "                    synonyms = [] # continue while loop\n",
    "                else:\n",
    "                    # print(\"selected_synonym: \", selected_synonym)\n",
    "                    syn_dict[chosen_word] = selected_synonym\n",
    "\n",
    "    # return syn_dict\n",
    "    # Replace selected words\n",
    "    for old, new in syn_dict.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      " I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "Synonym\n",
      " I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I truly did, but it is to good TV sci-fi as Babylon 5 is to star trek (the master). Silly prosthetics, inexpensive cardboard sets, stilted dialogue, CG that doesn't match the background, and painfully unidimensional fictional fictional character cannon be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's non. It's clichéd and uninspiring.) While US viewers might like emotion and fictional character development, sci-fi is a genre that does non take itself earnestly (californium. star trek). It may treat important issues, yet non as a serious philosophy. It's truly difficult to care about the fictional fictional character here as they are non merely foolish, just miss a spark of life. Their action and reaction are wooden and predictable, frequently painful to watch. The maker of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would non go on watching. Roddenberry's ash must be turning in their orbit as this dull, inexpensive, poorly edited (watching it without advert breaks truly bring this home) trudging Trabant of a show lumbers into infinite. Spoiler. sol, kill off a main fictional character. And then bring him back as anonher actor. Jeeez! Dallas all over again.\n",
      "{'simply': 'merely', 'ashes': 'ash', 'not': 'non', 'characters': 'fictional character', 'makers': 'maker', 'space': 'infinite', 'one-dimensional': 'unidimensional', 'actions': 'action', 'missing': 'miss', 'Star': 'star', 'dialogues': 'dialogue', 'Trek': 'trek', 'really': 'truly', 'seriously': 'earnestly', 'cheap': 'inexpensive', 'continue': 'go on', 'original': 'master', 'often': 'frequently', 'character': 'fictional character', 'cf': 'californium', 'brings': 'bring', 'So': 'sol'}\n",
      "Original\n",
      " Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\n",
      "Synonym\n",
      " Worth the entertainment value of a rental, especially if you like action films. Thbe one features the usual auto chases, battles with the great Van Damme boot style, shooting battles with the 40 shell loading scattergun, and even terrorbet style bombs. All of thbe be entertaining and competently manage but there be nothing that truly blows you away if you've seen your portion before.<bromine /><bromine />The secret plan be made interesting by the inclusion of a coney, which be clever but barely profound. Many of the characters are to a great extent stereotyped -- the angry veterans, the terrified illegal foreigner, the crooked bull, the indifferent feds, the bitchy tough lady station caput, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood film from the 1940s. All reasonably act but again nothing special.<bromine /><bromine />I thought the main villains be pretty well done and reasonably well act. By the end of the film you certainly know who the good guys be and ben't. There was an emotional lift as the truly bad one get their just deserts. Very simplbetic, but then you ben't expecting hamlet, right? The only thing I found truly annoying was the constant cuts to VDs daughter during the last battle scene.<bromine /><bromine />not bad. not good. passable 4.\n",
      "{'fight': 'battle', 'share': 'portion', 'Passable': 'passable', 'hardly': 'barely', 'car': 'auto', 'got': 'get', 'br': 'bromine', 'movie': 'film', 'really': 'truly', 'plot': 'secret plan', 'Not': 'not', 'fairly': 'reasonably', 'load': 'loading', 'handled': 'manage', 'were': 'be', 'shotgun': 'scattergun', 'Hamlet': 'hamlet', 'head': 'caput', 'passably': 'reasonably', 'is': 'be', 'kick': 'boot', 'aliens': 'foreigner', 'cops': 'bull', 'acted': 'act', 'heavily': 'to a great extent', 'ones': 'one', 'knew': 'know', 'rabbit': 'coney'}\n",
      "Original\n",
      " its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\n",
      "Synonym\n",
      " its a totally average movie with a few semi-alright action sequence that make the secret plan look a little better and remind the viewer of the classic avant-garde dike movies. parts of the secret plan don't make sense and look to be added in to use up time. the terminal secret plan be that of a very basic type that doesn't leave the viewer guessing and any twbets are obvious from the beginning. the terminal scene with the flask back don't make sense as they are added in and look to have little releavant-gardece to the hbetory of avant-garde dike's character. not really worth watching once again, bit dbeappointed in the terminal production, even though it be apparent it was shot on a low budget certain shooting and sections in the movie are of poor directed quality\n",
      "{'plot': 'secret plan', 'again': 'once again', 'shots': 'shooting', 'is': 'be', 'end': 'terminal', 'dam': 'dike', 'seem': 'look', 'backs': 'back', 'sequences': 'sequence', 'film': 'movie', 'twists': 'turn', 'van': 'avant-garde'}\n",
      "Original\n",
      " STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\n",
      "Synonym\n",
      " STangstromR evaluation: ***** Sabbatum night **** Fri night *** Fri morning ** Sunday night * Monday morning <bromine /><bromine />Former New Orleans homicide cop Jack Robideaux (Jean Claude avant-garde Damme) be re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Sir Leslie Stephen Lord, otherwbee known as Jase from East Enders) who be using a special method he learned in angstromfghanbetan to fight off hbe opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk amanner alive.<bromine /><bromine />angstromfter Until death, avant-garde Damme appeared to be on a high, showing he could make the best straight to video movies in the action market. While that was a far more drama orient movie, with The shepherd he has return to the high-kicking, no bromineainer action that first made him famous and has unhappily produce hbe worst movie since derail. It's nowhere near as bad as that movie, but what I said still stands.<bromine /><bromine />angstrom dull, predictable movie, with very little in the manner of any exciting action. What little there be mainly consbets of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that unhappily instead make them look more desperate. Being a Mexican set movie, director Isaac Florentine has tried to give the movie a Henry M. Robert Rodriguez/Desperado sort of feel, but thbe only adds to the desperation.<bromine /><bromine />VD gives a particularly uninspired public presentation and given he's never be a Henry M. Robert De Niro sort of histrion, that can't be good. angstroms the scoundrel, Lord shouldn't anticipate to leave the beeb anytime shortly. He gets little dialogue at the beginning as he struggles to muster an angstrommerican accent but gets cryptically better towards the end. angstromll the supporting cast are equally bland, and do nil to rabee the movies spirits at all.<bromine /><bromine />Thbe be one shepherd that's roll right from the flock. *\n",
      "{'film': 'movie', 'sadly': 'unhappily', 'Morning': 'morning', 'is': 'be', 'been': 'be', 'returned': 'return', 'Friday': 'Fri', 'villain': 'scoundrel', 'films': 'movie', 'mysteriously': 'cryptically', 'Stephen': 'Sir Leslie Stephen', 'Saturday': 'Sabbatum', 'br': 'bromine', 'Robert': 'Henry M. Robert', 'Night': 'night', 'expect': 'anticipate', 'Van': 'avant-garde', 'produced': 'produce', 'Afghanistan': 'Islamic State of Afghanistan', 'RATING': 'evaluation', 'A': 'angstrom', 'soon': 'shortly', 'Shepherd': 'shepherd', 'way': 'manner', 'oriented': 'orient', 'Derailed': 'derail', 'performance': 'public presentation', 'strayed': 'roll', 'Death': 'death', 'actor': 'histrion', 'nothing': 'nil'}\n",
      "Original\n",
      " First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n",
      "Synonym\n",
      " First off let me say, If you haven't enjoyed a avant-garde Damme film since bloodsport, you probably will non like thbe film. most of these films may non have the best plots or best actor but I enjoy these kind of films for what they are. Thbe film be much better than any of the films the other action guys (Segal and Dolph) have thought about putting out the past few years. avant-garde Damme be good in the film, the film be only worth watching to avant-garde Damme fan. It be non as good as aftermath of death (which i extremely recommend to anyone of likes avant-garde Damme) or In hell on earth but, in my opinion it's worth watching. It has the same type of feel to it as nowhere to Run. Good merriment stuff!\n",
      "{'fans': 'fan', 'hell': 'hell on earth', 'fun': 'merriment', 'movie': 'film', 'Most': 'most', 'Van': 'avant-garde', 'is': 'be', 'Death': 'death', 'kinds': 'kind', 'actors': 'actor', 'not': 'non', 'highly': 'extremely', 'Nowhere': 'nowhere', 'Wake': 'aftermath'}\n",
      "Original\n",
      " I had high hopes for this one until they changed the name to 'The Shepherd : Border Patrol, the lamest movie name ever, what was wrong with just 'The Shepherd'. This is a by the numbers action flick that tips its hat at many classic Van Damme films. There is a nice bit of action in a bar which reminded me of hard target and universal soldier but directed with no intensity or flair which is a shame. There is one great line about 'being p*ss drunk and carrying a rabbit' and some OK action scenes let down by the cheapness of it all. A lot of the times the dialogue doesn't match the characters mouth and the stunt men fall down dead a split second before even being shot. The end fight is one of the better Van Damme fights except the Director tries to go a bit too John Woo and fails also introducing flashbacks which no one really cares about just gets in the way of the action which is the whole point of a van Damme film.<br /><br />Not good, not bad, just average generic action.\n",
      "Synonym\n",
      " I had high hopes for thbe one until they change the name to 'The shepherd : Border patrol, the lamest movie name ever, what was wrong with just 'The shepherd'. Thbe be a by the Numbers action flick that tips its hat at many classic avant-garde Damme films. There be a nice spot of action in a bar which reminded me of hard target and universal soldier but direct with no intensity or flair which be a shame. There be one great line about 'being p*ss drunkard and carrying a rabspot' and some OK action scenes let down by the cheapness of it all. A lot of the times the dialogue doesn't match the fictional character mouth and the stunt work force fall down dead a split second before even being shot. The terminal battle be one of the better avant-garde Damme battles except the Director tries to go a spot too John Woo and fails besides introducing flashback which no one really cares about just gets in the way of the action which be the whole point of a van Damme film.<br /><br />not good, not bad, just average generic action.\n",
      "{'drunk': 'drunkard', 'flashbacks': 'flashback', 'numbers': 'Numbers', 'Van': 'avant-garde', 'directed': 'direct', 'Not': 'not', 'Shepherd': 'shepherd', 'bit': 'spot', 'end': 'terminal', 'changed': 'change', 'is': 'be', 'also': 'besides', 'men': 'work force', 'fight': 'battle', 'characters': 'fictional character', 'Patrol': 'patrol'}\n",
      "Original\n",
      " Isaac Florentine has made some of the best western Martial Arts action movies ever produced. In particular US Seals 2, Cold Harvest, Special Forces and Undisputed 2 are all action classics. You can tell Isaac has a real passion for the genre and his films are always eventful, creative and sharp affairs, with some of the best fight sequences an action fan could hope for. In particular he has found a muse with Scott Adkins, as talented an actor and action performer as you could hope for. This is borne out with Special Forces and Undisputed 2, but unfortunately The Shepherd just doesn't live up to their abilities.<br /><br />There is no doubt that JCVD looks better here fight-wise than he has done in years, especially in the fight he has (for pretty much no reason) in a prison cell, and in the final showdown with Scott, but look in his eyes. JCVD seems to be dead inside. There's nothing in his eyes at all. It's like he just doesn't care about anything throughout the whole film. And this is the leading man.<br /><br />There are other dodgy aspects to the film, script-wise and visually, but the main problem is that you are utterly unable to empathise with the hero of the film. A genuine shame as I know we all wanted this film to be as special as it genuinely could have been. There are some good bits, mostly the action scenes themselves. This film had a terrific director and action choreographer, and an awesome opponent for JCVD to face down. This could have been the one to bring the veteran action star back up to scratch in the balls-out action movie stakes.<br /><br />Sincerely a shame that this didn't happen.\n",
      "Synonym\n",
      " Isaac Florentine has made some of the best western Martial humanistic discipline action movies ever produce. In particular US Seals 2, Cold crop, Special force and Undbeputed 2 are all action classics. You can tell Isaac has a real passionateness for the genre and hbe movies are always eventful, creative and sharp personal business, with some of the best battle sequence an action fan could hope for. In particular he has found a Muse with George C. Scott Adkins, as talented an actor and action performing artist as you could hope for. Thbe be borne out with Special force and Undbeputed 2, but unfortunately The Shepherd just doesn't live up to their abilities.<bromine /><bromine />There be no doubt that JCVD expressions better here battle-wbee than he has done in old age, particularly in the battle he has (for pretty much no reason) in a prbeon house cell, and in the final showdown with George C. Scott, but expression in hbe eyes. JCVD seems to be dead inside. There's nothing in hbe eyes at all. It's like he just doesn't care about anything throughout the whole movie. And thbe be the leading man.<bromine /><bromine />There are other dodgy aspect to the movie, script-wbee and vbeually, but the main problem be that you are absolutely unable to empathbee with the hero of the movie. A echt shame as I know we all desire thbe movie to be as special as it echtly could have be. There are some good spot, largely the action scenes themselves. Thbe movie had a terrific manager and action choreographer, and an awesome opposition for JCVD to face down. Thbe could have be the one to bromineing the veteran action star back up to scratch in the balls-out action movie stakes.<bromine /><bromine />sincerely a shame that thbe didn't hap.\n",
      "{'prison': 'prison house', 'happen': 'hap', 'been': 'be', 'genuine': 'echt', 'fight': 'battle', 'years': 'old age', 'aspects': 'aspect', 'is': 'be', 'especially': 'particularly', 'opponent': 'opposition', 'Sincerely': 'sincerely', 'director': 'manager', 'look': 'expression', 'produced': 'produce', 'performer': 'performing artist', 'muse': 'Muse', 'utterly': 'absolutely', 'genuinely': 'truly', 'Arts': 'humanistic discipline', 'mostly': 'largely', 'Scott': 'George C. Scott', 'br': 'bromine', 'wanted': 'desire', 'film': 'movie', 'Harvest': 'crop', 'bits': 'spot', 'passion': 'passionateness', 'affairs': 'personal business', 'sequences': 'sequence', 'Forces': 'force', 'bring': 'convey'}\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "for i, data in enumerate(test_dataset):\n",
    "    sentence = data[\"text\"]\n",
    "    print(\"Original\\n\", sentence)\n",
    "    sentence_new, syn_dict = replace_with_synonym(sentence, 0.40)\n",
    "    print(\"Synonym\\n\", sentence_new)\n",
    "    print(syn_dict)\n",
    "    if i > N:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_layout = {\n",
    "    'Q': ['W', 'A', 'S'], 'W': ['Q', 'E', 'A', 'S', 'D'], 'E': ['W', 'R', 'S', 'D', 'F'], 'R': ['E', 'T', 'D', 'F', 'G'], 'T': ['R', 'Y', 'F', 'G', 'H'], \n",
    "    'Y': ['T', 'U', 'G', 'H', 'J'], 'U': ['Y', 'I', 'H', 'J', 'K'], 'I': ['U', 'O', 'J', 'K', 'L'], 'O': ['I', 'P', 'K', 'L'], 'P': ['O', 'L'],\n",
    "    'A': ['Q', 'W', 'S', 'Z', 'X'], 'S': ['Q', 'W', 'E', 'A', 'D', 'Z', 'X', 'C'], 'D': ['W', 'E', 'R', 'S', 'F', 'X', 'C', 'V'], \n",
    "    'F': ['E', 'R', 'T', 'D', 'G', 'C', 'V', 'B'], 'G': ['R', 'T', 'Y', 'F', 'H', 'V', 'B', 'N'], 'H': ['T', 'Y', 'U', 'G', 'J', 'B', 'N', 'M'], \n",
    "    'J': ['Y', 'U', 'I', 'H', 'K', 'N', 'M'], 'K': ['U', 'I', 'O', 'J', 'L', 'M'], 'L': ['I', 'O', 'P', 'K'], 'Z': ['A', 'S', 'X'], 'X': ['Z', 'A', 'S', 'D', 'C'],\n",
    "    'C': ['X', 'S', 'D', 'F', 'V'], 'V': ['C', 'D', 'F', 'G', 'B'], 'B': ['V', 'F', 'G', 'H', 'N'], 'N': ['B', 'G', 'H', 'J', 'M'], 'M': ['N', 'H', 'J', 'K'],\n",
    "    'I': ['U', 'O', 'J', 'K', 'L'], 'W': ['Q', 'E', 'A', 'S', 'D']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, len(word) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typo_qwert(word: str, index: int):\n",
    "    \"\"\"Replace words close to each other in the QWERT keyboard\n",
    "    :param word: \n",
    "    :param index: index of caracter to be replaced\n",
    "    \"\"\"\n",
    "    # \n",
    "    char = word[index]\n",
    "    if char.upper() in keyboard_layout.keys():\n",
    "        random_typo = random.choice(keyboard_layout[char.upper()]).lower()\n",
    "        word = word[:index] + random_typo + word[index+1:]\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typo_swap_characeters(word: str, index: int):\n",
    "    \"\"\"Swap character at index with index + 1\n",
    "    :param word: \n",
    "    :param index: index of caracter to be replaced\n",
    "    \"\"\"\n",
    "    char_list = list(word)\n",
    "    next_index = index + 1\n",
    "    char_list[index], char_list[next_index] = char_list[next_index], char_list[index]\n",
    "    word_swapped = \"\".join(char_list)\n",
    "    return word_swapped\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typo_remove_character(word: str, index: int):\n",
    "    \"\"\"Delete a character\n",
    "    :param word: \n",
    "    :param index: index of caracter to be replaced\n",
    "    \"\"\"\n",
    "    word = word[:index] + word[index + 1:]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typo_add_character(word: str, index: int):\n",
    "    \"\"\"Add character \n",
    "    :param word: \n",
    "    :param index: index of caracter to be replaced\n",
    "    \"\"\"\n",
    "    random_char = random.choice(string.ascii_letters)\n",
    "\n",
    "    return word[:index] + random_char + word[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_typos(text: str, pct_typos: float = 0.4, typos_list=['swap', 'remove', 'add', 'qwert']) -> str:\n",
    "    \"\"\"Replace {pct_typos}% of words in a sentence with their syntetic generated typo\n",
    "    :param text (str):\n",
    "    :param pct_typos (float): percentage of words to be modified. Between 0 and 1\n",
    "    :return: transformed text\n",
    "    \"\"\"\n",
    "    # Tokenize sentence\n",
    "    word_list  = word_tokenize(text)\n",
    "\n",
    "\n",
    "    # word_list words that have at least 3 characters\n",
    "    word_list_indices_filtered = [i for i in range(len(word_list)) if len(word_list[i]) >= 3]\n",
    "\n",
    "    n_tokens = len(word_list_indices_filtered) # only count the tokens that have >= characters\n",
    "    n_typos = int(n_tokens*pct_typos) # number of typos\n",
    "\n",
    "    # Typos functions\n",
    "    typos_functions = {\n",
    "        'swap': typo_swap_characeters,\n",
    "        'remove': typo_remove_character,\n",
    "        'add': typo_add_character,\n",
    "        'qwert': typo_qwert\n",
    "    }\n",
    "\n",
    "    # Select indices based on pct_typos\n",
    "    word_indices = [index for index in range(len(word_list)) if index in word_list_indices_filtered]\n",
    "    selected_indices = random.sample(word_indices, k = n_typos)\n",
    "\n",
    "    # Transformed words\n",
    "    transformed_words = {}\n",
    "    for index in selected_indices:\n",
    "\n",
    "        word = word_list[index]\n",
    "        typo_index= random.randint(0, len(word) - 2)\n",
    "        # Randomly select typo technique\n",
    "        selected_typo = random.choice(typos_list)\n",
    "\n",
    "        # Apply typo function\n",
    "        word_transformed = typos_functions[selected_typo](word, typo_index)\n",
    "        transformed_words[index] = word_transformed\n",
    "\n",
    "    \n",
    "    # Replace orginal words with sythetic generated typos\n",
    "    for i, new_word in transformed_words.items():\n",
    "        word_list[i] = new_word\n",
    "\n",
    "    # Put sentence together again:\n",
    "    text = TreebankWordDetokenizer().detokenize(word_list)\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I qlove sci-fi and am willing to pMut up wjth a lot . Si-fi movies/TV xre usualy underfunded, under-appreciated and misuderstood . I tridd to liDke ths, I really dd, but it is to good TV sci-fi as BPabylon 5 is to tar Tek (the Soriginal). SZilly prostheMtics, ceap cardboMard sets, stilted dialogues, CG tyat does nu\\'t mach the background, and painfulpy one-dimensional chaarcters cNan not be overcoem wiBth a \\'ssci-fi\\' detting . (I\\'m sjre thee are thlse of you tout there wtho tihnk Babyon 5 is good wci-fi TV . It\\'s nlt . It\\'s cliczhéd aNnd uninspiring .) Whlie US vewers Bmight like emotin and character developmet, sci-fi is a genre that does nit take tiself seriousyl (cf . Star rek). It may treat important issuse, yt not as a esrious philosophy . It\\'s really difficult to crae about hte characters hree as twhey ae not sinmply foolish, just jissing a sprak of Ulife . Teir ations adn reactjons rae wooden and predicatble, often painful to watch . hTe makmers of Ebarth KgOW it\\'s rubibsh as ghey bave to awlays aay \"Gene Roddenbery\\'s Erath ...\" otherwiJse people would unot cotinue watching . Roddenberry\\'s ashres must be turVning in tteir orbt as this qdull, heap, porly edietd (watching it ithout advert breaks rehally brings this home) trudging Traabnt of a shiw lumhers ihto spaec . Spoiler . So, kill ovf a msin charcater . nAd tehn bring im bZack as another actor . Jeeez! Dllas all ovrr agawin.'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_typos(example, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\",\n",
    "        \"won\\'t\": \"will not\",\n",
    "        \"can\\'t\": \"can not\",\n",
    "        \"n\\'t\": \" not\",\n",
    "        \"isn\\'t\": \"is not\",\n",
    "        \"\\'re\": \" are\",\n",
    "        \"\\'s\": \" is\",\n",
    "        \"\\'d\": \" would\",\n",
    "        \"\\'ll\": \" will\",\n",
    "        \"\\'ve\": \" have\",\n",
    "        \"\\'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(text: str, pct_swap: float = 0.2) -> str:\n",
    "    \"\"\"Swap {pct_swap}% of words in a sentence with a random chosen word\n",
    "    :param text (str):\n",
    "    :param pct_swap (float): percentage of words to be swapped. Between 0 and 1\n",
    "    :return: transformed text\n",
    "    \"\"\"\n",
    "    # Tokenize sentence\n",
    "    word_list  = word_tokenize(text)\n",
    "\n",
    "    n_tokens = len(word_list)\n",
    "    n_swaps = int(n_tokens*pct_swap)\n",
    "    for i in range(n_swaps):\n",
    "        # Get indices\n",
    "        index_1, index_2 = random.sample(range(len(word_list)), 2)\n",
    "        # Swap words\n",
    "        word_list[index_1], word_list[index_2] = word_list[index_2], word_list[index_1]\n",
    "\n",
    "\n",
    "    # Put sentence together again:\n",
    "    text = TreebankWordDetokenizer().detokenize(word_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, I LOVE Italian horror films. The cheesier they are, the better. However, this is not cheesy Italian. This is week-old spaghetti sauce with rotting meatballs. It is amateur hour on every level. There is no suspense, no horror, with just a few drops of blood scattered around to remind you that you are in fact watching a horror film. The \"special effects\" consist of the lights changing to red whenever the ghost (or whatever it was supposed to be) is around, and a string pulling bed sheets up and down. Oooh, can you feel the chills? The DVD quality is that of a VHS transfer (which actually helps the film more than hurts it). The dubbing is below even the lowest \"bad Italian movie\" standards and I gave it one star just because the dialogue is so hilarious! And what do we discover when she finally DOES look in the attic (in a scene that is daytime one minute and night the next)...well, I won't spoil it for anyone who really wants to see, but let's just say that it isn't very \"novel\"!\n",
      "Now \"I And Italian's for . the cheesier with are, the the . However, this is not cheesy\". This amateur week-old spaghetti sauce they rotting minute . It whenever is hour down every look . There is no suspense, no horror, with level a few drops dubbing blood scattered around to remind you that one of in that watching a I just consist The \"Italian effects\". of the lights changing to red see say ghost VHS or (is was supposed to be) is around, and daytime string pulling bed sheets up and on . Oooh, can you feel better chills? The DVD quality is that of a whatever anyone (which horror helps the are more than the and). The film is below even the lowest \"bad Italian movie let standards and the special it film star it the hurts dialogue is so hilarious! LOVE what do we horror when she finally DOES...in The attic (in a scene that is a\" meatballs it night discover next) one gave, I won't spoil it films it who really wants to transfer, but you actually just because fact just isn't very, novel well!\n"
     ]
    }
   ],
   "source": [
    "print(example)\n",
    "print(random_swap(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(text: str, pct_deletion: float = 0.1) -> str:\n",
    "    \"\"\"Remove {pct_deletion}% of words in a sentence\n",
    "    :param text (str):\n",
    "    :param pct_deletion (float): percentage of words to be deleted. Between 0 and 1\n",
    "    :return: transformed text\n",
    "    \"\"\"\n",
    "    # Tokenize sentence\n",
    "    word_list  = word_tokenize(text)\n",
    "\n",
    "    n_tokens = len(word_list)\n",
    "    n_deletion = int(n_tokens*pct_deletion)\n",
    "\n",
    "    indexes_deleted = random.sample(range(len(word_list)), n_deletion)\n",
    "    for index in sorted(indexes_deleted, reverse=True):\n",
    "        word_list.pop(index)\n",
    "\n",
    "\n",
    "    # Put sentence together again:\n",
    "    text = TreebankWordDetokenizer().detokenize(word_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(example):\n",
    "    \n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "    \n",
    "    # Design and implement the transformation as mentioned in pdf\n",
    "    # You are free to implement any transformation but the comments at the top roughly describe\n",
    "    # how you could implement two of them --- synonym replacement and typos.\n",
    "    \n",
    "    # You should update example[\"text\"] using your transformation\n",
    "    \n",
    "    # Select transformation\n",
    "    selected_transformation = random.choice([\"typo\", \"synonym\", \"swap\", \"deletion\"])\n",
    "\n",
    "    text = example[\"text\"]\n",
    "    if selected_transformation == \"typo\":\n",
    "        text = add_typos(text)\n",
    "    elif selected_transformation == \"synonym\":\n",
    "        text = replace_with_synonym(text)\n",
    "    elif selected_transformation == \"swap\":\n",
    "        text = random_swap(text)\n",
    "    else:\n",
    "        text = random_deletion(text)\n",
    "\n",
    "    # Apply contractions to all of them\n",
    "    text = expand_contradictions(text)\n",
    "    example[\"text\"] = text\n",
    "    ##### YOUR CODE ENDS HERE ######\n",
    "\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [01:03<00:00, 394.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = dataset[\"test\"].map(custom_transform, load_from_cache_file=False)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Augmented Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmented_size = 5000\n",
    "train_transformed_sample = dataset[\"train\"].shuffle(seed=42).select(range(train_augmented_size))\n",
    "train_transformed_sample = train_transformed_sample.map(custom_transform, load_from_cache_file=False)                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldacosta/miniconda3/envs/hack/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "dataset['train'] = concatenate_datasets([dataset[\"train\"], train_transformed_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "global tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30000/30000 [00:05<00:00, 5114.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare dataset for use by model\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Create dataloaders for iterating over the dataset\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create dataloaders for iterating over the dataset\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed_dataset = concatenate_datasets([dataset[\"train\"], train_transformed_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 35000\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a dataladoer for the augmented training dataset\n",
    "def create_augmented_dataloader(dataset):\n",
    "    \n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "    \n",
    "    # Here, 'dataset' is the original dataset. You should return a dataloader called 'train_dataloader' (with batch size = 8) -- this\n",
    "    # dataloader will be for the original training split augmented with 5k random transformed examples from the training set.\n",
    "    # You may want to set load_from_cache_file to False when using dataset maps\n",
    "    # You may find it helpful to see how the dataloader was created at other place in this code.\n",
    "\n",
    "    # 5000 randomly transformed examples\n",
    "    train_augmented_size = 5000\n",
    "    train_transformed_sample = dataset[\"train\"].shuffle(seed=42).select(range(train_augmented_size))\n",
    "    train_transformed_sample = train_transformed_sample.map(custom_transform, load_from_cache_file=False) \n",
    "\n",
    "    # Augment the training data with 5000 randomly transformed examples to create the new augmented training dataset\n",
    "    # Final dataset train size: \"25,000\" + \"5,000\" = \"30,000\" \n",
    "    train_transformed_dataset = concatenate_datasets([dataset[\"train\"], train_transformed_sample])                                                \n",
    "    \n",
    "    train_dataloader = None\n",
    "\n",
    "    tokenized_dataset = train_transformed_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Prepare dataset for use by model\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    # Create dataloaders for iterating over the dataset\n",
    "    train_dataloader = DataLoader(tokenized_dataset, shuffle=True, batch_size=8)\n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE ######\n",
    "    \n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:21<00:00, 236.76 examples/s]\n",
      "/Users/danieldacosta/miniconda3/envs/hack/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Map: 100%|██████████| 30000/30000 [00:05<00:00, 5074.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = create_augmented_dataloader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:21<00:00, 228.69 examples/s]\n",
      "/Users/danieldacosta/miniconda3/envs/hack/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "train_augmented_size = 5000\n",
    "train_transformed_sample = dataset[\"train\"].shuffle(seed=42).select(range(train_augmented_size))\n",
    "train_transformed_sample = train_transformed_sample.map(custom_transform, load_from_cache_file=False) \n",
    "\n",
    "# Augment the training data with 5000 randomly transformed examples to create the new augmented training dataset\n",
    "# Final dataset train size: \"25,000\" + \"5,000\" = \"30,000\" \n",
    "dataset['train'] = concatenate_datasets([dataset[\"train\"], train_transformed_sample])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30000/30000 [00:06<00:00, 4600.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare dataset for use by model\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformed_dataloader(dataset, debug_transformation):\n",
    "    \n",
    "    # Print 5 random transformed examples\n",
    "    if debug_transformation:\n",
    "        small_dataset = dataset[\"test\"].shuffle(seed=42).select(range(5))\n",
    "        small_transformed_dataset = small_dataset.map(custom_transform, load_from_cache_file=False)\n",
    "        for k in range(5):\n",
    "            print(\"Original Example \", str(k))\n",
    "            print(small_dataset[k])\n",
    "            print(\"\\n\")\n",
    "            print(\"Transformed Example \", str(k))\n",
    "            print(small_transformed_dataset[k])\n",
    "            print('='*30)\n",
    "\n",
    "        exit()\n",
    "      \n",
    "    \n",
    "    transformed_dataset = dataset[\"test\"].map(custom_transform, load_from_cache_file=False)                                                    \n",
    "    transformed_tokenized_dataset = transformed_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.remove_columns([\"text\"])\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    transformed_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    transformed_val_dataset = transformed_tokenized_dataset    \n",
    "    eval_dataloader = DataLoader(transformed_val_dataset, batch_size=8)\n",
    "    \n",
    "    return eval_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list  = word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_word = random.choice(word_list)\n",
    "synonyms = wn.synsets(chosen_word)\n",
    "\n",
    "# If chosen word doesn't have a synonym select retry until finding a word that does.\n",
    "max_retry = len(word_list)\n",
    "retry = 0 \n",
    "while (len(synonyms) == 0 and retry <= max_retry):\n",
    "    chosen_word = random.choice(word_list)\n",
    "    synonyms = wn.synsets(chosen_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(word_list)\n",
    "\n",
    "# Print the word and its POS tag\n",
    "chosen_word_tag = [tag for word, tag in pos_tags if word == chosen_word][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_list = set()\n",
    "for syn in synonyms:\n",
    "    # Only consider synonyms that have the same POS_TAG\n",
    "    if syn.pos() == chosen_word_wordnet_tag:\n",
    "        for lemma in syn.lemmas():\n",
    "            if (lemma.name() != chosen_word) & (lemma):\n",
    "\n",
    "                synonym_list.add(lemma.name().replace('_', ' ')) # Replace underscores for multi-word synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
