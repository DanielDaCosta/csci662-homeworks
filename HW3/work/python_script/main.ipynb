{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldacosta/miniconda3/envs/hack/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "import argparse\n",
    "from utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core training function\n",
    "def do_train(args, model, train_dataloader, save_dir=\"./out\"):\n",
    "\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    num_epochs = args.num_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    model.train()\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "    \n",
    "    # Implement the training loop --- make sure to use the optimizer and lr_sceduler (learning rate scheduler)\n",
    "    # Remember that pytorch uses gradient accumumlation so you need to use zero_grad (https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html)\n",
    "    # You can use progress_bar.update(1) to see the progress during training\n",
    "    # You can refer to the pytorch tutorial covered in class for reference\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            progress_bar.update(1)   \n",
    "    \n",
    "    print(\"Training completed...\")\n",
    "    print(\"Saving Model....\")\n",
    "    model.save_pretrained(save_dir)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core evaluation function\n",
    "def do_eval(eval_dataloader, output_dir, out_file):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "        # write to output file\n",
    "        for i in range(predictions.shape[0]):\n",
    "            out_file.write(str(predictions[i].item()) + \"\\n\")\n",
    "            #out_file.write(\"\\n\")\n",
    "            out_file.write(str(batch[\"labels\"][i].item()) + \"\\n\\n\")\n",
    "            #out_file.write(\"\\n\\n\")\n",
    "\n",
    "    score = metric.compute()\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# Created a dataladoer for the augmented training dataset\n",
    "def create_augmented_dataloader(dataset):\n",
    "    \n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "    \n",
    "    # Here, 'dataset' is the original dataset. You should return a dataloader called 'train_dataloader' (with batch size = 8) -- this\n",
    "    # dataloader will be for the original training split augmented with 5k random transformed examples from the training set.\n",
    "    # You may want to set load_from_cache_file to False when using dataset maps\n",
    "    # You may find it helpful to see how the dataloader was created at other place in this code.\n",
    "    \n",
    "    train_dataloader = None\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE ######\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "# Create a dataloader for the transformed test set\n",
    "def create_transformed_dataloader(dataset, debug_transformation):\n",
    "    \n",
    "    # Print 5 random transformed examples\n",
    "    if debug_transformation:\n",
    "        small_dataset = dataset[\"test\"].shuffle(seed=42).select(range(5))\n",
    "        small_transformed_dataset = small_dataset.map(custom_transform, load_from_cache_file=False)\n",
    "        for k in range(5):\n",
    "            print(\"Original Example \", str(k))\n",
    "            print(small_dataset[k])\n",
    "            print(\"\\n\")\n",
    "            print(\"Transformed Example \", str(k))\n",
    "            print(small_transformed_dataset[k])\n",
    "            print('='*30)\n",
    "\n",
    "        exit()\n",
    "      \n",
    "    \n",
    "    transformed_dataset = dataset[\"test\"].map(custom_transform, load_from_cache_file=False)                                                    \n",
    "    transformed_tokenized_dataset = transformed_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.remove_columns([\"text\"])\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    transformed_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    transformed_val_dataset = transformed_tokenized_dataset    \n",
    "    eval_dataloader = DataLoader(transformed_val_dataset, batch_size=8)\n",
    "    \n",
    "    return eval_dataloader   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 50.3kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 5.28MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 3.38MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 3.74MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.31k/4.31k [00:00<00:00, 9.54MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.17k/2.17k [00:00<00:00, 8.95MB/s]\n",
      "Downloading readme: 100%|██████████| 7.59k/7.59k [00:00<00:00, 18.8MB/s]\n",
      "Downloading data: 100%|██████████| 84.1M/84.1M [00:20<00:00, 4.12MB/s]\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:03<00:00, 8237.73 examples/s] \n",
      "Generating test split: 100%|██████████| 25000/25000 [00:03<00:00, 8295.24 examples/s] \n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:03<00:00, 15067.42 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5012.95 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5293.25 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:09<00:00, 5221.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for use by model\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 436M/436M [00:26<00:00, 16.7MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
