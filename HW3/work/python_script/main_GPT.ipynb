{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "import argparse\n",
    "from utils import *\n",
    "import os\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_content(text):\n",
    "    \"\"\"Add prompt to sentence\"\"\"\n",
    "    completion = {\"role\": \"user\", \"content\": f\"Classes: [`positive`, `negative`]\\nText: {text}\\n\\nClassify the text into one of the above classes. Only return the class.\"}\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the dataset is saved\n",
    "load_directory = \"dataset/\"\n",
    "\n",
    "# Load the dataset from the specified directory\n",
    "loaded_data = load_from_disk(load_directory)\n",
    "\n",
    "# Now you can access the individual splits (train, test, unsupervised) as follows:\n",
    "train_dataset = loaded_data[\"train\"]\n",
    "test_dataset = loaded_data[\"test\"]\n",
    "unsupervised_dataset = loaded_data[\"unsupervised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_TO_CLASSIFY = test_dataset['text'][:N]\n",
    "TRUE_LABES = test_dataset['label'][:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_texts(texts, labels, output_file):\n",
    "    with open(output_file, 'a') as file:\n",
    "        i = 0\n",
    "        print('Started')\n",
    "        for text, true_label in zip(texts, labels):\n",
    "\n",
    "            context = build_content(text)\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[context]\n",
    "            )\n",
    "\n",
    "            output_class = response.choices[0].message.content\n",
    "            file.write(f\"{output_class}, {true_label}\\n\")\n",
    "            print(f'Saved Row {i}: {output_class}, {true_label}')\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Saved Row 0: negative, 0\n",
      "Saved Row 1: positive, 0\n",
      "Saved Row 2: negative, 0\n",
      "Saved Row 3: negative, 0\n",
      "Saved Row 4: positive, 0\n",
      "Saved Row 5: negative, 0\n",
      "Saved Row 6: negative, 0\n",
      "Saved Row 7: negative, 0\n",
      "Saved Row 8: negative, 0\n",
      "Saved Row 9: negative, 0\n",
      "Saved Row 10: negative, 0\n",
      "Saved Row 11: negative, 0\n",
      "Saved Row 12: negative, 0\n",
      "Saved Row 13: negative, 0\n",
      "Saved Row 14: negative, 0\n",
      "Saved Row 15: negative, 0\n",
      "Saved Row 16: negative, 0\n",
      "Saved Row 17: negative, 0\n",
      "Saved Row 18: negative, 0\n",
      "Saved Row 19: negative, 0\n",
      "Saved Row 20: positive, 0\n",
      "Saved Row 21: positive, 0\n",
      "Saved Row 22: negative, 0\n",
      "Saved Row 23: negative, 0\n",
      "Saved Row 24: negative, 0\n",
      "Saved Row 25: negative, 0\n",
      "Saved Row 26: negative, 0\n",
      "Saved Row 27: negative, 0\n",
      "Saved Row 28: negative, 0\n",
      "Saved Row 29: negative, 0\n",
      "Saved Row 30: negative, 0\n",
      "Saved Row 31: negative, 0\n",
      "Saved Row 32: positive, 0\n",
      "Saved Row 33: negative, 0\n",
      "Saved Row 34: negative, 0\n",
      "Saved Row 35: negative, 0\n",
      "Saved Row 36: negative, 0\n",
      "Saved Row 37: positive, 0\n",
      "Saved Row 38: negative, 0\n",
      "Saved Row 39: negative, 0\n",
      "Saved Row 40: negative, 0\n",
      "Saved Row 41: negative, 0\n",
      "Saved Row 42: negative, 0\n",
      "Saved Row 43: negative, 0\n",
      "Saved Row 44: negative, 0\n",
      "Saved Row 45: negative, 0\n",
      "Saved Row 46: positive, 0\n",
      "Saved Row 47: negative, 0\n",
      "Saved Row 48: negative, 0\n",
      "Saved Row 49: negative, 0\n",
      "Saved Row 50: negative, 0\n",
      "Saved Row 51: negative, 0\n",
      "Saved Row 52: negative, 0\n",
      "Saved Row 53: negative, 0\n",
      "Saved Row 54: negative, 0\n",
      "Saved Row 55: negative, 0\n",
      "Saved Row 56: negative, 0\n",
      "Saved Row 57: negative, 0\n",
      "Saved Row 58: positive, 0\n",
      "Saved Row 59: negative, 0\n",
      "Saved Row 60: negative, 0\n",
      "Saved Row 61: positive, 0\n",
      "Saved Row 62: negative, 0\n",
      "Saved Row 63: negative, 0\n",
      "Saved Row 64: negative, 0\n",
      "Saved Row 65: negative, 0\n",
      "Saved Row 66: negative, 0\n",
      "Saved Row 67: negative, 0\n",
      "Saved Row 68: negative, 0\n",
      "Saved Row 69: negative, 0\n",
      "Saved Row 70: negative, 0\n",
      "Saved Row 71: negative, 0\n",
      "Saved Row 72: negative, 0\n",
      "Saved Row 73: positive, 0\n",
      "Saved Row 74: Negative, 0\n",
      "Saved Row 75: negative, 0\n",
      "Saved Row 76: positive, 0\n",
      "Saved Row 77: negative, 0\n",
      "Saved Row 78: negative, 0\n",
      "Saved Row 79: negative, 0\n",
      "Saved Row 80: negative, 0\n",
      "Saved Row 81: negative, 0\n",
      "Saved Row 82: negative, 0\n",
      "Saved Row 83: negative, 0\n",
      "Saved Row 84: negative, 0\n",
      "Saved Row 85: negative, 0\n",
      "Saved Row 86: negative, 0\n",
      "Saved Row 87: negative, 0\n",
      "Saved Row 88: positive, 0\n",
      "Saved Row 89: negative, 0\n",
      "Saved Row 90: negative, 0\n",
      "Saved Row 91: negative, 0\n",
      "Saved Row 92: negative, 0\n",
      "Saved Row 93: negative, 0\n",
      "Saved Row 94: negative, 0\n",
      "Saved Row 95: negative, 0\n",
      "Saved Row 96: negative, 0\n",
      "Saved Row 97: positive, 0\n",
      "Saved Row 98: negative, 0\n",
      "Saved Row 99: negative, 0\n",
      "CPU times: user 1.4 s, sys: 341 ms, total: 1.74 s\n",
      "Wall time: 8h 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify_texts(TEXTS_TO_CLASSIFY, TRUE_LABES, \"GPT_output/gpt_out_original.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**out_100_original.txt**\n",
    "\n",
    "100%|██████████| 13/13 [00:15<00:00,  1.16s/it]\n",
    "Score:  {'accuracy': 0.92}\n",
    "\n",
    "\n",
    "**out_distilBERT_100_original.txt**\n",
    "\n",
    "100%|██████████| 13/13 [00:07<00:00,  1.73it/s]\n",
    "Score:  {'accuracy': 0.92}\n",
    "\n",
    "**out_distilbert_augmented_100_transformed.txt**\n",
    "\n",
    "100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
    "Score:  {'accuracy': 0.89}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output = pd.read_csv(\"GPT_output/gpt_out_original.txt\", header=None, names=['pred', 'true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'negative': 0, 'positive': 1}\n",
    "gpt_output['pred_numeric'] = gpt_output['pred'].str.lower().map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (gpt_output['pred_numeric'] == gpt_output['true_label']).mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset = dataset[\"test\"].select(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:28<00:00,  3.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = dataset.map(custom_transform, load_from_cache_file=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS_TO_CLASSIFY = transformed_dataset['text']\n",
    "TRUE_LABES = transformed_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classify_texts(TEXTS_TO_CLASSIFY, TRUE_LABES, \"GPT_output/gpt_out_transformed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "import argparse\n",
    "from utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG Purpose\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = parser.parse_args()\n",
    "class MyDict:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        for key, value in data.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = MyDict({\n",
    "    \"train\": False,\n",
    "    \"train_augmented\": False,\n",
    "    \"eval\": True,\n",
    "    \"eval_transformed\": True,\n",
    "    \"model_dir\": \"./CARC_output/out\",\n",
    "    \"debug_transformation\": False,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"small\": False\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Core training function\n",
    "def do_train(args, model, train_dataloader, save_dir=\"./out\"):\n",
    "\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    num_epochs = args.num_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    model.train()\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # Implement the training loop --- make sure to use the optimizer and lr_sceduler (learning rate scheduler)\n",
    "    # Remember that pytorch uses gradient accumumlation so you need to use zero_grad (https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html)\n",
    "    # You can use progress_bar.update(1) to see the progress during training\n",
    "    # You can refer to the pytorch tutorial covered in class for reference\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            progress_bar.update(1)   \n",
    "    \n",
    "    print(\"Training completed...\")\n",
    "    print(\"Saving Model....\")\n",
    "    model.save_pretrained(save_dir)\n",
    "    \n",
    "    return\n",
    "    \n",
    "    \n",
    "# Core evaluation function\n",
    "def do_eval(eval_dataloader, output_dir, out_file):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "        # write to output file\n",
    "        for i in range(predictions.shape[0]):\n",
    "            out_file.write(str(predictions[i].item()) + \"\\n\")\n",
    "            #out_file.write(\"\\n\")\n",
    "            out_file.write(str(batch[\"labels\"][i].item()) + \"\\n\\n\")\n",
    "            #out_file.write(\"\\n\\n\")\n",
    "\n",
    "    score = metric.compute()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Created a dataladoer for the augmented training dataset\n",
    "def create_augmented_dataloader(dataset):\n",
    "    \n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "    \n",
    "    # Here, 'dataset' is the original dataset. You should return a dataloader called 'train_dataloader' (with batch size = 8) -- this\n",
    "    # dataloader will be for the original training split augmented with 5k random transformed examples from the training set.\n",
    "    # You may want to set load_from_cache_file to False when using dataset maps\n",
    "    # You may find it helpful to see how the dataloader was created at other place in this code.\n",
    "\n",
    "    # 5000 randomly transformed examples\n",
    "    train_augmented_size = 5000\n",
    "    train_transformed_sample = dataset[\"train\"].shuffle(seed=42).select(range(train_augmented_size))\n",
    "    train_transformed_sample = train_transformed_sample.map(custom_transform, load_from_cache_file=False) \n",
    "\n",
    "    # Augment the training data with 5000 randomly transformed examples to create the new augmented training dataset\n",
    "    # Final dataset train size: \"25,000\" + \"5,000\" = \"30,000\" \n",
    "    train_transformed_dataset = concatenate_datasets([dataset[\"train\"], train_transformed_sample])                                                \n",
    "    \n",
    "    train_dataloader = None\n",
    "\n",
    "    tokenized_dataset = train_transformed_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Prepare dataset for use by model\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    # Create dataloaders for iterating over the dataset\n",
    "    train_dataloader = DataLoader(tokenized_dataset, shuffle=True, batch_size=8)\n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE ######\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "# Create a dataloader for the transformed test set\n",
    "def create_transformed_dataloader(dataset, debug_transformation):\n",
    "    \n",
    "    # Print 5 random transformed examples\n",
    "    if debug_transformation:\n",
    "        small_dataset = dataset[\"test\"].shuffle(seed=42).select(range(5))\n",
    "        small_transformed_dataset = small_dataset.map(custom_transform, load_from_cache_file=False)\n",
    "        for k in range(5):\n",
    "            print(\"Original Example \", str(k))\n",
    "            print(small_dataset[k])\n",
    "            print(\"\\n\")\n",
    "            print(\"Transformed Example \", str(k))\n",
    "            print(small_transformed_dataset[k])\n",
    "            print('='*30)\n",
    "\n",
    "        exit()\n",
    "      \n",
    "    \n",
    "    transformed_dataset = dataset[\"test\"].map(custom_transform, load_from_cache_file=False)                                                    \n",
    "    transformed_tokenized_dataset = transformed_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.remove_columns([\"text\"])\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    transformed_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    transformed_val_dataset = transformed_tokenized_dataset    \n",
    "    eval_dataloader = DataLoader(transformed_val_dataset, batch_size=8)\n",
    "    \n",
    "    return eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "global device\n",
    "global tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"test\"].select(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for use by model\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(tokenized_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:15<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  {'accuracy': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the original test dataset\n",
    "if args.eval:\n",
    "    \n",
    "    out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
    "    out_file = out_file + f\"_{N}\" +\"_original.txt\"\n",
    "    out_file = open(out_file, \"w\")\n",
    "    \n",
    "    score = do_eval(eval_dataloader, args.model_dir, out_file)\n",
    "    print(\"Score: \", score)\n",
    "    \n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"] = dataset[\"test\"].select(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_dir = \"CARC_output/out_augmented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:28<00:00,  3.56 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2024.33 examples/s]\n",
      "100%|██████████| 13/13 [00:15<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  {'accuracy': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the transformed test dataset\n",
    "if args.eval_transformed:\n",
    "        \n",
    "    out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
    "    out_file = out_file + f\"_{N}\" + \"_transformed.txt\"\n",
    "    out_file = open(out_file, \"w\")\n",
    "    \n",
    "    eval_transformed_dataloader = create_transformed_dataloader(dataset, args.debug_transformation)\n",
    "    score = do_eval(eval_transformed_dataloader, args.model_dir, out_file)\n",
    "    print(\"Score: \", score)\n",
    "    \n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Destilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG Purpose\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = parser.parse_args()\n",
    "class MyDict:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        for key, value in data.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = MyDict({\n",
    "    \"train\": False,\n",
    "    \"train_augmented\": False,\n",
    "    \"eval\": True,\n",
    "    \"eval_transformed\": True,\n",
    "    \"model_dir\": \"./CARC_output/out_distilBERT\",\n",
    "    \"debug_transformation\": False,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"small\": False\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Core training function\n",
    "def do_train(args, model, train_dataloader, save_dir=\"./out_distilbert\"):\n",
    "\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "    num_epochs = args.num_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    model.train()\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # Implement the training loop --- make sure to use the optimizer and lr_sceduler (learning rate scheduler)\n",
    "    # Remember that pytorch uses gradient accumumlation so you need to use zero_grad (https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html)\n",
    "    # You can use progress_bar.update(1) to see the progress during training\n",
    "    # You can refer to the pytorch tutorial covered in class for reference\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            progress_bar.update(1)   \n",
    "    \n",
    "    print(\"Training completed...\")\n",
    "    print(\"Saving Model....\")\n",
    "    model.save_pretrained(save_dir)\n",
    "    \n",
    "    return\n",
    "    \n",
    "    \n",
    "# Core evaluation function\n",
    "def do_eval(eval_dataloader, output_dir, out_file):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "        # write to output file\n",
    "        for i in range(predictions.shape[0]):\n",
    "            out_file.write(str(predictions[i].item()) + \"\\n\")\n",
    "            #out_file.write(\"\\n\")\n",
    "            out_file.write(str(batch[\"labels\"][i].item()) + \"\\n\\n\")\n",
    "            #out_file.write(\"\\n\\n\")\n",
    "\n",
    "    score = metric.compute()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Created a dataladoer for the augmented training dataset\n",
    "def create_augmented_dataloader(dataset):\n",
    "    \n",
    "    ################################\n",
    "    ##### YOUR CODE BEGINGS HERE ###\n",
    "    \n",
    "    # Here, 'dataset' is the original dataset. You should return a dataloader called 'train_dataloader' (with batch size = 8) -- this\n",
    "    # dataloader will be for the original training split augmented with 5k random transformed examples from the training set.\n",
    "    # You may want to set load_from_cache_file to False when using dataset maps\n",
    "    # You may find it helpful to see how the dataloader was created at other place in this code.\n",
    "\n",
    "    # 5000 randomly transformed examples\n",
    "    train_augmented_size = 5000\n",
    "    train_transformed_sample = dataset[\"train\"].shuffle(seed=42).select(range(train_augmented_size))\n",
    "    train_transformed_sample = train_transformed_sample.map(custom_transform, load_from_cache_file=False) \n",
    "\n",
    "    # Augment the training data with 5000 randomly transformed examples to create the new augmented training dataset\n",
    "    # Final dataset train size: \"25,000\" + \"5,000\" = \"30,000\" \n",
    "    train_transformed_dataset = concatenate_datasets([dataset[\"train\"], train_transformed_sample])                                                \n",
    "    \n",
    "    train_dataloader = None\n",
    "\n",
    "    tokenized_dataset = train_transformed_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Prepare dataset for use by model\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    # Create dataloaders for iterating over the dataset\n",
    "    train_dataloader = DataLoader(tokenized_dataset, shuffle=True, batch_size=8)\n",
    "    \n",
    "    ##### YOUR CODE ENDS HERE ######\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "# Create a dataloader for the transformed test set\n",
    "def create_transformed_dataloader(dataset, debug_transformation):\n",
    "    \n",
    "    # Print 5 random transformed examples\n",
    "    if debug_transformation:\n",
    "        small_dataset = dataset[\"test\"].shuffle(seed=42).select(range(5))\n",
    "        small_transformed_dataset = small_dataset.map(custom_transform, load_from_cache_file=False)\n",
    "        for k in range(5):\n",
    "            print(\"Original Example \", str(k))\n",
    "            print(small_dataset[k])\n",
    "            print(\"\\n\")\n",
    "            print(\"Transformed Example \", str(k))\n",
    "            print(small_transformed_dataset[k])\n",
    "            print('='*30)\n",
    "\n",
    "        exit()\n",
    "      \n",
    "    \n",
    "    transformed_dataset = dataset[\"test\"].map(custom_transform, load_from_cache_file=False)                                                    \n",
    "    transformed_tokenized_dataset = transformed_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.remove_columns([\"text\"])\n",
    "    transformed_tokenized_dataset = transformed_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    transformed_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    transformed_val_dataset = transformed_tokenized_dataset    \n",
    "    eval_dataloader = DataLoader(transformed_val_dataset, batch_size=8)\n",
    "    \n",
    "    return eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "global device\n",
    "global tokenizer\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset = dataset[\"test\"].select(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for use by model\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(tokenized_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  {'accuracy': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the original test dataset\n",
    "if args.eval:\n",
    "    \n",
    "    out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
    "    out_file = out_file + f\"_{N}\" \"_original.txt\"\n",
    "    out_file = open(out_file, \"w\")\n",
    "    \n",
    "    score = do_eval(eval_dataloader, args.model_dir, out_file)\n",
    "    print(\"Score: \", score)\n",
    "    \n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "global device\n",
    "global tokenizer\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"] = dataset[\"test\"].select(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CARC_output/out_distilbert_augmented'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.normpath(args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:26<00:00,  3.77 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2255.41 examples/s]\n",
      "100%|██████████| 13/13 [00:07<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  {'accuracy': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the transformed test dataset\n",
    "if args.eval_transformed:\n",
    "        \n",
    "    out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
    "    out_file = out_file + f\"_{N}\" + \"_transformed.txt\"\n",
    "    out_file = open(out_file, \"w\")\n",
    "    \n",
    "    eval_transformed_dataloader = create_transformed_dataloader(dataset, args.debug_transformation)\n",
    "    score = do_eval(eval_transformed_dataloader, args.model_dir, out_file)\n",
    "    print(\"Score: \", score)\n",
    "    \n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
