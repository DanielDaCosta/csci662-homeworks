{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_class = Features_FeedForward(\"datasets/questions.train.txt\", \"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_class.embedding_matrix['UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from nn_layers import FeedForwardNetwork\n",
    "\n",
    "class NeuralModel(Model):\n",
    "    def __init__(self, embeddingfile, max_seq_length, hidden_units, minibatch_size, learning_rate, epochs): \n",
    "        # self.network = FeedForwardNetwork()\n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_units = hidden_units\n",
    "        self.weights_1 = None\n",
    "        self.bias_1 = None\n",
    "        self.weights_2 = None\n",
    "        self.bias_2 = None\n",
    "        self.Y_to_categorical = None\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = {}\n",
    "    \n",
    "    def initialize_weights(self, n_inputs, n_output):\n",
    "        # weights = np.zeros((n_inputs, n_output))\n",
    "        # bias = np.zeros(n_output)\n",
    "        # np.random.seed(0)\n",
    "        weights = np.random.rand(n_inputs, n_output)\n",
    "        bias = np.random.rand(n_output)\n",
    "        return weights, bias\n",
    "    \n",
    "    def relu_function(self, A):\n",
    "        '''A = x*W + b\n",
    "\n",
    "        :return: Z = relut(x*A+b)\n",
    "        '''\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def relu_derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        A = np.dot(X, self.weights_1) + self.bias_1\n",
    "        h = self.relu_function(A)\n",
    "\n",
    "        A_2 = np.dot(h, self.weights_2) + self.bias_2\n",
    "\n",
    "        O = self.softmax(A_2)\n",
    "\n",
    "        # Rows with highest probability\n",
    "        S_max = np.argmax(O, axis=1)\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "    def convert_to_embeddings(self, sentence):\n",
    "        '''Convert sentence to embeddings\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "            # try:\n",
    "        if emb: # if there is a word\n",
    "            emb_concat = np.concatenate(emb, axis=0)\n",
    "        else:\n",
    "            emb_concat = []\n",
    "        # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "        if len(emb) < self.max_seq_length:\n",
    "            # Missing words\n",
    "            words_missing = self.max_seq_length - len(emb)\n",
    "            # print(words_missing)\n",
    "            emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        # Convert to OneHot for computing Loss\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim × features (u)\n",
    "        n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "            features_ff_class.tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Wieghts\n",
    "        # Create W_a and b_a\n",
    "        # W_a[n_documents, hidden_units (u)]\n",
    "        # b_a[hidden_units (u)]\n",
    "        W_1, b_1 = self.initialize_weights(n_inputs, self.hidden_units)\n",
    "        # Create Wb and b_b\n",
    "        # W_b[hidden_units (u), num_labels (d)]\n",
    "        # b_b[num_labels]\n",
    "        W_2, b_2 = self.initialize_weights(self.hidden_units, num_labels)\n",
    "\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        self.weights_1 = W_1\n",
    "        self.bias_1 = b_1\n",
    "        self.weights_2 = W_2\n",
    "        self.bias_2 = b_2\n",
    "        for i in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "            \n",
    "                # ---------------- Input-to-Hidden Layer --------------- #\n",
    "                # Z1 = W_a*X + b_a\n",
    "                # Z1[n_documents, hidden_units (u)]\n",
    "                Z_1 = np.dot(X_mini_batch, self.weights_1) + self.bias_1\n",
    "                # Hidden Unit\n",
    "                # h = relu(A)\n",
    "                # h[n_documents, hidden_units (u)]\n",
    "                A_1 = self.relu_function(Z_1)\n",
    "\n",
    "                # ---------------- Hidden-to-Output Layer --------------- #\n",
    "                #  = W_b*h + b_b\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                Z_2 = np.dot(A_1, self.weights_2) + self.bias_2\n",
    "                # Output Layer\n",
    "                # A_2 = softmax(Z_2)\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                A_2 = self.softmax(Z_2)\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "\n",
    "                # Compute Gradients\n",
    "\n",
    "                dZ_2 = A_2 - y_mini_batch # [n_documents, num_labels (d)]\n",
    "                # np.dot(A_2, dZ_2) => (hidden_units, n_documents) X (n_documents, num_labels) = (hidden_units, num_labels)\n",
    "                dW_2 = (1/minibatch_size)*np.dot(A_1.T, dZ_2)\n",
    "                db_2 = (1/minibatch_size)*np.sum(dZ_2, axis=0, keepdims = True) # [num_labels]\n",
    "                # np.dot(self.weights_b, dZ_2) => [n_documents, num_labels (d)] X [num_labels (d), hidden_units (u)] => [n_documents, hidden_units]\n",
    "                dZ_1 = np.dot(dZ_2, self.weights_2.T)*self.relu_derivative(Z_1)\n",
    "                # np.dot(X, dZ_1) => (features, n_documents) X (n_documents, hidden_units) = (hidden_units, num_labels)\n",
    "                dW_1 = (1/minibatch_size)*np.dot(X_mini_batch.T, dZ_1)\n",
    "                db_1 = (1/minibatch_size)*np.sum(dZ_1, axis=0, keepdims = True) # [hidden_units]\n",
    "\n",
    "                # Update weights\n",
    "                self.weights_1 = self.weights_1 - self.learning_rate*dW_1\n",
    "                self.bias_1 = self.bias_1 - self.learning_rate*db_1\n",
    "                self.weights_2 = self.weights_2 - self.learning_rate*dW_2\n",
    "                self.bias_2 = self.bias_2 - self.learning_rate*db_2\n",
    "\n",
    "                ########\n",
    "                # Loss #\n",
    "                ########\n",
    "                mini_batch_loss.append(self.cross_entropy_loss(A_2, y_mini_batch))\n",
    "\n",
    "            loss = np.mean(mini_batch_loss)\n",
    "            self.loss[i] = loss\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "            tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Make Prediction\n",
    "        y_test = self.predict(X_test)\n",
    "        preds_label = []\n",
    "        for y in y_test:\n",
    "            tmp = self.Y_to_categorical[y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "# train_file = \"datasets/products/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "# model_file_name = \"nn.products.model\"\n",
    "# loss_file = \"datasets/products/loss.txt\"\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"nn.4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"nn.questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "train_file = \"datasets/odiya/train.txt\"\n",
    "emb_file = \"fasttext.wiki.300d.vec\"\n",
    "pred_file = \"datasets/odiya/val.test\"\n",
    "pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "model_file_name = \"nn.odiya.model\"\n",
    "loss_file = \"datasets/odiya/loss.txt\"\n",
    "nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.3689080946525617\n",
      "Epoch: 2 - Loss: 0.3235857197552775\n",
      "Epoch: 3 - Loss: 0.2936484100094434\n",
      "Epoch: 4 - Loss: 0.26352159594803354\n",
      "Epoch: 5 - Loss: 0.23764451643563353\n",
      "Epoch: 6 - Loss: 0.21800164605027886\n",
      "Epoch: 7 - Loss: 0.20325243465550616\n",
      "Epoch: 8 - Loss: 0.19236872624660573\n",
      "Epoch: 9 - Loss: 0.18419626091131025\n",
      "Epoch: 10 - Loss: 0.1777236197915786\n",
      "Epoch: 11 - Loss: 0.1721545916266068\n",
      "Epoch: 12 - Loss: 0.1671574032683084\n",
      "Epoch: 13 - Loss: 0.16266298219259104\n",
      "Epoch: 14 - Loss: 0.15852486890009898\n",
      "Epoch: 15 - Loss: 0.15468398721544724\n",
      "Epoch: 16 - Loss: 0.15107830296529468\n",
      "Epoch: 17 - Loss: 0.14769431963062984\n",
      "Epoch: 18 - Loss: 0.14455118635200417\n",
      "Epoch: 19 - Loss: 0.14152374656740854\n",
      "Epoch: 20 - Loss: 0.13867620597404318\n",
      "Epoch: 21 - Loss: 0.13589605865165189\n",
      "Epoch: 22 - Loss: 0.13327620658929243\n",
      "Epoch: 23 - Loss: 0.13072194141810134\n",
      "Epoch: 24 - Loss: 0.12828077133886762\n",
      "Epoch: 25 - Loss: 0.12590905460620072\n",
      "Epoch: 26 - Loss: 0.1236934601884644\n",
      "Epoch: 27 - Loss: 0.12140092543163582\n",
      "Epoch: 28 - Loss: 0.11919821949458319\n",
      "Epoch: 29 - Loss: 0.1172143877604881\n",
      "Epoch: 30 - Loss: 0.11519981762475891\n",
      "Epoch: 31 - Loss: 0.11331209963144852\n",
      "Epoch: 32 - Loss: 0.1113728095691458\n",
      "Epoch: 33 - Loss: 0.10959641492644319\n",
      "Epoch: 34 - Loss: 0.10775090008212122\n",
      "Epoch: 35 - Loss: 0.10604519231694667\n",
      "Epoch: 36 - Loss: 0.10429611899296555\n",
      "Epoch: 37 - Loss: 0.10263177133813382\n",
      "Epoch: 38 - Loss: 0.10098915626974317\n",
      "Epoch: 39 - Loss: 0.09934257982557786\n",
      "Epoch: 40 - Loss: 0.09768409624574013\n",
      "Epoch: 41 - Loss: 0.09607310180190136\n",
      "Epoch: 42 - Loss: 0.09457809623565228\n",
      "Epoch: 43 - Loss: 0.09307604089303834\n",
      "Epoch: 44 - Loss: 0.09150417824740034\n",
      "Epoch: 45 - Loss: 0.09015874984010114\n",
      "Epoch: 46 - Loss: 0.08876530109764913\n",
      "Epoch: 47 - Loss: 0.08758935524021806\n",
      "Epoch: 48 - Loss: 0.08620869622537862\n",
      "Epoch: 49 - Loss: 0.08503128035945211\n",
      "Epoch: 50 - Loss: 0.08387704974138309\n",
      "Epoch: 51 - Loss: 0.08266219863822612\n",
      "Epoch: 52 - Loss: 0.08166036283223789\n",
      "Epoch: 53 - Loss: 0.0805068733508502\n",
      "Epoch: 54 - Loss: 0.07939977381234252\n",
      "Epoch: 55 - Loss: 0.07837984744600354\n",
      "Epoch: 56 - Loss: 0.07731932768177781\n",
      "Epoch: 57 - Loss: 0.07624958084009788\n",
      "Epoch: 58 - Loss: 0.0752043792772498\n",
      "Epoch: 59 - Loss: 0.0742518609229764\n",
      "Epoch: 60 - Loss: 0.07320838487664427\n",
      "Epoch: 61 - Loss: 0.07224769041415328\n",
      "Epoch: 62 - Loss: 0.07140136019553507\n",
      "Epoch: 63 - Loss: 0.070437223375298\n",
      "Epoch: 64 - Loss: 0.06971078068191425\n",
      "Epoch: 65 - Loss: 0.06899821845387641\n",
      "Epoch: 66 - Loss: 0.0680622772494196\n",
      "Epoch: 67 - Loss: 0.06731467094848262\n",
      "Epoch: 68 - Loss: 0.06654854227134704\n",
      "Epoch: 69 - Loss: 0.06583904001469314\n",
      "Epoch: 70 - Loss: 0.06528361225155509\n",
      "Epoch: 71 - Loss: 0.06442981972569478\n",
      "Epoch: 72 - Loss: 0.0637344442966661\n",
      "Epoch: 73 - Loss: 0.06314833329454433\n",
      "Epoch: 74 - Loss: 0.062337915398147634\n",
      "Epoch: 75 - Loss: 0.06186959592789496\n",
      "Epoch: 76 - Loss: 0.06122026688321709\n",
      "Epoch: 77 - Loss: 0.06045035748390641\n",
      "Epoch: 78 - Loss: 0.06008444395824073\n",
      "Epoch: 79 - Loss: 0.059439476568165055\n",
      "Epoch: 80 - Loss: 0.058770578807705774\n",
      "Epoch: 81 - Loss: 0.05810675988544323\n",
      "Epoch: 82 - Loss: 0.05765164753726237\n",
      "Epoch: 83 - Loss: 0.05699671940104525\n",
      "Epoch: 84 - Loss: 0.056571411950787916\n",
      "Epoch: 85 - Loss: 0.056113337004267384\n",
      "Epoch: 86 - Loss: 0.05542761780066086\n",
      "Epoch: 87 - Loss: 0.0549770728041504\n",
      "Epoch: 88 - Loss: 0.05437711380290632\n",
      "Epoch: 89 - Loss: 0.05428170823975773\n",
      "Epoch: 90 - Loss: 0.053514097886203094\n",
      "Epoch: 91 - Loss: 0.052896783546478685\n",
      "Epoch: 92 - Loss: 0.052445269568928775\n",
      "Epoch: 93 - Loss: 0.05225591305916995\n",
      "Epoch: 94 - Loss: 0.0516680526342217\n",
      "Epoch: 95 - Loss: 0.05123292909333518\n",
      "Epoch: 96 - Loss: 0.05073892455575408\n",
      "Epoch: 97 - Loss: 0.050665045923899914\n",
      "Epoch: 98 - Loss: 0.05028759263526598\n",
      "Epoch: 99 - Loss: 0.04980175693440035\n",
      "Epoch: 100 - Loss: 0.04951679175724236\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.classify(pred_file + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           1380\n",
       "business          915\n",
       "entertainment     745\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           1532\n",
       "business          813\n",
       "entertainment     695\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.93%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
