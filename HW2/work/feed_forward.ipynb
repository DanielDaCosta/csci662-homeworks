{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file, threshold=0, max_features=None):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "        # self.vocabulary = list(self.embedding_matrix.keys())\n",
    "        # self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        # self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features-1))\n",
    "\n",
    "        # Add to vocabulary the Out-of-Vocabulary token\n",
    "        return list(flattened_list_count_filter.keys()) + ['UNK']\n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        # Create Vocabulary\n",
    "        self.vocabulary = self.create_vocabulary(tokenized_text, self.threshold, self.max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)\n",
    "    \n",
    "    def sort_by_tfidf(self, tfidf_matrix, max_seq_length):\n",
    "        \"\"\"Sort input documents based on tf*idf score.\n",
    "        Return top \"max_seq_length\" words\n",
    "        :param: tfidf_matrix\n",
    "        :param: max_seq_length\n",
    "        :return: sentences ordered by TF-IDF score\n",
    "        \"\"\"\n",
    "        \n",
    "        # Indices of sorted matrix in descending order\n",
    "        indices = np.argsort(-tfidf_matrix, axis=1)\n",
    "        tfidf_matrix_sorted = []\n",
    "\n",
    "        # Create sorted matrix\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            # sentence in orderd version\n",
    "            tmp = [self.index2word[index] for index in indices[i][:max_seq_length]]\n",
    "            tfidf_matrix_sorted.append(tmp)\n",
    "    \n",
    "        return tfidf_matrix_sorted\n",
    "    \n",
    "    def get_features_tfidf(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothinf\n",
    "        return tf*idf_array\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from nn_layers import FeedForwardNetwork\n",
    "\n",
    "class NeuralModel(Model):\n",
    "    def __init__(self, embeddingfile, max_seq_length, hidden_units, minibatch_size, learning_rate, epochs, tfidf=False, max_features=None, threshold=0, momentum=0): \n",
    "        # self.network = FeedForwardNetwork()\n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_units = hidden_units\n",
    "        self.weights_1 = None\n",
    "        self.bias_1 = None\n",
    "        self.weights_2 = None\n",
    "        self.bias_2 = None\n",
    "        self.Y_to_categorical = None\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = {}\n",
    "        # TF-IDF Sorting\n",
    "        self.tfidf = tfidf # enable sorting by tf-idf score\n",
    "        self.max_features = max_features\n",
    "        self.threshold = threshold\n",
    "        # Momentum\n",
    "        self.momentum = momentum\n",
    "        self.prev_dW_1 = 0\n",
    "        self.prev_db_1 = 0\n",
    "        self.prev_dW_2 = 0\n",
    "        self.prev_db_2 = 0\n",
    "    \n",
    "    def initialize_weights(self, n_inputs, n_output):\n",
    "        # weights = np.zeros((n_inputs, n_output))\n",
    "        # bias = np.zeros(n_output)\n",
    "        # np.random.seed(0)\n",
    "        weights = np.random.rand(n_inputs, n_output)\n",
    "        bias = np.random.rand(n_output)\n",
    "        return weights, bias\n",
    "    \n",
    "    def relu_function(self, A):\n",
    "        '''A = x*W + b\n",
    "\n",
    "        :return: Z = relut(x*A+b)\n",
    "        '''\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def relu_derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        S = np.maximum(epsilon, S)\n",
    "        S = np.minimum(1 - epsilon, S)\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        A = np.dot(X, self.weights_1) + self.bias_1\n",
    "        h = self.relu_function(A)\n",
    "\n",
    "        A_2 = np.dot(h, self.weights_2) + self.bias_2\n",
    "\n",
    "        O = self.softmax(A_2)\n",
    "\n",
    "        # Rows with highest probability\n",
    "        S_max = np.argmax(O, axis=1)\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "    def convert_to_embeddings(self, sentence):\n",
    "        '''Convert sentence to embeddings\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "            # try:\n",
    "        if emb: # if there is a word\n",
    "            emb_concat = np.concatenate(emb, axis=0)\n",
    "        else:\n",
    "            emb_concat = []\n",
    "        # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "        if len(emb) < self.max_seq_length:\n",
    "            # Missing words\n",
    "            words_missing = self.max_seq_length - len(emb)\n",
    "            # print(words_missing)\n",
    "            emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile, threshold=self.threshold, max_features=self.max_features)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        # Convert to OneHot for computing Loss\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim Ã— features (u)\n",
    "        n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        print(\"Computing TFIDF\")\n",
    "        if self.tfidf: # Truncate input to the max sequence length sorted by TF-IDF\n",
    "            tf_idf = features_ff_class.tf_idf(features_ff_class.tokenized_text)\n",
    "            trunc_tokenized_text = features_ff_class.sort_by_tfidf(\n",
    "                tf_idf,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "                features_ff_class.tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        print(\"Computing Embedding\")\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Wieghts\n",
    "        # Create W_a and b_a\n",
    "        # W_a[n_documents, hidden_units (u)]\n",
    "        # b_a[hidden_units (u)]\n",
    "        W_1, b_1 = self.initialize_weights(n_inputs, self.hidden_units)\n",
    "        # Create Wb and b_b\n",
    "        # W_b[hidden_units (u), num_labels (d)]\n",
    "        # b_b[num_labels]\n",
    "        W_2, b_2 = self.initialize_weights(self.hidden_units, num_labels)\n",
    "\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        self.weights_1 = W_1\n",
    "        self.bias_1 = b_1\n",
    "        self.weights_2 = W_2\n",
    "        self.bias_2 = b_2\n",
    "        for i in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+minibatch_size]\n",
    "\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "            \n",
    "                # ---------------- Input-to-Hidden Layer --------------- #\n",
    "                # Z1 = W_a*X + b_a\n",
    "                # Z1[n_documents, hidden_units (u)]\n",
    "                Z_1 = np.dot(X_mini_batch, self.weights_1) + self.bias_1\n",
    "                # Hidden Unit\n",
    "                # h = relu(A)\n",
    "                # h[n_documents, hidden_units (u)]\n",
    "                A_1 = self.relu_function(Z_1)\n",
    "\n",
    "                # ---------------- Hidden-to-Output Layer --------------- #\n",
    "                # Z_2 = W_b*h + b_b\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                Z_2 = np.dot(A_1, self.weights_2) + self.bias_2\n",
    "                # Output Layer\n",
    "                # A_2 = softmax(Z_2)\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                A_2 = self.softmax(Z_2)\n",
    "                # print(A_2)\n",
    "\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "\n",
    "                # Compute Gradients\n",
    "\n",
    "                dZ_2 = A_2 - y_mini_batch # [n_documents, num_labels (d)]\n",
    "                # np.dot(A_2, dZ_2) => (hidden_units, n_documents) X (n_documents, num_labels) = (hidden_units, num_labels)\n",
    "                dW_2 = (1/minibatch_size)*np.dot(A_1.T, dZ_2)\n",
    "                db_2 = (1/minibatch_size)*np.sum(dZ_2, axis=0, keepdims = True) # [num_labels]\n",
    "                # np.dot(self.weights_b, dZ_2) => [n_documents, num_labels (d)] X [num_labels (d), hidden_units (u)] => [n_documents, hidden_units]\n",
    "                dZ_1 = np.dot(dZ_2, self.weights_2.T)*self.relu_derivative(Z_1)\n",
    "                # np.dot(X, dZ_1) => (features, n_documents) X (n_documents, hidden_units) = (hidden_units, num_labels)\n",
    "                dW_1 = (1/minibatch_size)*np.dot(X_mini_batch.T, dZ_1)\n",
    "                db_1 = (1/minibatch_size)*np.sum(dZ_1, axis=0, keepdims = True) # [hidden_units]\n",
    "\n",
    "                # Update weights\n",
    "                self.weights_1 = self.weights_1 - (self.learning_rate*dW_1 + self.momentum*self.prev_dW_1)\n",
    "                self.bias_1 = self.bias_1 - (self.learning_rate*db_1 + self.momentum*self.prev_db_1)\n",
    "                self.weights_2 = self.weights_2 - (self.learning_rate*dW_2 + self.momentum*self.prev_dW_2)\n",
    "                self.bias_2 = self.bias_2 - (self.learning_rate*db_2 + self.momentum*self.prev_db_2)\n",
    "\n",
    "                # Save previous gradients for Momentum\n",
    "                self.prev_dW_1 = (self.learning_rate*dW_1 + self.momentum*self.prev_dW_1)\n",
    "                self.prev_db_1 = (self.learning_rate*db_1 + self.momentum*self.prev_db_1)\n",
    "                self.prev_dW_2 = (self.learning_rate*dW_2 + self.momentum*self.prev_dW_2)\n",
    "                self.prev_db_2 = (self.learning_rate*db_2 + self.momentum*self.prev_db_2)\n",
    "\n",
    "                ########\n",
    "                # Loss #\n",
    "                ########\n",
    "                mini_batch_loss.append(self.cross_entropy_loss(A_2, y_mini_batch))\n",
    "\n",
    "            loss = np.mean(mini_batch_loss)\n",
    "            self.loss[i] = loss\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        if self.tfidf:\n",
    "            tf_idf_inference = []\n",
    "            # Get features from inference file\n",
    "            for sentence in tokenized_text:\n",
    "                # Transform dataset to TF-IDF space\n",
    "                # Return features with format (1, size_vocabulary)\n",
    "                X_sentence = self.features_ff_class.get_features_tfidf(sentence, self.features_ff_class.idf)\n",
    "                tf_idf_inference.append(X_sentence)\n",
    "            tf_idf_inference = np.stack(tf_idf_inference)\n",
    "            trunc_tokenized_text = self.features_ff_class.sort_by_tfidf(\n",
    "                tf_idf_inference,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "                tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Make Prediction\n",
    "        y_test = self.predict(X_test)\n",
    "        preds_label = []\n",
    "        for y in y_test:\n",
    "            tmp = self.Y_to_categorical[y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "train_file = \"datasets/products/train.txt\"\n",
    "emb_file = \"glove.6B.50d.txt\"\n",
    "pred_file = \"datasets/products/val.test\"\n",
    "pred_true_labels = \"datasets/products/val.txt\"\n",
    "model_file_name = \"products.model\"\n",
    "loss_file = \"datasets/products/loss.txt\"\n",
    "nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4) # 65.6%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=256, learning_rate=0.1, epochs=500, tfidf=True, max_features=500, threshold=2) # 64%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=5, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.1) # 65%\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# # nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=32, learning_rate=0.05, epochs=100)\n",
    "# # nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=128, learning_rate=0.05, epochs=100, tfidf=True, threshold=2, max_features=1000)\n",
    "# # nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=80, minibatch_size=32, learning_rate=0.1, epochs=200)\n",
    "# # nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=100, minibatch_size=256, learning_rate=0.01, epochs=200) # 34%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=60, hidden_units=90, minibatch_size=64, learning_rate=0.02, epochs=200)\n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "# train_file = \"datasets/odiya/train.txt\"\n",
    "# emb_file = \"fasttext.wiki.300d.vec\"\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "# model_file_name = \"odiya.model\"\n",
    "# loss_file = \"datasets/odiya/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF\n",
      "Computing Embedding\n",
      "Epoch: 1 - Loss: 0.4487996914769489\n",
      "Epoch: 2 - Loss: 0.33713686485064864\n",
      "Epoch: 3 - Loss: 0.33679762834518295\n",
      "Epoch: 4 - Loss: 0.33638197208474035\n",
      "Epoch: 5 - Loss: 0.3359117568466072\n",
      "Epoch: 6 - Loss: 0.3355169067726891\n",
      "Epoch: 7 - Loss: 0.3352074471936887\n",
      "Epoch: 8 - Loss: 0.3349247277714218\n",
      "Epoch: 9 - Loss: 0.3346442026966194\n",
      "Epoch: 10 - Loss: 0.33434727896871863\n",
      "Epoch: 11 - Loss: 0.3340461020478941\n",
      "Epoch: 12 - Loss: 0.3337463488594543\n",
      "Epoch: 13 - Loss: 0.33344807686404054\n",
      "Epoch: 14 - Loss: 0.3331328926849885\n",
      "Epoch: 15 - Loss: 0.3328182336710502\n",
      "Epoch: 16 - Loss: 0.33248862974906385\n",
      "Epoch: 17 - Loss: 0.332133895091838\n",
      "Epoch: 18 - Loss: 0.33175319543614296\n",
      "Epoch: 19 - Loss: 0.33135807866178657\n",
      "Epoch: 20 - Loss: 0.3308966037531822\n",
      "Epoch: 21 - Loss: 0.3303628947710778\n",
      "Epoch: 22 - Loss: 0.3298098935608551\n",
      "Epoch: 23 - Loss: 0.32927208066199204\n",
      "Epoch: 24 - Loss: 0.32868300418360236\n",
      "Epoch: 25 - Loss: 0.32809955810867847\n",
      "Epoch: 26 - Loss: 0.3274848650506293\n",
      "Epoch: 27 - Loss: 0.3268873802812416\n",
      "Epoch: 28 - Loss: 0.3263077312869842\n",
      "Epoch: 29 - Loss: 0.32569836313770156\n",
      "Epoch: 30 - Loss: 0.3251098190938022\n",
      "Epoch: 31 - Loss: 0.3245023197521503\n",
      "Epoch: 32 - Loss: 0.32391423812277936\n",
      "Epoch: 33 - Loss: 0.32334963329568006\n",
      "Epoch: 34 - Loss: 0.3228001805873353\n",
      "Epoch: 35 - Loss: 0.32224062584572455\n",
      "Epoch: 36 - Loss: 0.3217412504226949\n",
      "Epoch: 37 - Loss: 0.3212074155851616\n",
      "Epoch: 38 - Loss: 0.32073073477378716\n",
      "Epoch: 39 - Loss: 0.32026195558065124\n",
      "Epoch: 40 - Loss: 0.31978376705792205\n",
      "Epoch: 41 - Loss: 0.3193173401247704\n",
      "Epoch: 42 - Loss: 0.3188458645852142\n",
      "Epoch: 43 - Loss: 0.31840943790927556\n",
      "Epoch: 44 - Loss: 0.3179577197183595\n",
      "Epoch: 45 - Loss: 0.31760160475975086\n",
      "Epoch: 46 - Loss: 0.31710948486853197\n",
      "Epoch: 47 - Loss: 0.3166859506199173\n",
      "Epoch: 48 - Loss: 0.31625687798391267\n",
      "Epoch: 49 - Loss: 0.31581422651121044\n",
      "Epoch: 50 - Loss: 0.3154037115146204\n",
      "Epoch: 51 - Loss: 0.3150351631333627\n",
      "Epoch: 52 - Loss: 0.31457831988609725\n",
      "Epoch: 53 - Loss: 0.31406424273658246\n",
      "Epoch: 54 - Loss: 0.31363515924121665\n",
      "Epoch: 55 - Loss: 0.31348034185919327\n",
      "Epoch: 56 - Loss: 0.31286324827613693\n",
      "Epoch: 57 - Loss: 0.31244929824852524\n",
      "Epoch: 58 - Loss: 0.31186133325515697\n",
      "Epoch: 59 - Loss: 0.31140945176214246\n",
      "Epoch: 60 - Loss: 0.31101966928729485\n",
      "Epoch: 61 - Loss: 0.3106545261592062\n",
      "Epoch: 62 - Loss: 0.3102854305156482\n",
      "Epoch: 63 - Loss: 0.30984351560983614\n",
      "Epoch: 64 - Loss: 0.30946218914476786\n",
      "Epoch: 65 - Loss: 0.3092092555796287\n",
      "Epoch: 66 - Loss: 0.30889070439471783\n",
      "Epoch: 67 - Loss: 0.30850145511220917\n",
      "Epoch: 68 - Loss: 0.3081825576224538\n",
      "Epoch: 69 - Loss: 0.3077277874898723\n",
      "Epoch: 70 - Loss: 0.3073684672867269\n",
      "Epoch: 71 - Loss: 0.30704382112386247\n",
      "Epoch: 72 - Loss: 0.3067878764545392\n",
      "Epoch: 73 - Loss: 0.3064268923854066\n",
      "Epoch: 74 - Loss: 0.3060593012150852\n",
      "Epoch: 75 - Loss: 0.305781610558044\n",
      "Epoch: 76 - Loss: 0.30543125346059957\n",
      "Epoch: 77 - Loss: 0.3051243834758558\n",
      "Epoch: 78 - Loss: 0.3048728941977476\n",
      "Epoch: 79 - Loss: 0.30459061213496424\n",
      "Epoch: 80 - Loss: 0.3043222146736102\n",
      "Epoch: 81 - Loss: 0.3039803617065047\n",
      "Epoch: 82 - Loss: 0.3037965631624117\n",
      "Epoch: 83 - Loss: 0.30355135334488553\n",
      "Epoch: 84 - Loss: 0.3032370963254815\n",
      "Epoch: 85 - Loss: 0.3029471809408115\n",
      "Epoch: 86 - Loss: 0.3026619270145771\n",
      "Epoch: 87 - Loss: 0.3024206861608327\n",
      "Epoch: 88 - Loss: 0.3022235337792763\n",
      "Epoch: 89 - Loss: 0.30200157780377435\n",
      "Epoch: 90 - Loss: 0.30176759968427214\n",
      "Epoch: 91 - Loss: 0.3014750010318464\n",
      "Epoch: 92 - Loss: 0.30123305693492936\n",
      "Epoch: 93 - Loss: 0.3009614281837946\n",
      "Epoch: 94 - Loss: 0.30070410093010186\n",
      "Epoch: 95 - Loss: 0.30047602304766896\n",
      "Epoch: 96 - Loss: 0.3002476944145432\n",
      "Epoch: 97 - Loss: 0.3000283570654596\n",
      "Epoch: 98 - Loss: 0.29983382270799336\n",
      "Epoch: 99 - Loss: 0.29964918203175456\n",
      "Epoch: 100 - Loss: 0.29944504635630625\n",
      "Epoch: 101 - Loss: 0.2992685301448576\n",
      "Epoch: 102 - Loss: 0.2990983708380136\n",
      "Epoch: 103 - Loss: 0.2989071059779823\n",
      "Epoch: 104 - Loss: 0.29873869939487313\n",
      "Epoch: 105 - Loss: 0.2985541526299662\n",
      "Epoch: 106 - Loss: 0.2983696510106584\n",
      "Epoch: 107 - Loss: 0.2982075200463089\n",
      "Epoch: 108 - Loss: 0.29803290817275063\n",
      "Epoch: 109 - Loss: 0.29787783779465904\n",
      "Epoch: 110 - Loss: 0.29775280188922004\n",
      "Epoch: 111 - Loss: 0.2975956605102276\n",
      "Epoch: 112 - Loss: 0.2974542027196224\n",
      "Epoch: 113 - Loss: 0.2973500725914888\n",
      "Epoch: 114 - Loss: 0.2972366861536649\n",
      "Epoch: 115 - Loss: 0.2970887119225425\n",
      "Epoch: 116 - Loss: 0.2969488290497777\n",
      "Epoch: 117 - Loss: 0.29687769439428047\n",
      "Epoch: 118 - Loss: 0.29676028467689847\n",
      "Epoch: 119 - Loss: 0.2966278373023292\n",
      "Epoch: 120 - Loss: 0.296513052450735\n",
      "Epoch: 121 - Loss: 0.29645590522191706\n",
      "Epoch: 122 - Loss: 0.2963417617375755\n",
      "Epoch: 123 - Loss: 0.29622629690567825\n",
      "Epoch: 124 - Loss: 0.2961348887134503\n",
      "Epoch: 125 - Loss: 0.2960194467189713\n",
      "Epoch: 126 - Loss: 0.2959258848488185\n",
      "Epoch: 127 - Loss: 0.29585213620172235\n",
      "Epoch: 128 - Loss: 0.29574353392943925\n",
      "Epoch: 129 - Loss: 0.29565073580573115\n",
      "Epoch: 130 - Loss: 0.29550748635328083\n",
      "Epoch: 131 - Loss: 0.2954299356126072\n",
      "Epoch: 132 - Loss: 0.2952966175341984\n",
      "Epoch: 133 - Loss: 0.29521380353141896\n",
      "Epoch: 134 - Loss: 0.29510754481403184\n",
      "Epoch: 135 - Loss: 0.2950171989503644\n",
      "Epoch: 136 - Loss: 0.29491800764543036\n",
      "Epoch: 137 - Loss: 0.2948186328187813\n",
      "Epoch: 138 - Loss: 0.2947508247700383\n",
      "Epoch: 139 - Loss: 0.29469017203998943\n",
      "Epoch: 140 - Loss: 0.2945695430121675\n",
      "Epoch: 141 - Loss: 0.2944844070567928\n",
      "Epoch: 142 - Loss: 0.29439758068318134\n",
      "Epoch: 143 - Loss: 0.29431211844423166\n",
      "Epoch: 144 - Loss: 0.29421791486326115\n",
      "Epoch: 145 - Loss: 0.29415518697172444\n",
      "Epoch: 146 - Loss: 0.2940642424487453\n",
      "Epoch: 147 - Loss: 0.2940165762408703\n",
      "Epoch: 148 - Loss: 0.29392357134534264\n",
      "Epoch: 149 - Loss: 0.2938478101033157\n",
      "Epoch: 150 - Loss: 0.293778384161888\n",
      "Epoch: 151 - Loss: 0.2937099504865607\n",
      "Epoch: 152 - Loss: 0.2936472760963766\n",
      "Epoch: 153 - Loss: 0.2935709194197416\n",
      "Epoch: 154 - Loss: 0.2934918880200177\n",
      "Epoch: 155 - Loss: 0.29342175594862585\n",
      "Epoch: 156 - Loss: 0.29332900238316334\n",
      "Epoch: 157 - Loss: 0.29327813189619784\n",
      "Epoch: 158 - Loss: 0.293235825202428\n",
      "Epoch: 159 - Loss: 0.29318417805658736\n",
      "Epoch: 160 - Loss: 0.29313314783715255\n",
      "Epoch: 161 - Loss: 0.2931026405612483\n",
      "Epoch: 162 - Loss: 0.2930492289106659\n",
      "Epoch: 163 - Loss: 0.29301203597586156\n",
      "Epoch: 164 - Loss: 0.2929677926435638\n",
      "Epoch: 165 - Loss: 0.29292968733752484\n",
      "Epoch: 166 - Loss: 0.29289184650087907\n",
      "Epoch: 167 - Loss: 0.2928443265764403\n",
      "Epoch: 168 - Loss: 0.29281529587266986\n",
      "Epoch: 169 - Loss: 0.2927509790444252\n",
      "Epoch: 170 - Loss: 0.29272579012014394\n",
      "Epoch: 171 - Loss: 0.29267213333652714\n",
      "Epoch: 172 - Loss: 0.2926160682815975\n",
      "Epoch: 173 - Loss: 0.2925557107600215\n",
      "Epoch: 174 - Loss: 0.2924881379654063\n",
      "Epoch: 175 - Loss: 0.2924636912549038\n",
      "Epoch: 176 - Loss: 0.29241603492801926\n",
      "Epoch: 177 - Loss: 0.2923993273549359\n",
      "Epoch: 178 - Loss: 0.2923496793762019\n",
      "Epoch: 179 - Loss: 0.29233022780960016\n",
      "Epoch: 180 - Loss: 0.29229524337128665\n",
      "Epoch: 181 - Loss: 0.2922560201574047\n",
      "Epoch: 182 - Loss: 0.29224128140855765\n",
      "Epoch: 183 - Loss: 0.292217656945462\n",
      "Epoch: 184 - Loss: 0.2921934564462171\n",
      "Epoch: 185 - Loss: 0.2921322485336159\n",
      "Epoch: 186 - Loss: 0.2920866872743319\n",
      "Epoch: 187 - Loss: 0.292074169878857\n",
      "Epoch: 188 - Loss: 0.2920194239081899\n",
      "Epoch: 189 - Loss: 0.2919728812238543\n",
      "Epoch: 190 - Loss: 0.2919663072701752\n",
      "Epoch: 191 - Loss: 0.29191473306381255\n",
      "Epoch: 192 - Loss: 0.29191320884909855\n",
      "Epoch: 193 - Loss: 0.29187140163369335\n",
      "Epoch: 194 - Loss: 0.29183251068513144\n",
      "Epoch: 195 - Loss: 0.29180610242144933\n",
      "Epoch: 196 - Loss: 0.29179155853185396\n",
      "Epoch: 197 - Loss: 0.29176404714880416\n",
      "Epoch: 198 - Loss: 0.2917318259374005\n",
      "Epoch: 199 - Loss: 0.29168603453022685\n",
      "Epoch: 200 - Loss: 0.2916631516963941\n",
      "Epoch: 201 - Loss: 0.29164761547607004\n",
      "Epoch: 202 - Loss: 0.29158376381635326\n",
      "Epoch: 203 - Loss: 0.29158260968527216\n",
      "Epoch: 204 - Loss: 0.29154221492096943\n",
      "Epoch: 205 - Loss: 0.2915207367457854\n",
      "Epoch: 206 - Loss: 0.291508884471119\n",
      "Epoch: 207 - Loss: 0.2914684739917653\n",
      "Epoch: 208 - Loss: 0.291430092686309\n",
      "Epoch: 209 - Loss: 0.2914193699090581\n",
      "Epoch: 210 - Loss: 0.2913883288920383\n",
      "Epoch: 211 - Loss: 0.29136947521853546\n",
      "Epoch: 212 - Loss: 0.29134957008538803\n",
      "Epoch: 213 - Loss: 0.291324267830515\n",
      "Epoch: 214 - Loss: 0.2913001533604322\n",
      "Epoch: 215 - Loss: 0.2912553422058838\n",
      "Epoch: 216 - Loss: 0.29126147307724953\n",
      "Epoch: 217 - Loss: 0.29123482071549994\n",
      "Epoch: 218 - Loss: 0.2911948212599487\n",
      "Epoch: 219 - Loss: 0.2911513793483239\n",
      "Epoch: 220 - Loss: 0.29115017614553956\n",
      "Epoch: 221 - Loss: 0.29111981688827315\n",
      "Epoch: 222 - Loss: 0.2911231488569135\n",
      "Epoch: 223 - Loss: 0.29110762119873196\n",
      "Epoch: 224 - Loss: 0.2910963604454292\n",
      "Epoch: 225 - Loss: 0.2910913754194069\n",
      "Epoch: 226 - Loss: 0.2910869537063478\n",
      "Epoch: 227 - Loss: 0.2910520672474236\n",
      "Epoch: 228 - Loss: 0.29103495050228034\n",
      "Epoch: 229 - Loss: 0.290974125989884\n",
      "Epoch: 230 - Loss: 0.29093234036741045\n",
      "Epoch: 231 - Loss: 0.2909311589085431\n",
      "Epoch: 232 - Loss: 0.2909211764143742\n",
      "Epoch: 233 - Loss: 0.2909186745008979\n",
      "Epoch: 234 - Loss: 0.2908698095501734\n",
      "Epoch: 235 - Loss: 0.2908484386733245\n",
      "Epoch: 236 - Loss: 0.2908818830123421\n",
      "Epoch: 237 - Loss: 0.29085673847074855\n",
      "Epoch: 238 - Loss: 0.2908647186735623\n",
      "Epoch: 239 - Loss: 0.29083650220632024\n",
      "Epoch: 240 - Loss: 0.29081580842552873\n",
      "Epoch: 241 - Loss: 0.29080331939289206\n",
      "Epoch: 242 - Loss: 0.290751893329539\n",
      "Epoch: 243 - Loss: 0.2907548588813757\n",
      "Epoch: 244 - Loss: 0.2907225297467797\n",
      "Epoch: 245 - Loss: 0.2907172461799506\n",
      "Epoch: 246 - Loss: 0.29071205476034623\n",
      "Epoch: 247 - Loss: 0.29067549841504847\n",
      "Epoch: 248 - Loss: 0.2906706724892438\n",
      "Epoch: 249 - Loss: 0.29064872077181475\n",
      "Epoch: 250 - Loss: 0.2906009602109009\n",
      "Epoch: 251 - Loss: 0.2905861740345258\n",
      "Epoch: 252 - Loss: 0.29058500930186115\n",
      "Epoch: 253 - Loss: 0.29057731498071365\n",
      "Epoch: 254 - Loss: 0.2905432179549127\n",
      "Epoch: 255 - Loss: 0.2905468808135735\n",
      "Epoch: 256 - Loss: 0.2904988140847063\n",
      "Epoch: 257 - Loss: 0.29047045087414175\n",
      "Epoch: 258 - Loss: 0.29046403875165594\n",
      "Epoch: 259 - Loss: 0.2904340255589862\n",
      "Epoch: 260 - Loss: 0.29043325015761284\n",
      "Epoch: 261 - Loss: 0.29040845489465866\n",
      "Epoch: 262 - Loss: 0.2904062471616034\n",
      "Epoch: 263 - Loss: 0.2903811905923475\n",
      "Epoch: 264 - Loss: 0.2903520104782438\n",
      "Epoch: 265 - Loss: 0.29034628019363623\n",
      "Epoch: 266 - Loss: 0.2903247669392483\n",
      "Epoch: 267 - Loss: 0.2903294601759798\n",
      "Epoch: 268 - Loss: 0.290303672097189\n",
      "Epoch: 269 - Loss: 0.2902814991961947\n",
      "Epoch: 270 - Loss: 0.2902927154504316\n",
      "Epoch: 271 - Loss: 0.29025868013008504\n",
      "Epoch: 272 - Loss: 0.290271351383827\n",
      "Epoch: 273 - Loss: 0.2902737966020668\n",
      "Epoch: 274 - Loss: 0.2902214399842465\n",
      "Epoch: 275 - Loss: 0.2902229776182354\n",
      "Epoch: 276 - Loss: 0.29022467318571316\n",
      "Epoch: 277 - Loss: 0.2901831658861678\n",
      "Epoch: 278 - Loss: 0.2901667672577662\n",
      "Epoch: 279 - Loss: 0.2901659955768083\n",
      "Epoch: 280 - Loss: 0.2901469811171392\n",
      "Epoch: 281 - Loss: 0.290145836633874\n",
      "Epoch: 282 - Loss: 0.2901329778524211\n",
      "Epoch: 283 - Loss: 0.2900986945954263\n",
      "Epoch: 284 - Loss: 0.29013087443405433\n",
      "Epoch: 285 - Loss: 0.29011449584946414\n",
      "Epoch: 286 - Loss: 0.2901122738502089\n",
      "Epoch: 287 - Loss: 0.2901079658598788\n",
      "Epoch: 288 - Loss: 0.2901163369049794\n",
      "Epoch: 289 - Loss: 0.2901042436283429\n",
      "Epoch: 290 - Loss: 0.29007337233211156\n",
      "Epoch: 291 - Loss: 0.2901040978107736\n",
      "Epoch: 292 - Loss: 0.29003623523938\n",
      "Epoch: 293 - Loss: 0.2900342354357274\n",
      "Epoch: 294 - Loss: 0.2900376170679358\n",
      "Epoch: 295 - Loss: 0.29000259714404714\n",
      "Epoch: 296 - Loss: 0.28998428173692287\n",
      "Epoch: 297 - Loss: 0.28998119418731033\n",
      "Epoch: 298 - Loss: 0.2899627226484378\n",
      "Epoch: 299 - Loss: 0.28993239781546387\n",
      "Epoch: 300 - Loss: 0.2899408331207754\n",
      "Epoch: 301 - Loss: 0.28991433741473516\n",
      "Epoch: 302 - Loss: 0.2899305577652943\n",
      "Epoch: 303 - Loss: 0.2899345117534536\n",
      "Epoch: 304 - Loss: 0.28994618968781055\n",
      "Epoch: 305 - Loss: 0.2899273494872567\n",
      "Epoch: 306 - Loss: 0.28991720066213994\n",
      "Epoch: 307 - Loss: 0.28992663006069214\n",
      "Epoch: 308 - Loss: 0.28988439735870813\n",
      "Epoch: 309 - Loss: 0.28992879937673544\n",
      "Epoch: 310 - Loss: 0.2898842036551783\n",
      "Epoch: 311 - Loss: 0.2899025736646476\n",
      "Epoch: 312 - Loss: 0.2899348785124829\n",
      "Epoch: 313 - Loss: 0.28989983614817566\n",
      "Epoch: 314 - Loss: 0.2898915202518623\n",
      "Epoch: 315 - Loss: 0.28986944690015287\n",
      "Epoch: 316 - Loss: 0.28985490924728136\n",
      "Epoch: 317 - Loss: 0.2898677985099486\n",
      "Epoch: 318 - Loss: 0.2898696960215035\n",
      "Epoch: 319 - Loss: 0.28984300810569613\n",
      "Epoch: 320 - Loss: 0.2898612723092015\n",
      "Epoch: 321 - Loss: 0.28984156402615024\n",
      "Epoch: 322 - Loss: 0.2898287349919581\n",
      "Epoch: 323 - Loss: 0.2898245494891095\n",
      "Epoch: 324 - Loss: 0.28978414908313727\n",
      "Epoch: 325 - Loss: 0.28976988997004927\n",
      "Epoch: 326 - Loss: 0.2897532712994111\n",
      "Epoch: 327 - Loss: 0.2897510258079573\n",
      "Epoch: 328 - Loss: 0.2897519443940675\n",
      "Epoch: 329 - Loss: 0.28974785409336434\n",
      "Epoch: 330 - Loss: 0.28975198142651387\n",
      "Epoch: 331 - Loss: 0.28975408149164167\n",
      "Epoch: 332 - Loss: 0.2897439897141938\n",
      "Epoch: 333 - Loss: 0.28974317955439516\n",
      "Epoch: 334 - Loss: 0.28972167688749995\n",
      "Epoch: 335 - Loss: 0.28975172687871104\n",
      "Epoch: 336 - Loss: 0.2897249716748903\n",
      "Epoch: 337 - Loss: 0.28970856893884195\n",
      "Epoch: 338 - Loss: 0.2897296951807938\n",
      "Epoch: 339 - Loss: 0.2897124764462231\n",
      "Epoch: 340 - Loss: 0.28969129974226265\n",
      "Epoch: 341 - Loss: 0.2897093985914534\n",
      "Epoch: 342 - Loss: 0.28965019591401925\n",
      "Epoch: 343 - Loss: 0.28965373797647215\n",
      "Epoch: 344 - Loss: 0.2896843123648631\n",
      "Epoch: 345 - Loss: 0.28964718494654623\n",
      "Epoch: 346 - Loss: 0.28963689652972985\n",
      "Epoch: 347 - Loss: 0.2896332522282227\n",
      "Epoch: 348 - Loss: 0.2896067785076362\n",
      "Epoch: 349 - Loss: 0.2896113065579157\n",
      "Epoch: 350 - Loss: 0.2896122675402087\n",
      "Epoch: 351 - Loss: 0.2896115318496311\n",
      "Epoch: 352 - Loss: 0.28958592211007583\n",
      "Epoch: 353 - Loss: 0.2895759016283419\n",
      "Epoch: 354 - Loss: 0.2895569320122375\n",
      "Epoch: 355 - Loss: 0.2895679953027775\n",
      "Epoch: 356 - Loss: 0.28958532660865427\n",
      "Epoch: 357 - Loss: 0.2895389023630888\n",
      "Epoch: 358 - Loss: 0.28954576664191384\n",
      "Epoch: 359 - Loss: 0.28957364009699105\n",
      "Epoch: 360 - Loss: 0.2895626972737745\n",
      "Epoch: 361 - Loss: 0.2895503665914278\n",
      "Epoch: 362 - Loss: 0.28953879830401946\n",
      "Epoch: 363 - Loss: 0.28956347873004534\n",
      "Epoch: 364 - Loss: 0.28955229270031857\n",
      "Epoch: 365 - Loss: 0.2895481653318632\n",
      "Epoch: 366 - Loss: 0.2895519018327887\n",
      "Epoch: 367 - Loss: 0.28954284229315264\n",
      "Epoch: 368 - Loss: 0.2895633031867955\n",
      "Epoch: 369 - Loss: 0.2895573767952538\n",
      "Epoch: 370 - Loss: 0.28951366454163147\n",
      "Epoch: 371 - Loss: 0.2895272487141672\n",
      "Epoch: 372 - Loss: 0.28952110161040573\n",
      "Epoch: 373 - Loss: 0.2895682723011547\n",
      "Epoch: 374 - Loss: 0.28955871530493615\n",
      "Epoch: 375 - Loss: 0.2895238967221985\n",
      "Epoch: 376 - Loss: 0.28951284590715626\n",
      "Epoch: 377 - Loss: 0.28954176421694233\n",
      "Epoch: 378 - Loss: 0.28951108888727556\n",
      "Epoch: 379 - Loss: 0.2895041246454489\n",
      "Epoch: 380 - Loss: 0.289468858210899\n",
      "Epoch: 381 - Loss: 0.289492352212427\n",
      "Epoch: 382 - Loss: 0.2894619073593178\n",
      "Epoch: 383 - Loss: 0.28947207614310183\n",
      "Epoch: 384 - Loss: 0.28947255989521153\n",
      "Epoch: 385 - Loss: 0.2894086753052458\n",
      "Epoch: 386 - Loss: 0.28943746979238205\n",
      "Epoch: 387 - Loss: 0.28946835310478275\n",
      "Epoch: 388 - Loss: 0.2894178441493619\n",
      "Epoch: 389 - Loss: 0.2894035381375911\n",
      "Epoch: 390 - Loss: 0.28939222242284574\n",
      "Epoch: 391 - Loss: 0.28939423912135076\n",
      "Epoch: 392 - Loss: 0.28941773671630444\n",
      "Epoch: 393 - Loss: 0.2894088997674476\n",
      "Epoch: 394 - Loss: 0.2893923061691284\n",
      "Epoch: 395 - Loss: 0.28936257233363194\n",
      "Epoch: 396 - Loss: 0.28939607111553767\n",
      "Epoch: 397 - Loss: 0.28934447450772\n",
      "Epoch: 398 - Loss: 0.2893315349370391\n",
      "Epoch: 399 - Loss: 0.2893070542390603\n",
      "Epoch: 400 - Loss: 0.2893228938552177\n",
      "Epoch: 401 - Loss: 0.28931447917861663\n",
      "Epoch: 402 - Loss: 0.2893125140962524\n",
      "Epoch: 403 - Loss: 0.2893102024202126\n",
      "Epoch: 404 - Loss: 0.28932295110647177\n",
      "Epoch: 405 - Loss: 0.28931319810210787\n",
      "Epoch: 406 - Loss: 0.28927589649419955\n",
      "Epoch: 407 - Loss: 0.2893021653443796\n",
      "Epoch: 408 - Loss: 0.2892645980076333\n",
      "Epoch: 409 - Loss: 0.2892623965838786\n",
      "Epoch: 410 - Loss: 0.28929929308185376\n",
      "Epoch: 411 - Loss: 0.28926617976621505\n",
      "Epoch: 412 - Loss: 0.28927468476353935\n",
      "Epoch: 413 - Loss: 0.2892449505863996\n",
      "Epoch: 414 - Loss: 0.2892449637585963\n",
      "Epoch: 415 - Loss: 0.2892202881753638\n",
      "Epoch: 416 - Loss: 0.2892416571063555\n",
      "Epoch: 417 - Loss: 0.2892263230196756\n",
      "Epoch: 418 - Loss: 0.28923023823528793\n",
      "Epoch: 419 - Loss: 0.2891860647914617\n",
      "Epoch: 420 - Loss: 0.2892064992747488\n",
      "Epoch: 421 - Loss: 0.2891695918849293\n",
      "Epoch: 422 - Loss: 0.28919534081340786\n",
      "Epoch: 423 - Loss: 0.2891686309463401\n",
      "Epoch: 424 - Loss: 0.28917279528940565\n",
      "Epoch: 425 - Loss: 0.28913742993276365\n",
      "Epoch: 426 - Loss: 0.2891655993999814\n",
      "Epoch: 427 - Loss: 0.28913079619538495\n",
      "Epoch: 428 - Loss: 0.28917892596321876\n",
      "Epoch: 429 - Loss: 0.28912460827329733\n",
      "Epoch: 430 - Loss: 0.28914419835093247\n",
      "Epoch: 431 - Loss: 0.2891394148309098\n",
      "Epoch: 432 - Loss: 0.28914300390448683\n",
      "Epoch: 433 - Loss: 0.2891120539871432\n",
      "Epoch: 434 - Loss: 0.2891118961105416\n",
      "Epoch: 435 - Loss: 0.28915130570326825\n",
      "Epoch: 436 - Loss: 0.2891218568390349\n",
      "Epoch: 437 - Loss: 0.289069450540437\n",
      "Epoch: 438 - Loss: 0.2891070458004903\n",
      "Epoch: 439 - Loss: 0.2890907624064324\n",
      "Epoch: 440 - Loss: 0.2890859943472545\n",
      "Epoch: 441 - Loss: 0.28909080561021233\n",
      "Epoch: 442 - Loss: 0.2890852176412699\n",
      "Epoch: 443 - Loss: 0.28905026352141094\n",
      "Epoch: 444 - Loss: 0.2890542636720949\n",
      "Epoch: 445 - Loss: 0.28907246363436573\n",
      "Epoch: 446 - Loss: 0.2890481114073121\n",
      "Epoch: 447 - Loss: 0.2890545903184894\n",
      "Epoch: 448 - Loss: 0.2890615644594016\n",
      "Epoch: 449 - Loss: 0.289040864628991\n",
      "Epoch: 450 - Loss: 0.28902220304672144\n",
      "Epoch: 451 - Loss: 0.2890639791971282\n",
      "Epoch: 452 - Loss: 0.2890217282657919\n",
      "Epoch: 453 - Loss: 0.2890065066854273\n",
      "Epoch: 454 - Loss: 0.2890066457994472\n",
      "Epoch: 455 - Loss: 0.2890218903369909\n",
      "Epoch: 456 - Loss: 0.28897122457339314\n",
      "Epoch: 457 - Loss: 0.2889842243603588\n",
      "Epoch: 458 - Loss: 0.2889872705610629\n",
      "Epoch: 459 - Loss: 0.2890058162880856\n",
      "Epoch: 460 - Loss: 0.28896339400543136\n",
      "Epoch: 461 - Loss: 0.28896211594258125\n",
      "Epoch: 462 - Loss: 0.28896766906095783\n",
      "Epoch: 463 - Loss: 0.2889654454119006\n",
      "Epoch: 464 - Loss: 0.2889609077881534\n",
      "Epoch: 465 - Loss: 0.28889804564798754\n",
      "Epoch: 466 - Loss: 0.28897080329595104\n",
      "Epoch: 467 - Loss: 0.2889521835611236\n",
      "Epoch: 468 - Loss: 0.28891774745686877\n",
      "Epoch: 469 - Loss: 0.2889490669287967\n",
      "Epoch: 470 - Loss: 0.28892079399687853\n",
      "Epoch: 471 - Loss: 0.28894474086915883\n",
      "Epoch: 472 - Loss: 0.2889292931187762\n",
      "Epoch: 473 - Loss: 0.2889628132619805\n",
      "Epoch: 474 - Loss: 0.2889501350451674\n",
      "Epoch: 475 - Loss: 0.28890893493663605\n",
      "Epoch: 476 - Loss: 0.2888859553277765\n",
      "Epoch: 477 - Loss: 0.28891473888471536\n",
      "Epoch: 478 - Loss: 0.2888883897818318\n",
      "Epoch: 479 - Loss: 0.28895880895135123\n",
      "Epoch: 480 - Loss: 0.2889020786344056\n",
      "Epoch: 481 - Loss: 0.2888874945087447\n",
      "Epoch: 482 - Loss: 0.28891972899358986\n",
      "Epoch: 483 - Loss: 0.2888864211930133\n",
      "Epoch: 484 - Loss: 0.2888742509501743\n",
      "Epoch: 485 - Loss: 0.28888912168922887\n",
      "Epoch: 486 - Loss: 0.28889475647534346\n",
      "Epoch: 487 - Loss: 0.28887334617739546\n",
      "Epoch: 488 - Loss: 0.2888723646223121\n",
      "Epoch: 489 - Loss: 0.2888495218987346\n",
      "Epoch: 490 - Loss: 0.28882057177848686\n",
      "Epoch: 491 - Loss: 0.28883653149821037\n",
      "Epoch: 492 - Loss: 0.2888372751489617\n",
      "Epoch: 493 - Loss: 0.2888114014451556\n",
      "Epoch: 494 - Loss: 0.2888707626202154\n",
      "Epoch: 495 - Loss: 0.28883859676639884\n",
      "Epoch: 496 - Loss: 0.2888225562801809\n",
      "Epoch: 497 - Loss: 0.28881590911944627\n",
      "Epoch: 498 - Loss: 0.2888545760213328\n",
      "Epoch: 499 - Loss: 0.28878430365872215\n",
      "Epoch: 500 - Loss: 0.28881512338876897\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.classify(pred_file + \".txt\")\n",
    "# preds, t1, t2 = model.classify(pred_file + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "\n",
    "# products\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    3844\n",
       "neg    2670\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    3354\n",
       "pos    3160\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.58%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
