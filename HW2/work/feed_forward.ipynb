{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file, threshold=0, max_features=None):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "        # self.vocabulary = list(self.embedding_matrix.keys())\n",
    "        # self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        # self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features-1))\n",
    "\n",
    "        # Add to vocabulary the Out-of-Vocabulary token\n",
    "        return list(flattened_list_count_filter.keys()) + ['UNK']\n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        # Create Vocabulary\n",
    "        self.vocabulary = self.create_vocabulary(tokenized_text, self.threshold, self.max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)\n",
    "    \n",
    "    def sort_by_tfidf(self, tfidf_matrix, max_seq_length):\n",
    "        \"\"\"Sort input documents based on tf*idf score.\n",
    "        Return top \"max_seq_length\" words\n",
    "        :param: tfidf_matrix\n",
    "        :param: max_seq_length\n",
    "        :return: sentences ordered by TF-IDF score\n",
    "        \"\"\"\n",
    "        \n",
    "        # Indices of sorted matrix in descending order\n",
    "        indices = np.argsort(-tfidf_matrix, axis=1)\n",
    "        tfidf_matrix_sorted = []\n",
    "\n",
    "        # Create sorted matrix\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            # sentence in orderd version\n",
    "            tmp = [self.index2word[index] for index in indices[i][:max_seq_length]]\n",
    "            tfidf_matrix_sorted.append(tmp)\n",
    "    \n",
    "        return tfidf_matrix_sorted\n",
    "    \n",
    "    def get_features_tfidf(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothinf\n",
    "        return tf*idf_array\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from nn_layers import FeedForwardNetwork\n",
    "\n",
    "class NeuralModel(Model):\n",
    "    def __init__(self, embeddingfile, max_seq_length, hidden_units, minibatch_size, learning_rate, epochs, tfidf=False, max_features=None, threshold=0, momentum=0): \n",
    "        # self.network = FeedForwardNetwork()\n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_units = hidden_units\n",
    "        self.weights_1 = None\n",
    "        self.bias_1 = None\n",
    "        self.weights_2 = None\n",
    "        self.bias_2 = None\n",
    "        self.Y_to_categorical = None\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = {}\n",
    "        # TF-IDF Sorting\n",
    "        self.tfidf = tfidf # enable sorting by tf-idf score\n",
    "        self.max_features = max_features\n",
    "        self.threshold = threshold\n",
    "        # Momentum\n",
    "        self.momentum = momentum\n",
    "        self.prev_dW_1 = 0\n",
    "        self.prev_db_1 = 0\n",
    "        self.prev_dW_2 = 0\n",
    "        self.prev_db_2 = 0\n",
    "    \n",
    "    def initialize_weights(self, n_inputs, n_output):\n",
    "        # weights = np.zeros((n_inputs, n_output))\n",
    "        # bias = np.zeros(n_output)\n",
    "        # np.random.seed(0)\n",
    "        weights = np.random.rand(n_inputs, n_output)\n",
    "        bias = np.random.rand(n_output)\n",
    "        return weights, bias\n",
    "    \n",
    "    def relu_function(self, A):\n",
    "        '''A = x*W + b\n",
    "\n",
    "        :return: Z = relut(x*A+b)\n",
    "        '''\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def relu_derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        A = np.dot(X, self.weights_1) + self.bias_1\n",
    "        h = self.relu_function(A)\n",
    "\n",
    "        A_2 = np.dot(h, self.weights_2) + self.bias_2\n",
    "\n",
    "        O = self.softmax(A_2)\n",
    "\n",
    "        # Rows with highest probability\n",
    "        S_max = np.argmax(O, axis=1)\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "    def convert_to_embeddings(self, sentence):\n",
    "        '''Convert sentence to embeddings\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "            # try:\n",
    "        if emb: # if there is a word\n",
    "            emb_concat = np.concatenate(emb, axis=0)\n",
    "        else:\n",
    "            emb_concat = []\n",
    "        # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "        if len(emb) < self.max_seq_length:\n",
    "            # Missing words\n",
    "            words_missing = self.max_seq_length - len(emb)\n",
    "            # print(words_missing)\n",
    "            emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile, threshold=self.threshold, max_features=self.max_features)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        # Convert to OneHot for computing Loss\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim × features (u)\n",
    "        n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        print(\"Computing TFIDF\")\n",
    "        if self.tfidf: # Truncate input to the max sequence length sorted by TF-IDF\n",
    "            tf_idf = features_ff_class.tf_idf(features_ff_class.tokenized_text)\n",
    "            trunc_tokenized_text = features_ff_class.sort_by_tfidf(\n",
    "                tf_idf,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "                features_ff_class.tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        print(\"Computing Embedding\")\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Wieghts\n",
    "        # Create W_a and b_a\n",
    "        # W_a[n_documents, hidden_units (u)]\n",
    "        # b_a[hidden_units (u)]\n",
    "        W_1, b_1 = self.initialize_weights(n_inputs, self.hidden_units)\n",
    "        # Create Wb and b_b\n",
    "        # W_b[hidden_units (u), num_labels (d)]\n",
    "        # b_b[num_labels]\n",
    "        W_2, b_2 = self.initialize_weights(self.hidden_units, num_labels)\n",
    "\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        self.weights_1 = W_1\n",
    "        self.bias_1 = b_1\n",
    "        self.weights_2 = W_2\n",
    "        self.bias_2 = b_2\n",
    "        for i in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "            \n",
    "                # ---------------- Input-to-Hidden Layer --------------- #\n",
    "                # Z1 = W_a*X + b_a\n",
    "                # Z1[n_documents, hidden_units (u)]\n",
    "                Z_1 = np.dot(X_mini_batch, self.weights_1) + self.bias_1\n",
    "                # Hidden Unit\n",
    "                # h = relu(A)\n",
    "                # h[n_documents, hidden_units (u)]\n",
    "                A_1 = self.relu_function(Z_1)\n",
    "\n",
    "                # ---------------- Hidden-to-Output Layer --------------- #\n",
    "                #  = W_b*h + b_b\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                Z_2 = np.dot(A_1, self.weights_2) + self.bias_2\n",
    "                # Output Layer\n",
    "                # A_2 = softmax(Z_2)\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                A_2 = self.softmax(Z_2)\n",
    "                # print(A_2)\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "\n",
    "                # Compute Gradients\n",
    "\n",
    "                dZ_2 = A_2 - y_mini_batch # [n_documents, num_labels (d)]\n",
    "                # np.dot(A_2, dZ_2) => (hidden_units, n_documents) X (n_documents, num_labels) = (hidden_units, num_labels)\n",
    "                dW_2 = (1/minibatch_size)*np.dot(A_1.T, dZ_2)\n",
    "                db_2 = (1/minibatch_size)*np.sum(dZ_2, axis=0, keepdims = True) # [num_labels]\n",
    "                # np.dot(self.weights_b, dZ_2) => [n_documents, num_labels (d)] X [num_labels (d), hidden_units (u)] => [n_documents, hidden_units]\n",
    "                dZ_1 = np.dot(dZ_2, self.weights_2.T)*self.relu_derivative(Z_1)\n",
    "                # np.dot(X, dZ_1) => (features, n_documents) X (n_documents, hidden_units) = (hidden_units, num_labels)\n",
    "                dW_1 = (1/minibatch_size)*np.dot(X_mini_batch.T, dZ_1)\n",
    "                db_1 = (1/minibatch_size)*np.sum(dZ_1, axis=0, keepdims = True) # [hidden_units]\n",
    "\n",
    "                # Update weights\n",
    "                self.weights_1 = self.weights_1 - (self.learning_rate*dW_1 + self.momentum*self.prev_dW_1)\n",
    "                self.bias_1 = self.bias_1 - (self.learning_rate*db_1 + self.momentum*self.prev_db_1)\n",
    "                self.weights_2 = self.weights_2 - (self.learning_rate*dW_2 + self.momentum*self.prev_dW_2)\n",
    "                self.bias_2 = self.bias_2 - (self.learning_rate*db_2 + self.momentum*self.prev_db_2)\n",
    "\n",
    "                # Save previous gradients for Momentum\n",
    "                self.prev_dW_1 = (self.learning_rate*dW_1 + self.momentum*self.prev_dW_1)\n",
    "                self.prev_db_1 = (self.learning_rate*db_1 + self.momentum*self.prev_db_1)\n",
    "                self.prev_dW_2 = (self.learning_rate*dW_2 + self.momentum*self.prev_dW_2)\n",
    "                self.prev_db_2 = (self.learning_rate*db_2 + self.momentum*self.prev_db_2)\n",
    "\n",
    "                ########\n",
    "                # Loss #\n",
    "                ########\n",
    "                mini_batch_loss.append(self.cross_entropy_loss(A_2, y_mini_batch))\n",
    "\n",
    "            loss = np.mean(mini_batch_loss)\n",
    "            self.loss[i] = loss\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        if self.tfidf:\n",
    "            tf_idf_inference = []\n",
    "            # Get features from inference file\n",
    "            for sentence in tokenized_text:\n",
    "                # Transform dataset to TF-IDF space\n",
    "                # Return features with format (1, size_vocabulary)\n",
    "                X_sentence = self.features_ff_class.get_features_tfidf(sentence, self.features_ff_class.idf)\n",
    "                tf_idf_inference.append(X_sentence)\n",
    "            tf_idf_inference = np.stack(tf_idf_inference)\n",
    "            trunc_tokenized_text = self.features_ff_class.sort_by_tfidf(\n",
    "                tf_idf_inference,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "                tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Make Prediction\n",
    "        y_test = self.predict(X_test)\n",
    "        preds_label = []\n",
    "        for y in y_test:\n",
    "            tmp = self.Y_to_categorical[y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "train_file = \"datasets/products/train.txt\"\n",
    "emb_file = \"glove.6B.50d.txt\"\n",
    "pred_file = \"datasets/products/val.test\"\n",
    "pred_true_labels = \"datasets/products/val.txt\"\n",
    "model_file_name = \"products.model\"\n",
    "loss_file = \"datasets/products/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4) # 65.6%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=256, learning_rate=0.1, epochs=500, tfidf=True, max_features=500, threshold=2) # 64%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2)\n",
    "nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=5, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.1) # 65%\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=32, learning_rate=0.05, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=128, learning_rate=0.05, epochs=100, tfidf=True, threshold=2, max_features=1000)\n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "# train_file = \"datasets/odiya/train.txt\"\n",
    "# emb_file = \"fasttext.wiki.300d.vec\"\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "# model_file_name = \"odiya.model\"\n",
    "# loss_file = \"datasets/odiya/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF\n",
      "Computing Embedding\n",
      "Epoch: 1 - Loss: 0.6069923927538446\n",
      "Epoch: 2 - Loss: 0.33839128023313725\n",
      "Epoch: 3 - Loss: 0.33791682701400566\n",
      "Epoch: 4 - Loss: 0.33775458262536007\n",
      "Epoch: 5 - Loss: 0.33763091239872833\n",
      "Epoch: 6 - Loss: 0.337506259455302\n",
      "Epoch: 7 - Loss: 0.33741230841183806\n",
      "Epoch: 8 - Loss: 0.3372946785714306\n",
      "Epoch: 9 - Loss: 0.3371443258952316\n",
      "Epoch: 10 - Loss: 0.33691814669992337\n",
      "Epoch: 11 - Loss: 0.3366057325337387\n",
      "Epoch: 12 - Loss: 0.3362373465396263\n",
      "Epoch: 13 - Loss: 0.3359382918307077\n",
      "Epoch: 14 - Loss: 0.3355946337652542\n",
      "Epoch: 15 - Loss: 0.3352664002201429\n",
      "Epoch: 16 - Loss: 0.3349722954333685\n",
      "Epoch: 17 - Loss: 0.33467559983362\n",
      "Epoch: 18 - Loss: 0.3343480996624657\n",
      "Epoch: 19 - Loss: 0.3339982248034762\n",
      "Epoch: 20 - Loss: 0.3336559334677548\n",
      "Epoch: 21 - Loss: 0.33330985873167157\n",
      "Epoch: 22 - Loss: 0.3329589791233159\n",
      "Epoch: 23 - Loss: 0.3326317733659989\n",
      "Epoch: 24 - Loss: 0.3322725253482785\n",
      "Epoch: 25 - Loss: 0.3319026167043369\n",
      "Epoch: 26 - Loss: 0.33154287243118746\n",
      "Epoch: 27 - Loss: 0.33116972792794447\n",
      "Epoch: 28 - Loss: 0.3308116034862904\n",
      "Epoch: 29 - Loss: 0.33041052888836575\n",
      "Epoch: 30 - Loss: 0.33000563768670993\n",
      "Epoch: 31 - Loss: 0.3295761649448837\n",
      "Epoch: 32 - Loss: 0.3291685191393235\n",
      "Epoch: 33 - Loss: 0.3287882676587906\n",
      "Epoch: 34 - Loss: 0.32838329225766\n",
      "Epoch: 35 - Loss: 0.3279668948701837\n",
      "Epoch: 36 - Loss: 0.32757328344939085\n",
      "Epoch: 37 - Loss: 0.32717326954449566\n",
      "Epoch: 38 - Loss: 0.3267656177328801\n",
      "Epoch: 39 - Loss: 0.32632136180291027\n",
      "Epoch: 40 - Loss: 0.32589687744098467\n",
      "Epoch: 41 - Loss: 0.3254537490395325\n",
      "Epoch: 42 - Loss: 0.32503051722735177\n",
      "Epoch: 43 - Loss: 0.32463416589009986\n",
      "Epoch: 44 - Loss: 0.324236254540648\n",
      "Epoch: 45 - Loss: 0.3238478214468769\n",
      "Epoch: 46 - Loss: 0.3234463131523043\n",
      "Epoch: 47 - Loss: 0.32308376341859074\n",
      "Epoch: 48 - Loss: 0.3226708776123497\n",
      "Epoch: 49 - Loss: 0.3222870582827092\n",
      "Epoch: 50 - Loss: 0.32188401198743527\n",
      "Epoch: 51 - Loss: 0.3215133696453271\n",
      "Epoch: 52 - Loss: 0.3211032517495501\n",
      "Epoch: 53 - Loss: 0.320749070478988\n",
      "Epoch: 54 - Loss: 0.32038911418781857\n",
      "Epoch: 55 - Loss: 0.3200382081954761\n",
      "Epoch: 56 - Loss: 0.3196754931488185\n",
      "Epoch: 57 - Loss: 0.3193147862356983\n",
      "Epoch: 58 - Loss: 0.31899254922259185\n",
      "Epoch: 59 - Loss: 0.3186663378429267\n",
      "Epoch: 60 - Loss: 0.3184162064637341\n",
      "Epoch: 61 - Loss: 0.31808289207074625\n",
      "Epoch: 62 - Loss: 0.3178443479836877\n",
      "Epoch: 63 - Loss: 0.3175246917638008\n",
      "Epoch: 64 - Loss: 0.3172187227678333\n",
      "Epoch: 65 - Loss: 0.3169727003362841\n",
      "Epoch: 66 - Loss: 0.31670614550675524\n",
      "Epoch: 67 - Loss: 0.31643969501770897\n",
      "Epoch: 68 - Loss: 0.3161989451042748\n",
      "Epoch: 69 - Loss: 0.3158710077867982\n",
      "Epoch: 70 - Loss: 0.3155101688066824\n",
      "Epoch: 71 - Loss: 0.3152266407887509\n",
      "Epoch: 72 - Loss: 0.31493138118277847\n",
      "Epoch: 73 - Loss: 0.3146149396807753\n",
      "Epoch: 74 - Loss: 0.3143421382746442\n",
      "Epoch: 75 - Loss: 0.3140239747107673\n",
      "Epoch: 76 - Loss: 0.3137760236614441\n",
      "Epoch: 77 - Loss: 0.3134603891978312\n",
      "Epoch: 78 - Loss: 0.31318353726013853\n",
      "Epoch: 79 - Loss: 0.31295693287180576\n",
      "Epoch: 80 - Loss: 0.3126479671814158\n",
      "Epoch: 81 - Loss: 0.3123724456341703\n",
      "Epoch: 82 - Loss: 0.3121321929824483\n",
      "Epoch: 83 - Loss: 0.31194353633135435\n",
      "Epoch: 84 - Loss: 0.3116401951270187\n",
      "Epoch: 85 - Loss: 0.31140362461519167\n",
      "Epoch: 86 - Loss: 0.31104302291817976\n",
      "Epoch: 87 - Loss: 0.31078983047190106\n",
      "Epoch: 88 - Loss: 0.3105597737551137\n",
      "Epoch: 89 - Loss: 0.3102937483432549\n",
      "Epoch: 90 - Loss: 0.3099764288426199\n",
      "Epoch: 91 - Loss: 0.30969279530773414\n",
      "Epoch: 92 - Loss: 0.3093991743528646\n",
      "Epoch: 93 - Loss: 0.3091004768963348\n",
      "Epoch: 94 - Loss: 0.3088589845056593\n",
      "Epoch: 95 - Loss: 0.30864823824926785\n",
      "Epoch: 96 - Loss: 0.3084225937672949\n",
      "Epoch: 97 - Loss: 0.308156122032597\n",
      "Epoch: 98 - Loss: 0.3078892615884673\n",
      "Epoch: 99 - Loss: 0.30766859639441607\n",
      "Epoch: 100 - Loss: 0.3073749440732666\n",
      "Epoch: 101 - Loss: 0.3071434916898317\n",
      "Epoch: 102 - Loss: 0.30689675047720144\n",
      "Epoch: 103 - Loss: 0.3065777037601314\n",
      "Epoch: 104 - Loss: 0.3063340005542745\n",
      "Epoch: 105 - Loss: 0.30613044186294486\n",
      "Epoch: 106 - Loss: 0.30597999167434403\n",
      "Epoch: 107 - Loss: 0.30570947657312\n",
      "Epoch: 108 - Loss: 0.3054980647651938\n",
      "Epoch: 109 - Loss: 0.30530139458624483\n",
      "Epoch: 110 - Loss: 0.30507209912637057\n",
      "Epoch: 111 - Loss: 0.30484264789497756\n",
      "Epoch: 112 - Loss: 0.3046268722299215\n",
      "Epoch: 113 - Loss: 0.30442161134061885\n",
      "Epoch: 114 - Loss: 0.3042462728037897\n",
      "Epoch: 115 - Loss: 0.30407731309079056\n",
      "Epoch: 116 - Loss: 0.30387981754739335\n",
      "Epoch: 117 - Loss: 0.30364081711166285\n",
      "Epoch: 118 - Loss: 0.303440292858426\n",
      "Epoch: 119 - Loss: 0.30323552911048085\n",
      "Epoch: 120 - Loss: 0.30298869242419296\n",
      "Epoch: 121 - Loss: 0.3027844909380574\n",
      "Epoch: 122 - Loss: 0.3026040199160759\n",
      "Epoch: 123 - Loss: 0.30246055207917716\n",
      "Epoch: 124 - Loss: 0.30225943346060086\n",
      "Epoch: 125 - Loss: 0.3020930090170701\n",
      "Epoch: 126 - Loss: 0.301905165741618\n",
      "Epoch: 127 - Loss: 0.30170761310887373\n",
      "Epoch: 128 - Loss: 0.301551983717789\n",
      "Epoch: 129 - Loss: 0.3013517527215685\n",
      "Epoch: 130 - Loss: 0.3012122892952676\n",
      "Epoch: 131 - Loss: 0.3009935996208719\n",
      "Epoch: 132 - Loss: 0.30085869353714545\n",
      "Epoch: 133 - Loss: 0.30073065182451686\n",
      "Epoch: 134 - Loss: 0.3005702418507758\n",
      "Epoch: 135 - Loss: 0.3003646456179832\n",
      "Epoch: 136 - Loss: 0.3001842112527604\n",
      "Epoch: 137 - Loss: 0.3000029795553898\n",
      "Epoch: 138 - Loss: 0.2998258287814958\n",
      "Epoch: 139 - Loss: 0.29966323541005435\n",
      "Epoch: 140 - Loss: 0.2994853376261541\n",
      "Epoch: 141 - Loss: 0.29934243400891103\n",
      "Epoch: 142 - Loss: 0.29915238253462106\n",
      "Epoch: 143 - Loss: 0.2989921028373765\n",
      "Epoch: 144 - Loss: 0.29886772526004735\n",
      "Epoch: 145 - Loss: 0.2987298858756998\n",
      "Epoch: 146 - Loss: 0.29856379630430446\n",
      "Epoch: 147 - Loss: 0.29843128522531537\n",
      "Epoch: 148 - Loss: 0.2982853946849702\n",
      "Epoch: 149 - Loss: 0.2981438211063151\n",
      "Epoch: 150 - Loss: 0.2979972888983161\n",
      "Epoch: 151 - Loss: 0.29783976124255496\n",
      "Epoch: 152 - Loss: 0.2976925500287764\n",
      "Epoch: 153 - Loss: 0.2975362839419299\n",
      "Epoch: 154 - Loss: 0.2974457003285667\n",
      "Epoch: 155 - Loss: 0.29733099269422647\n",
      "Epoch: 156 - Loss: 0.2971808324472536\n",
      "Epoch: 157 - Loss: 0.2970003974638289\n",
      "Epoch: 158 - Loss: 0.29691244528240335\n",
      "Epoch: 159 - Loss: 0.2967509014631088\n",
      "Epoch: 160 - Loss: 0.2966566478240324\n",
      "Epoch: 161 - Loss: 0.2965336636577007\n",
      "Epoch: 162 - Loss: 0.2963837890355813\n",
      "Epoch: 163 - Loss: 0.2962858934753683\n",
      "Epoch: 164 - Loss: 0.296177496022425\n",
      "Epoch: 165 - Loss: 0.29602392435917896\n",
      "Epoch: 166 - Loss: 0.295907013829215\n",
      "Epoch: 167 - Loss: 0.2957967745569014\n",
      "Epoch: 168 - Loss: 0.2956916048485952\n",
      "Epoch: 169 - Loss: 0.29561530692934634\n",
      "Epoch: 170 - Loss: 0.2955137646500511\n",
      "Epoch: 171 - Loss: 0.2954262556589487\n",
      "Epoch: 172 - Loss: 0.29534339017646\n",
      "Epoch: 173 - Loss: 0.29524608483712733\n",
      "Epoch: 174 - Loss: 0.29517651072817286\n",
      "Epoch: 175 - Loss: 0.29507486242045367\n",
      "Epoch: 176 - Loss: 0.29496415156318484\n",
      "Epoch: 177 - Loss: 0.29485088355802597\n",
      "Epoch: 178 - Loss: 0.2947569243985156\n",
      "Epoch: 179 - Loss: 0.2946538051640519\n",
      "Epoch: 180 - Loss: 0.2945701729165063\n",
      "Epoch: 181 - Loss: 0.2944846943739416\n",
      "Epoch: 182 - Loss: 0.2944041894920774\n",
      "Epoch: 183 - Loss: 0.2943203271731491\n",
      "Epoch: 184 - Loss: 0.2941950984052976\n",
      "Epoch: 185 - Loss: 0.29413911099893025\n",
      "Epoch: 186 - Loss: 0.29404629016552947\n",
      "Epoch: 187 - Loss: 0.2939388660611418\n",
      "Epoch: 188 - Loss: 0.29384619121782846\n",
      "Epoch: 189 - Loss: 0.29381873393365243\n",
      "Epoch: 190 - Loss: 0.2937236058794649\n",
      "Epoch: 191 - Loss: 0.2936737023005762\n",
      "Epoch: 192 - Loss: 0.2935938766664418\n",
      "Epoch: 193 - Loss: 0.2935248933797426\n",
      "Epoch: 194 - Loss: 0.29346847002601484\n",
      "Epoch: 195 - Loss: 0.2933917179908392\n",
      "Epoch: 196 - Loss: 0.29331435759184515\n",
      "Epoch: 197 - Loss: 0.29326439824615363\n",
      "Epoch: 198 - Loss: 0.29320854572870914\n",
      "Epoch: 199 - Loss: 0.2931193555948534\n",
      "Epoch: 200 - Loss: 0.2930459827509329\n",
      "Epoch: 201 - Loss: 0.29297163931744263\n",
      "Epoch: 202 - Loss: 0.2929237003507855\n",
      "Epoch: 203 - Loss: 0.2928370545229246\n",
      "Epoch: 204 - Loss: 0.29277208280033706\n",
      "Epoch: 205 - Loss: 0.29269029349062853\n",
      "Epoch: 206 - Loss: 0.2926451607573287\n",
      "Epoch: 207 - Loss: 0.2925859327290666\n",
      "Epoch: 208 - Loss: 0.2925321933674597\n",
      "Epoch: 209 - Loss: 0.2924660597808541\n",
      "Epoch: 210 - Loss: 0.29241253051510635\n",
      "Epoch: 211 - Loss: 0.29233718833159544\n",
      "Epoch: 212 - Loss: 0.29230240890062864\n",
      "Epoch: 213 - Loss: 0.2922473826530558\n",
      "Epoch: 214 - Loss: 0.2921924012794996\n",
      "Epoch: 215 - Loss: 0.29212705556664953\n",
      "Epoch: 216 - Loss: 0.29206069147781155\n",
      "Epoch: 217 - Loss: 0.2919937078561604\n",
      "Epoch: 218 - Loss: 0.2919353721036367\n",
      "Epoch: 219 - Loss: 0.2918718885811394\n",
      "Epoch: 220 - Loss: 0.2918064977300021\n",
      "Epoch: 221 - Loss: 0.2917422036335903\n",
      "Epoch: 222 - Loss: 0.29171905514122687\n",
      "Epoch: 223 - Loss: 0.2916817013080473\n",
      "Epoch: 224 - Loss: 0.2916264067061136\n",
      "Epoch: 225 - Loss: 0.29157327934879684\n",
      "Epoch: 226 - Loss: 0.29149880711139486\n",
      "Epoch: 227 - Loss: 0.2914417733958924\n",
      "Epoch: 228 - Loss: 0.2914138550253252\n",
      "Epoch: 229 - Loss: 0.2913441488123228\n",
      "Epoch: 230 - Loss: 0.2913048828856602\n",
      "Epoch: 231 - Loss: 0.2912415092208225\n",
      "Epoch: 232 - Loss: 0.2911872259827996\n",
      "Epoch: 233 - Loss: 0.2911576117882328\n",
      "Epoch: 234 - Loss: 0.29111493725509896\n",
      "Epoch: 235 - Loss: 0.2910439990658436\n",
      "Epoch: 236 - Loss: 0.2910038270741026\n",
      "Epoch: 237 - Loss: 0.2909332159302735\n",
      "Epoch: 238 - Loss: 0.29089978310407505\n",
      "Epoch: 239 - Loss: 0.2908609876677687\n",
      "Epoch: 240 - Loss: 0.2908250268272168\n",
      "Epoch: 241 - Loss: 0.29078237407953006\n",
      "Epoch: 242 - Loss: 0.2907386896557431\n",
      "Epoch: 243 - Loss: 0.29071142288591395\n",
      "Epoch: 244 - Loss: 0.2906760872432293\n",
      "Epoch: 245 - Loss: 0.29063138331606814\n",
      "Epoch: 246 - Loss: 0.290586557570372\n",
      "Epoch: 247 - Loss: 0.29054417181972736\n",
      "Epoch: 248 - Loss: 0.29049369092321475\n",
      "Epoch: 249 - Loss: 0.290488788279146\n",
      "Epoch: 250 - Loss: 0.2904373978537425\n",
      "Epoch: 251 - Loss: 0.29039986870165935\n",
      "Epoch: 252 - Loss: 0.29035837443538587\n",
      "Epoch: 253 - Loss: 0.2903134157014823\n",
      "Epoch: 254 - Loss: 0.2902857105523635\n",
      "Epoch: 255 - Loss: 0.2902366538431506\n",
      "Epoch: 256 - Loss: 0.2902088331635338\n",
      "Epoch: 257 - Loss: 0.2901738640684232\n",
      "Epoch: 258 - Loss: 0.2901353249763152\n",
      "Epoch: 259 - Loss: 0.29011740374805406\n",
      "Epoch: 260 - Loss: 0.2900689919231947\n",
      "Epoch: 261 - Loss: 0.29004470621018696\n",
      "Epoch: 262 - Loss: 0.290007686677826\n",
      "Epoch: 263 - Loss: 0.2899608779588962\n",
      "Epoch: 264 - Loss: 0.28992210761903686\n",
      "Epoch: 265 - Loss: 0.28987521573022995\n",
      "Epoch: 266 - Loss: 0.2898620327680058\n",
      "Epoch: 267 - Loss: 0.2898099480017384\n",
      "Epoch: 268 - Loss: 0.28977925082808903\n",
      "Epoch: 269 - Loss: 0.28975748586220557\n",
      "Epoch: 270 - Loss: 0.2897365330328605\n",
      "Epoch: 271 - Loss: 0.2896979634835628\n",
      "Epoch: 272 - Loss: 0.2896693173690425\n",
      "Epoch: 273 - Loss: 0.2896584288025921\n",
      "Epoch: 274 - Loss: 0.28961070148693396\n",
      "Epoch: 275 - Loss: 0.2895969424840634\n",
      "Epoch: 276 - Loss: 0.2895884090495778\n",
      "Epoch: 277 - Loss: 0.28955811144742905\n",
      "Epoch: 278 - Loss: 0.2895432301280761\n",
      "Epoch: 279 - Loss: 0.28951447343316294\n",
      "Epoch: 280 - Loss: 0.2894745620058791\n",
      "Epoch: 281 - Loss: 0.28945637093428533\n",
      "Epoch: 282 - Loss: 0.28944954717388977\n",
      "Epoch: 283 - Loss: 0.2894103666801995\n",
      "Epoch: 284 - Loss: 0.2893518494598728\n",
      "Epoch: 285 - Loss: 0.2893425332051577\n",
      "Epoch: 286 - Loss: 0.2893256515464572\n",
      "Epoch: 287 - Loss: 0.28931082841464856\n",
      "Epoch: 288 - Loss: 0.2892745861807018\n",
      "Epoch: 289 - Loss: 0.28925999553460435\n",
      "Epoch: 290 - Loss: 0.2892506689660653\n",
      "Epoch: 291 - Loss: 0.2892320047930206\n",
      "Epoch: 292 - Loss: 0.28920451307891776\n",
      "Epoch: 293 - Loss: 0.28918872220467895\n",
      "Epoch: 294 - Loss: 0.2891995465282881\n",
      "Epoch: 295 - Loss: 0.289178221295158\n",
      "Epoch: 296 - Loss: 0.28912703852298094\n",
      "Epoch: 297 - Loss: 0.2891251410591011\n",
      "Epoch: 298 - Loss: 0.2890880781240376\n",
      "Epoch: 299 - Loss: 0.2890882410254552\n",
      "Epoch: 300 - Loss: 0.28905854345374293\n",
      "Epoch: 301 - Loss: 0.28903689095538937\n",
      "Epoch: 302 - Loss: 0.2890052334331407\n",
      "Epoch: 303 - Loss: 0.2889950628269583\n",
      "Epoch: 304 - Loss: 0.2889768628920719\n",
      "Epoch: 305 - Loss: 0.28894633501462763\n",
      "Epoch: 306 - Loss: 0.2889328826926424\n",
      "Epoch: 307 - Loss: 0.2888978220434329\n",
      "Epoch: 308 - Loss: 0.28887174472236576\n",
      "Epoch: 309 - Loss: 0.2888562633669172\n",
      "Epoch: 310 - Loss: 0.28883639819386225\n",
      "Epoch: 311 - Loss: 0.2888003345680144\n",
      "Epoch: 312 - Loss: 0.28877564588406224\n",
      "Epoch: 313 - Loss: 0.2887485785730313\n",
      "Epoch: 314 - Loss: 0.2887155155755199\n",
      "Epoch: 315 - Loss: 0.2887043505429435\n",
      "Epoch: 316 - Loss: 0.28868087971433515\n",
      "Epoch: 317 - Loss: 0.28864477824252305\n",
      "Epoch: 318 - Loss: 0.2886171077792975\n",
      "Epoch: 319 - Loss: 0.2885984127895041\n",
      "Epoch: 320 - Loss: 0.2885960706912573\n",
      "Epoch: 321 - Loss: 0.28856359637943946\n",
      "Epoch: 322 - Loss: 0.28851561657656255\n",
      "Epoch: 323 - Loss: 0.288528503925432\n",
      "Epoch: 324 - Loss: 0.28848945368388923\n",
      "Epoch: 325 - Loss: 0.2884891945027565\n",
      "Epoch: 326 - Loss: 0.2884703491415909\n",
      "Epoch: 327 - Loss: 0.28843404937565464\n",
      "Epoch: 328 - Loss: 0.288400547378031\n",
      "Epoch: 329 - Loss: 0.2884198102259708\n",
      "Epoch: 330 - Loss: 0.28839273706420365\n",
      "Epoch: 331 - Loss: 0.28833574410661156\n",
      "Epoch: 332 - Loss: 0.2883266217341352\n",
      "Epoch: 333 - Loss: 0.2883105477573718\n",
      "Epoch: 334 - Loss: 0.2882692256060307\n",
      "Epoch: 335 - Loss: 0.2882270852054398\n",
      "Epoch: 336 - Loss: 0.28822060033992364\n",
      "Epoch: 337 - Loss: 0.2881902339051866\n",
      "Epoch: 338 - Loss: 0.28816941128134455\n",
      "Epoch: 339 - Loss: 0.288147703347831\n",
      "Epoch: 340 - Loss: 0.2881636964369977\n",
      "Epoch: 341 - Loss: 0.28812015713164707\n",
      "Epoch: 342 - Loss: 0.28808445088345386\n",
      "Epoch: 343 - Loss: 0.28808471434377175\n",
      "Epoch: 344 - Loss: 0.2880435711970055\n",
      "Epoch: 345 - Loss: 0.288014677940003\n",
      "Epoch: 346 - Loss: 0.2880025108638952\n",
      "Epoch: 347 - Loss: 0.2879947132381182\n",
      "Epoch: 348 - Loss: 0.2879784485457218\n",
      "Epoch: 349 - Loss: 0.2879405176913212\n",
      "Epoch: 350 - Loss: 0.2879314418583418\n",
      "Epoch: 351 - Loss: 0.2879179296207267\n",
      "Epoch: 352 - Loss: 0.2879154164166615\n",
      "Epoch: 353 - Loss: 0.2878814896836002\n",
      "Epoch: 354 - Loss: 0.2878838214648643\n",
      "Epoch: 355 - Loss: 0.2878394797479457\n",
      "Epoch: 356 - Loss: 0.28781495739307333\n",
      "Epoch: 357 - Loss: 0.28777862230978285\n",
      "Epoch: 358 - Loss: 0.28777345798499426\n",
      "Epoch: 359 - Loss: 0.2877785640917431\n",
      "Epoch: 360 - Loss: 0.28774075482784583\n",
      "Epoch: 361 - Loss: 0.2877233503965861\n",
      "Epoch: 362 - Loss: 0.28768948771638236\n",
      "Epoch: 363 - Loss: 0.2877127446900664\n",
      "Epoch: 364 - Loss: 0.28768169210392364\n",
      "Epoch: 365 - Loss: 0.2876495004553522\n",
      "Epoch: 366 - Loss: 0.28762130155075716\n",
      "Epoch: 367 - Loss: 0.28762927891976886\n",
      "Epoch: 368 - Loss: 0.2875604860783053\n",
      "Epoch: 369 - Loss: 0.2875318192694559\n",
      "Epoch: 370 - Loss: 0.28755864242525997\n",
      "Epoch: 371 - Loss: 0.2875375995171501\n",
      "Epoch: 372 - Loss: 0.2874853345189147\n",
      "Epoch: 373 - Loss: 0.28745972238628476\n",
      "Epoch: 374 - Loss: 0.28742315197040436\n",
      "Epoch: 375 - Loss: 0.2874223091344571\n",
      "Epoch: 376 - Loss: 0.2874159942247319\n",
      "Epoch: 377 - Loss: 0.28735530442505514\n",
      "Epoch: 378 - Loss: 0.2873433570339204\n",
      "Epoch: 379 - Loss: 0.2873333667222505\n",
      "Epoch: 380 - Loss: 0.287326835243086\n",
      "Epoch: 381 - Loss: 0.28729455274804777\n",
      "Epoch: 382 - Loss: 0.28729504207952095\n",
      "Epoch: 383 - Loss: 0.2872507192186038\n",
      "Epoch: 384 - Loss: 0.2872718160532627\n",
      "Epoch: 385 - Loss: 0.28722246100407595\n",
      "Epoch: 386 - Loss: 0.28723506742009625\n",
      "Epoch: 387 - Loss: 0.28720168210220853\n",
      "Epoch: 388 - Loss: 0.2871725280083038\n",
      "Epoch: 389 - Loss: 0.2871573494334639\n",
      "Epoch: 390 - Loss: 0.2871344627727684\n",
      "Epoch: 391 - Loss: 0.2871272909087226\n",
      "Epoch: 392 - Loss: 0.2871030102343363\n",
      "Epoch: 393 - Loss: 0.2870887231750858\n",
      "Epoch: 394 - Loss: 0.28708093821457287\n",
      "Epoch: 395 - Loss: 0.28706131957299763\n",
      "Epoch: 396 - Loss: 0.2870472866948539\n",
      "Epoch: 397 - Loss: 0.28705231709784607\n",
      "Epoch: 398 - Loss: 0.2870108825568424\n",
      "Epoch: 399 - Loss: 0.2870035633365897\n",
      "Epoch: 400 - Loss: 0.28700774327869083\n",
      "Epoch: 401 - Loss: 0.28697046604998844\n",
      "Epoch: 402 - Loss: 0.28696947454090627\n",
      "Epoch: 403 - Loss: 0.28695307062659053\n",
      "Epoch: 404 - Loss: 0.2869164768951706\n",
      "Epoch: 405 - Loss: 0.2869281340790513\n",
      "Epoch: 406 - Loss: 0.2869020376697944\n",
      "Epoch: 407 - Loss: 0.2868965894372911\n",
      "Epoch: 408 - Loss: 0.2869061013383969\n",
      "Epoch: 409 - Loss: 0.28686800457558465\n",
      "Epoch: 410 - Loss: 0.2868433769852967\n",
      "Epoch: 411 - Loss: 0.2868265280257331\n",
      "Epoch: 412 - Loss: 0.286823201938007\n",
      "Epoch: 413 - Loss: 0.286796890281988\n",
      "Epoch: 414 - Loss: 0.28677814668369594\n",
      "Epoch: 415 - Loss: 0.2867764944267542\n",
      "Epoch: 416 - Loss: 0.2867364853567909\n",
      "Epoch: 417 - Loss: 0.2867127962293639\n",
      "Epoch: 418 - Loss: 0.28670899754149387\n",
      "Epoch: 419 - Loss: 0.2866955869960995\n",
      "Epoch: 420 - Loss: 0.2866813997395729\n",
      "Epoch: 421 - Loss: 0.28668503470159223\n",
      "Epoch: 422 - Loss: 0.2866829350889283\n",
      "Epoch: 423 - Loss: 0.286634628089831\n",
      "Epoch: 424 - Loss: 0.28664466830301677\n",
      "Epoch: 425 - Loss: 0.2866166474418605\n",
      "Epoch: 426 - Loss: 0.2865867725927553\n",
      "Epoch: 427 - Loss: 0.28659684909704436\n",
      "Epoch: 428 - Loss: 0.2866058191571678\n",
      "Epoch: 429 - Loss: 0.28656052139127147\n",
      "Epoch: 430 - Loss: 0.28655775449506643\n",
      "Epoch: 431 - Loss: 0.2865582535166488\n",
      "Epoch: 432 - Loss: 0.2865647471705157\n",
      "Epoch: 433 - Loss: 0.2865172456384311\n",
      "Epoch: 434 - Loss: 0.28652016053906215\n",
      "Epoch: 435 - Loss: 0.2865335390526695\n",
      "Epoch: 436 - Loss: 0.28649256361140324\n",
      "Epoch: 437 - Loss: 0.2864884280981996\n",
      "Epoch: 438 - Loss: 0.28648614380601317\n",
      "Epoch: 439 - Loss: 0.2865144098350873\n",
      "Epoch: 440 - Loss: 0.28648465309381643\n",
      "Epoch: 441 - Loss: 0.28647683920228917\n",
      "Epoch: 442 - Loss: 0.28646906253434185\n",
      "Epoch: 443 - Loss: 0.28648744384540137\n",
      "Epoch: 444 - Loss: 0.2864432514937205\n",
      "Epoch: 445 - Loss: 0.2864613201125902\n",
      "Epoch: 446 - Loss: 0.2864315846898345\n",
      "Epoch: 447 - Loss: 0.28643180055721823\n",
      "Epoch: 448 - Loss: 0.28640314994518684\n",
      "Epoch: 449 - Loss: 0.2864177122081738\n",
      "Epoch: 450 - Loss: 0.2863774791273018\n",
      "Epoch: 451 - Loss: 0.2863824094766191\n",
      "Epoch: 452 - Loss: 0.2863905615049797\n",
      "Epoch: 453 - Loss: 0.28633982196754976\n",
      "Epoch: 454 - Loss: 0.28632872293555484\n",
      "Epoch: 455 - Loss: 0.2863698917056599\n",
      "Epoch: 456 - Loss: 0.2863253150577817\n",
      "Epoch: 457 - Loss: 0.2863009180193793\n",
      "Epoch: 458 - Loss: 0.28631685225518116\n",
      "Epoch: 459 - Loss: 0.28629831272312\n",
      "Epoch: 460 - Loss: 0.28626886762426795\n",
      "Epoch: 461 - Loss: 0.28625046813921223\n",
      "Epoch: 462 - Loss: 0.28623066851310247\n",
      "Epoch: 463 - Loss: 0.28623041239311214\n",
      "Epoch: 464 - Loss: 0.2862059645877336\n",
      "Epoch: 465 - Loss: 0.2862073374446101\n",
      "Epoch: 466 - Loss: 0.28617656058069535\n",
      "Epoch: 467 - Loss: 0.28615365157426603\n",
      "Epoch: 468 - Loss: 0.2861261560564954\n",
      "Epoch: 469 - Loss: 0.2861687374351931\n",
      "Epoch: 470 - Loss: 0.28612981709705465\n",
      "Epoch: 471 - Loss: 0.2860874533666806\n",
      "Epoch: 472 - Loss: 0.28610448410168304\n",
      "Epoch: 473 - Loss: 0.28608149416396716\n",
      "Epoch: 474 - Loss: 0.2860629343301585\n",
      "Epoch: 475 - Loss: 0.2860634120456592\n",
      "Epoch: 476 - Loss: 0.2860674761274812\n",
      "Epoch: 477 - Loss: 0.28601325546511763\n",
      "Epoch: 478 - Loss: 0.28601430340563827\n",
      "Epoch: 479 - Loss: 0.2859713848361553\n",
      "Epoch: 480 - Loss: 0.2859883547746276\n",
      "Epoch: 481 - Loss: 0.2859490534363111\n",
      "Epoch: 482 - Loss: 0.2859501330659542\n",
      "Epoch: 483 - Loss: 0.28595672569432434\n",
      "Epoch: 484 - Loss: 0.2859494032304312\n",
      "Epoch: 485 - Loss: 0.28593182530142225\n",
      "Epoch: 486 - Loss: 0.285921711667411\n",
      "Epoch: 487 - Loss: 0.28592541016862477\n",
      "Epoch: 488 - Loss: 0.2859122166367916\n",
      "Epoch: 489 - Loss: 0.28588572427568415\n",
      "Epoch: 490 - Loss: 0.2858994988963642\n",
      "Epoch: 491 - Loss: 0.2858876581917805\n",
      "Epoch: 492 - Loss: 0.28586077762774764\n",
      "Epoch: 493 - Loss: 0.2858489734894808\n",
      "Epoch: 494 - Loss: 0.2858478026568209\n",
      "Epoch: 495 - Loss: 0.28586047722494706\n",
      "Epoch: 496 - Loss: 0.28585302934502915\n",
      "Epoch: 497 - Loss: 0.2858215640213235\n",
      "Epoch: 498 - Loss: 0.2858300852138109\n",
      "Epoch: 499 - Loss: 0.28579962314800655\n",
      "Epoch: 500 - Loss: 0.28582399463372765\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.classify(pred_file + \".txt\")\n",
    "# preds, t1, t2 = model.classify(pred_file + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "\n",
    "# products\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    3844\n",
       "neg    2670\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    3457\n",
       "pos    3057\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.14%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
