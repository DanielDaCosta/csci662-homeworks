{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file, threshold=0, max_features=None):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "        # self.vocabulary = list(self.embedding_matrix.keys())\n",
    "        # self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        # self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features-1))\n",
    "\n",
    "        # Add to vocabulary the Out-of-Vocabulary token\n",
    "        return list(flattened_list_count_filter.keys()) + ['UNK']\n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        # Create Vocabulary\n",
    "        self.vocabulary = self.create_vocabulary(tokenized_text, self.threshold, self.max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)\n",
    "    \n",
    "    def sort_by_tfidf(self, tfidf_matrix, max_seq_length):\n",
    "        \"\"\"Sort input documents based on tf*idf score.\n",
    "        Return top \"max_seq_length\" words\n",
    "        :param: tfidf_matrix\n",
    "        :param: max_seq_length\n",
    "        :return: sentences ordered by TF-IDF score\n",
    "        \"\"\"\n",
    "        \n",
    "        # Indices of sorted matrix in descending order\n",
    "        indices = np.argsort(-tfidf_matrix, axis=1)\n",
    "        tfidf_matrix_sorted = []\n",
    "\n",
    "        # Create sorted matrix\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            # sentence in orderd version\n",
    "            tmp = [self.index2word[index] for index in indices[i][:max_seq_length]]\n",
    "            tfidf_matrix_sorted.append(tmp)\n",
    "    \n",
    "        return tfidf_matrix_sorted\n",
    "    \n",
    "    def get_features_tfidf(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothinf\n",
    "        return tf*idf_array\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from nn_layers import FeedForwardNetwork\n",
    "\n",
    "class NeuralModel(Model):\n",
    "    def __init__(self, embeddingfile, max_seq_length, hidden_units, minibatch_size, learning_rate, epochs, tfidf=False, max_features=None, threshold=0): \n",
    "        # self.network = FeedForwardNetwork()\n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_units = hidden_units\n",
    "        self.weights_1 = None\n",
    "        self.bias_1 = None\n",
    "        self.weights_2 = None\n",
    "        self.bias_2 = None\n",
    "        self.Y_to_categorical = None\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = {}\n",
    "        # TF-IDF Sorting\n",
    "        self.tfidf = tfidf # enable sorting by tf-idf score\n",
    "        self.max_features = max_features\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def initialize_weights(self, n_inputs, n_output):\n",
    "        # weights = np.zeros((n_inputs, n_output))\n",
    "        # bias = np.zeros(n_output)\n",
    "        # np.random.seed(0)\n",
    "        weights = np.random.rand(n_inputs, n_output)\n",
    "        bias = np.random.rand(n_output)\n",
    "        return weights, bias\n",
    "    \n",
    "    def relu_function(self, A):\n",
    "        '''A = x*W + b\n",
    "\n",
    "        :return: Z = relut(x*A+b)\n",
    "        '''\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def relu_derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        A = np.dot(X, self.weights_1) + self.bias_1\n",
    "        h = self.relu_function(A)\n",
    "\n",
    "        A_2 = np.dot(h, self.weights_2) + self.bias_2\n",
    "\n",
    "        O = self.softmax(A_2)\n",
    "\n",
    "        # Rows with highest probability\n",
    "        S_max = np.argmax(O, axis=1)\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "    def convert_to_embeddings(self, sentence):\n",
    "        '''Convert sentence to embeddings\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "            # try:\n",
    "        if emb: # if there is a word\n",
    "            emb_concat = np.concatenate(emb, axis=0)\n",
    "        else:\n",
    "            emb_concat = []\n",
    "        # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "        if len(emb) < self.max_seq_length:\n",
    "            # Missing words\n",
    "            words_missing = self.max_seq_length - len(emb)\n",
    "            # print(words_missing)\n",
    "            emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile, threshold=self.threshold, max_features=self.max_features)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        # Convert to OneHot for computing Loss\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim Ã— features (u)\n",
    "        n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        print(\"Computing TFIDF\")\n",
    "        if self.tfidf: # Truncate input to the max sequence length sorted by TF-IDF\n",
    "            tf_idf = features_ff_class.tf_idf(features_ff_class.tokenized_text)\n",
    "            trunc_tokenized_text = features_ff_class.sort_by_tfidf(\n",
    "                tf_idf,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "                features_ff_class.tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        print(\"Computing Embedding\")\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Wieghts\n",
    "        # Create W_a and b_a\n",
    "        # W_a[n_documents, hidden_units (u)]\n",
    "        # b_a[hidden_units (u)]\n",
    "        W_1, b_1 = self.initialize_weights(n_inputs, self.hidden_units)\n",
    "        # Create Wb and b_b\n",
    "        # W_b[hidden_units (u), num_labels (d)]\n",
    "        # b_b[num_labels]\n",
    "        W_2, b_2 = self.initialize_weights(self.hidden_units, num_labels)\n",
    "\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        self.weights_1 = W_1\n",
    "        self.bias_1 = b_1\n",
    "        self.weights_2 = W_2\n",
    "        self.bias_2 = b_2\n",
    "        for i in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "            \n",
    "                # ---------------- Input-to-Hidden Layer --------------- #\n",
    "                # Z1 = W_a*X + b_a\n",
    "                # Z1[n_documents, hidden_units (u)]\n",
    "                Z_1 = np.dot(X_mini_batch, self.weights_1) + self.bias_1\n",
    "                # Hidden Unit\n",
    "                # h = relu(A)\n",
    "                # h[n_documents, hidden_units (u)]\n",
    "                A_1 = self.relu_function(Z_1)\n",
    "\n",
    "                # ---------------- Hidden-to-Output Layer --------------- #\n",
    "                #  = W_b*h + b_b\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                Z_2 = np.dot(A_1, self.weights_2) + self.bias_2\n",
    "                # Output Layer\n",
    "                # A_2 = softmax(Z_2)\n",
    "                # A_2[n_documents, num_labels (d)]\n",
    "                A_2 = self.softmax(Z_2)\n",
    "                # print(A_2)\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "\n",
    "                # Compute Gradients\n",
    "\n",
    "                dZ_2 = A_2 - y_mini_batch # [n_documents, num_labels (d)]\n",
    "                # np.dot(A_2, dZ_2) => (hidden_units, n_documents) X (n_documents, num_labels) = (hidden_units, num_labels)\n",
    "                dW_2 = (1/minibatch_size)*np.dot(A_1.T, dZ_2)\n",
    "                db_2 = (1/minibatch_size)*np.sum(dZ_2, axis=0, keepdims = True) # [num_labels]\n",
    "                # np.dot(self.weights_b, dZ_2) => [n_documents, num_labels (d)] X [num_labels (d), hidden_units (u)] => [n_documents, hidden_units]\n",
    "                dZ_1 = np.dot(dZ_2, self.weights_2.T)*self.relu_derivative(Z_1)\n",
    "                # np.dot(X, dZ_1) => (features, n_documents) X (n_documents, hidden_units) = (hidden_units, num_labels)\n",
    "                dW_1 = (1/minibatch_size)*np.dot(X_mini_batch.T, dZ_1)\n",
    "                db_1 = (1/minibatch_size)*np.sum(dZ_1, axis=0, keepdims = True) # [hidden_units]\n",
    "\n",
    "                # Update weights\n",
    "                self.weights_1 = self.weights_1 - self.learning_rate*dW_1\n",
    "                self.bias_1 = self.bias_1 - self.learning_rate*db_1\n",
    "                self.weights_2 = self.weights_2 - self.learning_rate*dW_2\n",
    "                self.bias_2 = self.bias_2 - self.learning_rate*db_2\n",
    "\n",
    "                ########\n",
    "                # Loss #\n",
    "                ########\n",
    "                mini_batch_loss.append(self.cross_entropy_loss(A_2, y_mini_batch))\n",
    "\n",
    "            loss = np.mean(mini_batch_loss)\n",
    "            self.loss[i] = loss\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        if self.tfidf:\n",
    "            tf_idf_inference = []\n",
    "            # Get features from inference file\n",
    "            for sentence in tokenized_text:\n",
    "                # Transform dataset to TF-IDF space\n",
    "                # Return features with format (1, size_vocabulary)\n",
    "                X_sentence = self.features_ff_class.get_features_tfidf(sentence, self.features_ff_class.idf)\n",
    "                tf_idf_inference.append(X_sentence)\n",
    "            tf_idf_inference = np.stack(tf_idf_inference)\n",
    "            trunc_tokenized_text = self.features_ff_class.sort_by_tfidf(\n",
    "                tf_idf_inference,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "                tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Make Prediction\n",
    "        y_test = self.predict(X_test)\n",
    "        preds_label = []\n",
    "        for y in y_test:\n",
    "            tmp = self.Y_to_categorical[y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "train_file = \"datasets/products/train.txt\"\n",
    "emb_file = \"glove.6B.50d.txt\"\n",
    "pred_file = \"datasets/products/val.test\"\n",
    "pred_true_labels = \"datasets/products/val.txt\"\n",
    "model_file_name = \"products.model\"\n",
    "loss_file = \"datasets/products/loss.txt\"\n",
    "nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2) # 65%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=256, learning_rate=0.1, epochs=500, tfidf=True, max_features=500, threshold=2) # 64%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2)\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=32, learning_rate=0.05, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=128, learning_rate=0.05, epochs=100, tfidf=True, threshold=2, max_features=1000)\n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "# train_file = \"datasets/odiya/train.txt\"\n",
    "# emb_file = \"fasttext.wiki.300d.vec\"\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "# model_file_name = \"odiya.model\"\n",
    "# loss_file = \"datasets/odiya/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF\n",
      "Computing Embedding\n",
      "Epoch: 1 - Loss: 1.1995253731079238\n",
      "Epoch: 2 - Loss: 0.34003656586685055\n",
      "Epoch: 3 - Loss: 0.33820896163287006\n",
      "Epoch: 4 - Loss: 0.33728433314037093\n",
      "Epoch: 5 - Loss: 0.33668541495136567\n",
      "Epoch: 6 - Loss: 0.33625604639948253\n",
      "Epoch: 7 - Loss: 0.3358474604084416\n",
      "Epoch: 8 - Loss: 0.3354807657467681\n",
      "Epoch: 9 - Loss: 0.33518050778562486\n",
      "Epoch: 10 - Loss: 0.33489039296914297\n",
      "Epoch: 11 - Loss: 0.33463438569789705\n",
      "Epoch: 12 - Loss: 0.33438132736914516\n",
      "Epoch: 13 - Loss: 0.33414969417971907\n",
      "Epoch: 14 - Loss: 0.3339378588739833\n",
      "Epoch: 15 - Loss: 0.3337139467964728\n",
      "Epoch: 16 - Loss: 0.3335051712049158\n",
      "Epoch: 17 - Loss: 0.33328295810856307\n",
      "Epoch: 18 - Loss: 0.33306123448301006\n",
      "Epoch: 19 - Loss: 0.3328403879552119\n",
      "Epoch: 20 - Loss: 0.3326107251649328\n",
      "Epoch: 21 - Loss: 0.3323682654001583\n",
      "Epoch: 22 - Loss: 0.3321299059778196\n",
      "Epoch: 23 - Loss: 0.3318693062620282\n",
      "Epoch: 24 - Loss: 0.3315846307940797\n",
      "Epoch: 25 - Loss: 0.3312837134646719\n",
      "Epoch: 26 - Loss: 0.3309808720408219\n",
      "Epoch: 27 - Loss: 0.3306606069065704\n",
      "Epoch: 28 - Loss: 0.33035611330669107\n",
      "Epoch: 29 - Loss: 0.33003951235443585\n",
      "Epoch: 30 - Loss: 0.329724528877671\n",
      "Epoch: 31 - Loss: 0.329397976429914\n",
      "Epoch: 32 - Loss: 0.3290771828195849\n",
      "Epoch: 33 - Loss: 0.3287099860013659\n",
      "Epoch: 34 - Loss: 0.3283445031300037\n",
      "Epoch: 35 - Loss: 0.32800968928474383\n",
      "Epoch: 36 - Loss: 0.32766869409700716\n",
      "Epoch: 37 - Loss: 0.32735002516482326\n",
      "Epoch: 38 - Loss: 0.3269963359354548\n",
      "Epoch: 39 - Loss: 0.32662957780743135\n",
      "Epoch: 40 - Loss: 0.3262965275601716\n",
      "Epoch: 41 - Loss: 0.32591387948431294\n",
      "Epoch: 42 - Loss: 0.3255684151620629\n",
      "Epoch: 43 - Loss: 0.3252097607736348\n",
      "Epoch: 44 - Loss: 0.32485230818872485\n",
      "Epoch: 45 - Loss: 0.32448421247403497\n",
      "Epoch: 46 - Loss: 0.32413860843693837\n",
      "Epoch: 47 - Loss: 0.32379030485939103\n",
      "Epoch: 48 - Loss: 0.3234395461642319\n",
      "Epoch: 49 - Loss: 0.323142709038653\n",
      "Epoch: 50 - Loss: 0.32285455815476904\n",
      "Epoch: 51 - Loss: 0.32249592857765585\n",
      "Epoch: 52 - Loss: 0.32215359126347404\n",
      "Epoch: 53 - Loss: 0.32177268992754027\n",
      "Epoch: 54 - Loss: 0.32148870946133923\n",
      "Epoch: 55 - Loss: 0.3211337304051388\n",
      "Epoch: 56 - Loss: 0.32080279610070234\n",
      "Epoch: 57 - Loss: 0.3204625020320249\n",
      "Epoch: 58 - Loss: 0.3201432713300178\n",
      "Epoch: 59 - Loss: 0.3198327532728254\n",
      "Epoch: 60 - Loss: 0.3194809588767496\n",
      "Epoch: 61 - Loss: 0.3191732711140168\n",
      "Epoch: 62 - Loss: 0.3188500735737068\n",
      "Epoch: 63 - Loss: 0.3184880660903731\n",
      "Epoch: 64 - Loss: 0.31820191207110743\n",
      "Epoch: 65 - Loss: 0.317837976129468\n",
      "Epoch: 66 - Loss: 0.31757449286501593\n",
      "Epoch: 67 - Loss: 0.31725831389690445\n",
      "Epoch: 68 - Loss: 0.3170341511684481\n",
      "Epoch: 69 - Loss: 0.31678131487176503\n",
      "Epoch: 70 - Loss: 0.3165320704902218\n",
      "Epoch: 71 - Loss: 0.3162818052333187\n",
      "Epoch: 72 - Loss: 0.3159857716099651\n",
      "Epoch: 73 - Loss: 0.3156789403105371\n",
      "Epoch: 74 - Loss: 0.31543078298015426\n",
      "Epoch: 75 - Loss: 0.31511797903129324\n",
      "Epoch: 76 - Loss: 0.3148383700375092\n",
      "Epoch: 77 - Loss: 0.31447451413632793\n",
      "Epoch: 78 - Loss: 0.3141523346971409\n",
      "Epoch: 79 - Loss: 0.31390167872234986\n",
      "Epoch: 80 - Loss: 0.31354854831846135\n",
      "Epoch: 81 - Loss: 0.3132336372237694\n",
      "Epoch: 82 - Loss: 0.3129329594099266\n",
      "Epoch: 83 - Loss: 0.31266108559445754\n",
      "Epoch: 84 - Loss: 0.31240718425599384\n",
      "Epoch: 85 - Loss: 0.312122714129877\n",
      "Epoch: 86 - Loss: 0.3118350889612016\n",
      "Epoch: 87 - Loss: 0.3115885921159805\n",
      "Epoch: 88 - Loss: 0.3112965394910192\n",
      "Epoch: 89 - Loss: 0.31101338354658364\n",
      "Epoch: 90 - Loss: 0.31069208524439534\n",
      "Epoch: 91 - Loss: 0.31045388520548545\n",
      "Epoch: 92 - Loss: 0.3101737058247375\n",
      "Epoch: 93 - Loss: 0.3099122040708715\n",
      "Epoch: 94 - Loss: 0.3096936200360742\n",
      "Epoch: 95 - Loss: 0.3094828709390387\n",
      "Epoch: 96 - Loss: 0.3092477450968604\n",
      "Epoch: 97 - Loss: 0.3090332810432517\n",
      "Epoch: 98 - Loss: 0.3087922234449971\n",
      "Epoch: 99 - Loss: 0.3085625045834359\n",
      "Epoch: 100 - Loss: 0.30834933490007754\n",
      "Epoch: 101 - Loss: 0.3080652826643595\n",
      "Epoch: 102 - Loss: 0.3078982848362522\n",
      "Epoch: 103 - Loss: 0.3076234414958609\n",
      "Epoch: 104 - Loss: 0.3073811320153344\n",
      "Epoch: 105 - Loss: 0.30714845124952056\n",
      "Epoch: 106 - Loss: 0.30702629673217785\n",
      "Epoch: 107 - Loss: 0.30678605159136857\n",
      "Epoch: 108 - Loss: 0.306520456911304\n",
      "Epoch: 109 - Loss: 0.3063729887465544\n",
      "Epoch: 110 - Loss: 0.30615912880605634\n",
      "Epoch: 111 - Loss: 0.30595539404674604\n",
      "Epoch: 112 - Loss: 0.30578698546676103\n",
      "Epoch: 113 - Loss: 0.3055566582237731\n",
      "Epoch: 114 - Loss: 0.3053538834220327\n",
      "Epoch: 115 - Loss: 0.3051612549479769\n",
      "Epoch: 116 - Loss: 0.30498170892711945\n",
      "Epoch: 117 - Loss: 0.30476358339714893\n",
      "Epoch: 118 - Loss: 0.30455831396268895\n",
      "Epoch: 119 - Loss: 0.3044022385544333\n",
      "Epoch: 120 - Loss: 0.3042063671887852\n",
      "Epoch: 121 - Loss: 0.30407365215488324\n",
      "Epoch: 122 - Loss: 0.3039140690366872\n",
      "Epoch: 123 - Loss: 0.303749060752102\n",
      "Epoch: 124 - Loss: 0.30354105518385577\n",
      "Epoch: 125 - Loss: 0.3033033417076615\n",
      "Epoch: 126 - Loss: 0.303062173513398\n",
      "Epoch: 127 - Loss: 0.3029002559887039\n",
      "Epoch: 128 - Loss: 0.3027272917934007\n",
      "Epoch: 129 - Loss: 0.3025994897738031\n",
      "Epoch: 130 - Loss: 0.30241154022838657\n",
      "Epoch: 131 - Loss: 0.30223384819659144\n",
      "Epoch: 132 - Loss: 0.3020964061453481\n",
      "Epoch: 133 - Loss: 0.30190490305823875\n",
      "Epoch: 134 - Loss: 0.3017440050095607\n",
      "Epoch: 135 - Loss: 0.30158003930678196\n",
      "Epoch: 136 - Loss: 0.30146573445641023\n",
      "Epoch: 137 - Loss: 0.30134642204722967\n",
      "Epoch: 138 - Loss: 0.30118707095887526\n",
      "Epoch: 139 - Loss: 0.30104594249019656\n",
      "Epoch: 140 - Loss: 0.30087858930885736\n",
      "Epoch: 141 - Loss: 0.30070434125281587\n",
      "Epoch: 142 - Loss: 0.3005562444245224\n",
      "Epoch: 143 - Loss: 0.3003993456929526\n",
      "Epoch: 144 - Loss: 0.3002318240032597\n",
      "Epoch: 145 - Loss: 0.300074750827493\n",
      "Epoch: 146 - Loss: 0.29994246907165706\n",
      "Epoch: 147 - Loss: 0.2997768884809268\n",
      "Epoch: 148 - Loss: 0.2996534032241913\n",
      "Epoch: 149 - Loss: 0.29952869660110537\n",
      "Epoch: 150 - Loss: 0.2994034735066938\n",
      "Epoch: 151 - Loss: 0.2992626553103043\n",
      "Epoch: 152 - Loss: 0.29914580566071863\n",
      "Epoch: 153 - Loss: 0.299046134679769\n",
      "Epoch: 154 - Loss: 0.2989025015934031\n",
      "Epoch: 155 - Loss: 0.29880449052054975\n",
      "Epoch: 156 - Loss: 0.298674321150074\n",
      "Epoch: 157 - Loss: 0.29857517011898366\n",
      "Epoch: 158 - Loss: 0.29844778765117846\n",
      "Epoch: 159 - Loss: 0.29833101229138564\n",
      "Epoch: 160 - Loss: 0.2981970938959049\n",
      "Epoch: 161 - Loss: 0.29807932359359707\n",
      "Epoch: 162 - Loss: 0.297989405647165\n",
      "Epoch: 163 - Loss: 0.297865118718825\n",
      "Epoch: 164 - Loss: 0.2977300196446568\n",
      "Epoch: 165 - Loss: 0.29762892707339655\n",
      "Epoch: 166 - Loss: 0.29748405105265374\n",
      "Epoch: 167 - Loss: 0.2973418842005849\n",
      "Epoch: 168 - Loss: 0.29723960537552446\n",
      "Epoch: 169 - Loss: 0.2971127312033552\n",
      "Epoch: 170 - Loss: 0.29702109239512425\n",
      "Epoch: 171 - Loss: 0.2968973104674491\n",
      "Epoch: 172 - Loss: 0.29679673250163846\n",
      "Epoch: 173 - Loss: 0.29668653889743674\n",
      "Epoch: 174 - Loss: 0.2965653658052828\n",
      "Epoch: 175 - Loss: 0.29647989388880513\n",
      "Epoch: 176 - Loss: 0.29639094941538946\n",
      "Epoch: 177 - Loss: 0.29626959255887164\n",
      "Epoch: 178 - Loss: 0.2961919659980862\n",
      "Epoch: 179 - Loss: 0.2960927022402969\n",
      "Epoch: 180 - Loss: 0.29599953205631635\n",
      "Epoch: 181 - Loss: 0.295913257848892\n",
      "Epoch: 182 - Loss: 0.29580196156593425\n",
      "Epoch: 183 - Loss: 0.2957429269288869\n",
      "Epoch: 184 - Loss: 0.29565247037250864\n",
      "Epoch: 185 - Loss: 0.2955910678227866\n",
      "Epoch: 186 - Loss: 0.2954752675574741\n",
      "Epoch: 187 - Loss: 0.29540146712451404\n",
      "Epoch: 188 - Loss: 0.29531204353443924\n",
      "Epoch: 189 - Loss: 0.2952289172933213\n",
      "Epoch: 190 - Loss: 0.2951624414791989\n",
      "Epoch: 191 - Loss: 0.2950830977111952\n",
      "Epoch: 192 - Loss: 0.29496886155789365\n",
      "Epoch: 193 - Loss: 0.29492876556294484\n",
      "Epoch: 194 - Loss: 0.29485543554717547\n",
      "Epoch: 195 - Loss: 0.2947538253699302\n",
      "Epoch: 196 - Loss: 0.29466785347602525\n",
      "Epoch: 197 - Loss: 0.294590911736665\n",
      "Epoch: 198 - Loss: 0.29453490175268177\n",
      "Epoch: 199 - Loss: 0.2944631183520076\n",
      "Epoch: 200 - Loss: 0.2944043632893367\n",
      "Epoch: 201 - Loss: 0.29433663085995687\n",
      "Epoch: 202 - Loss: 0.2942872749907657\n",
      "Epoch: 203 - Loss: 0.29422614244756107\n",
      "Epoch: 204 - Loss: 0.2941976724227112\n",
      "Epoch: 205 - Loss: 0.29413339654397874\n",
      "Epoch: 206 - Loss: 0.29406641799348143\n",
      "Epoch: 207 - Loss: 0.2939916844257414\n",
      "Epoch: 208 - Loss: 0.29390869287205906\n",
      "Epoch: 209 - Loss: 0.2938264854531936\n",
      "Epoch: 210 - Loss: 0.29376299582863263\n",
      "Epoch: 211 - Loss: 0.2936868913955426\n",
      "Epoch: 212 - Loss: 0.29361423778377943\n",
      "Epoch: 213 - Loss: 0.2935563346710366\n",
      "Epoch: 214 - Loss: 0.29350131780564237\n",
      "Epoch: 215 - Loss: 0.29343599023171385\n",
      "Epoch: 216 - Loss: 0.2933732623265549\n",
      "Epoch: 217 - Loss: 0.29330611196126355\n",
      "Epoch: 218 - Loss: 0.2933028879978915\n",
      "Epoch: 219 - Loss: 0.29322484563436385\n",
      "Epoch: 220 - Loss: 0.29318656607811827\n",
      "Epoch: 221 - Loss: 0.2931320344322267\n",
      "Epoch: 222 - Loss: 0.2930780234810384\n",
      "Epoch: 223 - Loss: 0.2930287273183446\n",
      "Epoch: 224 - Loss: 0.2930004622254562\n",
      "Epoch: 225 - Loss: 0.2929375830121629\n",
      "Epoch: 226 - Loss: 0.2929141101149112\n",
      "Epoch: 227 - Loss: 0.29287469752126327\n",
      "Epoch: 228 - Loss: 0.29283208380408726\n",
      "Epoch: 229 - Loss: 0.2928037798434691\n",
      "Epoch: 230 - Loss: 0.2927756892836041\n",
      "Epoch: 231 - Loss: 0.29276285049073697\n",
      "Epoch: 232 - Loss: 0.29269996679342997\n",
      "Epoch: 233 - Loss: 0.29265882060172893\n",
      "Epoch: 234 - Loss: 0.2926022222113269\n",
      "Epoch: 235 - Loss: 0.2925495160151729\n",
      "Epoch: 236 - Loss: 0.2925194025543049\n",
      "Epoch: 237 - Loss: 0.2924572975554097\n",
      "Epoch: 238 - Loss: 0.29241410157726727\n",
      "Epoch: 239 - Loss: 0.2923549281023518\n",
      "Epoch: 240 - Loss: 0.2922992265281972\n",
      "Epoch: 241 - Loss: 0.29225316924424044\n",
      "Epoch: 242 - Loss: 0.29222526022052\n",
      "Epoch: 243 - Loss: 0.2921671502886395\n",
      "Epoch: 244 - Loss: 0.2921308054020382\n",
      "Epoch: 245 - Loss: 0.29210799213565136\n",
      "Epoch: 246 - Loss: 0.2920759055071934\n",
      "Epoch: 247 - Loss: 0.2920371463919355\n",
      "Epoch: 248 - Loss: 0.29200931336784586\n",
      "Epoch: 249 - Loss: 0.29196032896287366\n",
      "Epoch: 250 - Loss: 0.2919337000916462\n",
      "Epoch: 251 - Loss: 0.29191229472960617\n",
      "Epoch: 252 - Loss: 0.2918576680259416\n",
      "Epoch: 253 - Loss: 0.29181015744263045\n",
      "Epoch: 254 - Loss: 0.2917904840632173\n",
      "Epoch: 255 - Loss: 0.2917344021953481\n",
      "Epoch: 256 - Loss: 0.2916891155333211\n",
      "Epoch: 257 - Loss: 0.291673667130733\n",
      "Epoch: 258 - Loss: 0.29161576354853624\n",
      "Epoch: 259 - Loss: 0.2915924466966061\n",
      "Epoch: 260 - Loss: 0.29157510798414016\n",
      "Epoch: 261 - Loss: 0.29154504780556756\n",
      "Epoch: 262 - Loss: 0.29149829732586713\n",
      "Epoch: 263 - Loss: 0.29147429325753305\n",
      "Epoch: 264 - Loss: 0.2914543978797127\n",
      "Epoch: 265 - Loss: 0.2914141220006286\n",
      "Epoch: 266 - Loss: 0.2913772876146841\n",
      "Epoch: 267 - Loss: 0.2913495236480833\n",
      "Epoch: 268 - Loss: 0.2913188558995784\n",
      "Epoch: 269 - Loss: 0.29129378650510346\n",
      "Epoch: 270 - Loss: 0.2912537328811718\n",
      "Epoch: 271 - Loss: 0.29124059080148346\n",
      "Epoch: 272 - Loss: 0.2912195725110591\n",
      "Epoch: 273 - Loss: 0.2911854345792624\n",
      "Epoch: 274 - Loss: 0.2911621822302999\n",
      "Epoch: 275 - Loss: 0.29114529020150576\n",
      "Epoch: 276 - Loss: 0.29110817242541503\n",
      "Epoch: 277 - Loss: 0.2910991954788783\n",
      "Epoch: 278 - Loss: 0.29107283906480474\n",
      "Epoch: 279 - Loss: 0.2910376857584929\n",
      "Epoch: 280 - Loss: 0.29100530479306186\n",
      "Epoch: 281 - Loss: 0.2909911640107047\n",
      "Epoch: 282 - Loss: 0.2909753795470607\n",
      "Epoch: 283 - Loss: 0.2909402420411974\n",
      "Epoch: 284 - Loss: 0.29089672146211787\n",
      "Epoch: 285 - Loss: 0.29088376836388796\n",
      "Epoch: 286 - Loss: 0.29085124714428523\n",
      "Epoch: 287 - Loss: 0.2908328808119822\n",
      "Epoch: 288 - Loss: 0.29078374810549656\n",
      "Epoch: 289 - Loss: 0.2907564715378518\n",
      "Epoch: 290 - Loss: 0.29073043629885187\n",
      "Epoch: 291 - Loss: 0.29070927607148034\n",
      "Epoch: 292 - Loss: 0.29068426123301433\n",
      "Epoch: 293 - Loss: 0.29065524274168286\n",
      "Epoch: 294 - Loss: 0.2906453590633853\n",
      "Epoch: 295 - Loss: 0.2906247452484506\n",
      "Epoch: 296 - Loss: 0.29063483077707436\n",
      "Epoch: 297 - Loss: 0.2906005639969687\n",
      "Epoch: 298 - Loss: 0.29058358253281313\n",
      "Epoch: 299 - Loss: 0.29055450638683805\n",
      "Epoch: 300 - Loss: 0.2905328400198333\n",
      "Epoch: 301 - Loss: 0.29051592830491957\n",
      "Epoch: 302 - Loss: 0.2904871009743743\n",
      "Epoch: 303 - Loss: 0.2904583805265485\n",
      "Epoch: 304 - Loss: 0.290436482343514\n",
      "Epoch: 305 - Loss: 0.29041933922576235\n",
      "Epoch: 306 - Loss: 0.29040738792609594\n",
      "Epoch: 307 - Loss: 0.2903963365425882\n",
      "Epoch: 308 - Loss: 0.2903817987538055\n",
      "Epoch: 309 - Loss: 0.29036281677418957\n",
      "Epoch: 310 - Loss: 0.29033550757334564\n",
      "Epoch: 311 - Loss: 0.2902919119460606\n",
      "Epoch: 312 - Loss: 0.2902867067123858\n",
      "Epoch: 313 - Loss: 0.2902687904154901\n",
      "Epoch: 314 - Loss: 0.29024392046200714\n",
      "Epoch: 315 - Loss: 0.2902111123640793\n",
      "Epoch: 316 - Loss: 0.2902013706797464\n",
      "Epoch: 317 - Loss: 0.29019573731149123\n",
      "Epoch: 318 - Loss: 0.2901823359291943\n",
      "Epoch: 319 - Loss: 0.29015977805251253\n",
      "Epoch: 320 - Loss: 0.29014267043005154\n",
      "Epoch: 321 - Loss: 0.29008967805462743\n",
      "Epoch: 322 - Loss: 0.2900854341113584\n",
      "Epoch: 323 - Loss: 0.2900776794197364\n",
      "Epoch: 324 - Loss: 0.2900348498524625\n",
      "Epoch: 325 - Loss: 0.29001865967929846\n",
      "Epoch: 326 - Loss: 0.29000892680247115\n",
      "Epoch: 327 - Loss: 0.28997199485865977\n",
      "Epoch: 328 - Loss: 0.2899660697534952\n",
      "Epoch: 329 - Loss: 0.2899488196894221\n",
      "Epoch: 330 - Loss: 0.2899701374355398\n",
      "Epoch: 331 - Loss: 0.2899536904532639\n",
      "Epoch: 332 - Loss: 0.2899387058427702\n",
      "Epoch: 333 - Loss: 0.28992292781216694\n",
      "Epoch: 334 - Loss: 0.2899081292938831\n",
      "Epoch: 335 - Loss: 0.2898848249900766\n",
      "Epoch: 336 - Loss: 0.28987224670733314\n",
      "Epoch: 337 - Loss: 0.2898474299168284\n",
      "Epoch: 338 - Loss: 0.2898082952424866\n",
      "Epoch: 339 - Loss: 0.28980192463172705\n",
      "Epoch: 340 - Loss: 0.2897710826313318\n",
      "Epoch: 341 - Loss: 0.2897631028812908\n",
      "Epoch: 342 - Loss: 0.2897231275997943\n",
      "Epoch: 343 - Loss: 0.2897060001261677\n",
      "Epoch: 344 - Loss: 0.2896851464188058\n",
      "Epoch: 345 - Loss: 0.28964010216762653\n",
      "Epoch: 346 - Loss: 0.28963789371957743\n",
      "Epoch: 347 - Loss: 0.2896481262496204\n",
      "Epoch: 348 - Loss: 0.2896349204567382\n",
      "Epoch: 349 - Loss: 0.289627148100905\n",
      "Epoch: 350 - Loss: 0.28958391840264575\n",
      "Epoch: 351 - Loss: 0.2895788605762774\n",
      "Epoch: 352 - Loss: 0.28958339993031795\n",
      "Epoch: 353 - Loss: 0.2895576650132399\n",
      "Epoch: 354 - Loss: 0.2895388695964104\n",
      "Epoch: 355 - Loss: 0.28952403681554284\n",
      "Epoch: 356 - Loss: 0.28951157763766794\n",
      "Epoch: 357 - Loss: 0.28948570922023065\n",
      "Epoch: 358 - Loss: 0.2894866138720809\n",
      "Epoch: 359 - Loss: 0.28945685198511445\n",
      "Epoch: 360 - Loss: 0.2894476139154827\n",
      "Epoch: 361 - Loss: 0.28943948637737044\n",
      "Epoch: 362 - Loss: 0.2894173925714358\n",
      "Epoch: 363 - Loss: 0.28942418085544147\n",
      "Epoch: 364 - Loss: 0.2894125737304493\n",
      "Epoch: 365 - Loss: 0.28940712047229034\n",
      "Epoch: 366 - Loss: 0.2894173076841685\n",
      "Epoch: 367 - Loss: 0.2894243617690805\n",
      "Epoch: 368 - Loss: 0.2893639918572069\n",
      "Epoch: 369 - Loss: 0.28938322162709684\n",
      "Epoch: 370 - Loss: 0.28933472008795974\n",
      "Epoch: 371 - Loss: 0.28931335874105313\n",
      "Epoch: 372 - Loss: 0.2893112458275473\n",
      "Epoch: 373 - Loss: 0.2892970673476002\n",
      "Epoch: 374 - Loss: 0.2892617288212185\n",
      "Epoch: 375 - Loss: 0.289266803496272\n",
      "Epoch: 376 - Loss: 0.28925635440664543\n",
      "Epoch: 377 - Loss: 0.28921295235599154\n",
      "Epoch: 378 - Loss: 0.28920733712532837\n",
      "Epoch: 379 - Loss: 0.28917502770703235\n",
      "Epoch: 380 - Loss: 0.2891582073333671\n",
      "Epoch: 381 - Loss: 0.2891405591663273\n",
      "Epoch: 382 - Loss: 0.28914315920316414\n",
      "Epoch: 383 - Loss: 0.28911155755145557\n",
      "Epoch: 384 - Loss: 0.28911983170467825\n",
      "Epoch: 385 - Loss: 0.2890984446327804\n",
      "Epoch: 386 - Loss: 0.28909019465743796\n",
      "Epoch: 387 - Loss: 0.2890706607978032\n",
      "Epoch: 388 - Loss: 0.28907572894546024\n",
      "Epoch: 389 - Loss: 0.28904796753572837\n",
      "Epoch: 390 - Loss: 0.2890470866238502\n",
      "Epoch: 391 - Loss: 0.28902509009282984\n",
      "Epoch: 392 - Loss: 0.2890114314750781\n",
      "Epoch: 393 - Loss: 0.28903337503999393\n",
      "Epoch: 394 - Loss: 0.28898189065009106\n",
      "Epoch: 395 - Loss: 0.288972962003818\n",
      "Epoch: 396 - Loss: 0.288954363787937\n",
      "Epoch: 397 - Loss: 0.28891900133721643\n",
      "Epoch: 398 - Loss: 0.2889138670492205\n",
      "Epoch: 399 - Loss: 0.2889075797374958\n",
      "Epoch: 400 - Loss: 0.28889293494590396\n",
      "Epoch: 401 - Loss: 0.2888570781436953\n",
      "Epoch: 402 - Loss: 0.2888404592707152\n",
      "Epoch: 403 - Loss: 0.28883082276163685\n",
      "Epoch: 404 - Loss: 0.2888080663731097\n",
      "Epoch: 405 - Loss: 0.28881619619375937\n",
      "Epoch: 406 - Loss: 0.2887873668696362\n",
      "Epoch: 407 - Loss: 0.2887680395266519\n",
      "Epoch: 408 - Loss: 0.28876778228542155\n",
      "Epoch: 409 - Loss: 0.2887444238383023\n",
      "Epoch: 410 - Loss: 0.28871945605971044\n",
      "Epoch: 411 - Loss: 0.2887084578801567\n",
      "Epoch: 412 - Loss: 0.28870896844230814\n",
      "Epoch: 413 - Loss: 0.2886934261260756\n",
      "Epoch: 414 - Loss: 0.2886551158622933\n",
      "Epoch: 415 - Loss: 0.28862827800927165\n",
      "Epoch: 416 - Loss: 0.28863618088260273\n",
      "Epoch: 417 - Loss: 0.2886058539925652\n",
      "Epoch: 418 - Loss: 0.288602761574486\n",
      "Epoch: 419 - Loss: 0.2885697411384702\n",
      "Epoch: 420 - Loss: 0.2885633476126258\n",
      "Epoch: 421 - Loss: 0.2885425674441946\n",
      "Epoch: 422 - Loss: 0.28851484096749297\n",
      "Epoch: 423 - Loss: 0.2884949612196264\n",
      "Epoch: 424 - Loss: 0.2885074471131521\n",
      "Epoch: 425 - Loss: 0.28848372437013636\n",
      "Epoch: 426 - Loss: 0.2884780732681507\n",
      "Epoch: 427 - Loss: 0.288445680913655\n",
      "Epoch: 428 - Loss: 0.28842787017314164\n",
      "Epoch: 429 - Loss: 0.2884384014612393\n",
      "Epoch: 430 - Loss: 0.28839658911337046\n",
      "Epoch: 431 - Loss: 0.2884029326343407\n",
      "Epoch: 432 - Loss: 0.28838409811172316\n",
      "Epoch: 433 - Loss: 0.28837203742764844\n",
      "Epoch: 434 - Loss: 0.28835480510621897\n",
      "Epoch: 435 - Loss: 0.28833410263512393\n",
      "Epoch: 436 - Loss: 0.2883198761775603\n",
      "Epoch: 437 - Loss: 0.28830717993500105\n",
      "Epoch: 438 - Loss: 0.2883013978045523\n",
      "Epoch: 439 - Loss: 0.28829055244401436\n",
      "Epoch: 440 - Loss: 0.2882739731185705\n",
      "Epoch: 441 - Loss: 0.2882859573612137\n",
      "Epoch: 442 - Loss: 0.2882546992625398\n",
      "Epoch: 443 - Loss: 0.2882437294079645\n",
      "Epoch: 444 - Loss: 0.2882335579592422\n",
      "Epoch: 445 - Loss: 0.28822097666634267\n",
      "Epoch: 446 - Loss: 0.2882144353871946\n",
      "Epoch: 447 - Loss: 0.2881950026164203\n",
      "Epoch: 448 - Loss: 0.28819150015164197\n",
      "Epoch: 449 - Loss: 0.2882012731200481\n",
      "Epoch: 450 - Loss: 0.2881908768734956\n",
      "Epoch: 451 - Loss: 0.28816311350910995\n",
      "Epoch: 452 - Loss: 0.28818602328897197\n",
      "Epoch: 453 - Loss: 0.2881397878316297\n",
      "Epoch: 454 - Loss: 0.2881839746431191\n",
      "Epoch: 455 - Loss: 0.2881526561870347\n",
      "Epoch: 456 - Loss: 0.2881210809966049\n",
      "Epoch: 457 - Loss: 0.28808619335752184\n",
      "Epoch: 458 - Loss: 0.28808077767423396\n",
      "Epoch: 459 - Loss: 0.28808277219959055\n",
      "Epoch: 460 - Loss: 0.2880582133849663\n",
      "Epoch: 461 - Loss: 0.2880377844268079\n",
      "Epoch: 462 - Loss: 0.2880416474510432\n",
      "Epoch: 463 - Loss: 0.2880179567615504\n",
      "Epoch: 464 - Loss: 0.2880345443355754\n",
      "Epoch: 465 - Loss: 0.28802124903802456\n",
      "Epoch: 466 - Loss: 0.2879734040182374\n",
      "Epoch: 467 - Loss: 0.28799413610635466\n",
      "Epoch: 468 - Loss: 0.2879683650642103\n",
      "Epoch: 469 - Loss: 0.2879434700631022\n",
      "Epoch: 470 - Loss: 0.2879705476949814\n",
      "Epoch: 471 - Loss: 0.28794792108795525\n",
      "Epoch: 472 - Loss: 0.2879558822589201\n",
      "Epoch: 473 - Loss: 0.2879166153477288\n",
      "Epoch: 474 - Loss: 0.2879280881215277\n",
      "Epoch: 475 - Loss: 0.28789758605136995\n",
      "Epoch: 476 - Loss: 0.2879298438494126\n",
      "Epoch: 477 - Loss: 0.2879091943844393\n",
      "Epoch: 478 - Loss: 0.2879062138935781\n",
      "Epoch: 479 - Loss: 0.28786170788962145\n",
      "Epoch: 480 - Loss: 0.2878533003531487\n",
      "Epoch: 481 - Loss: 0.2878511578149009\n",
      "Epoch: 482 - Loss: 0.28785850310226724\n",
      "Epoch: 483 - Loss: 0.28786207928098556\n",
      "Epoch: 484 - Loss: 0.2878147347175406\n",
      "Epoch: 485 - Loss: 0.287834276097272\n",
      "Epoch: 486 - Loss: 0.28779858764484967\n",
      "Epoch: 487 - Loss: 0.28779687600172715\n",
      "Epoch: 488 - Loss: 0.2877630150773115\n",
      "Epoch: 489 - Loss: 0.28776873092583105\n",
      "Epoch: 490 - Loss: 0.287769193626902\n",
      "Epoch: 491 - Loss: 0.2877328198870039\n",
      "Epoch: 492 - Loss: 0.2877441009801235\n",
      "Epoch: 493 - Loss: 0.2877288774897789\n",
      "Epoch: 494 - Loss: 0.2876989730259349\n",
      "Epoch: 495 - Loss: 0.28771910287309543\n",
      "Epoch: 496 - Loss: 0.28773081988808563\n",
      "Epoch: 497 - Loss: 0.28766852976338825\n",
      "Epoch: 498 - Loss: 0.28770495428029164\n",
      "Epoch: 499 - Loss: 0.28769383241553825\n",
      "Epoch: 500 - Loss: 0.2876800779533638\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.classify(pred_file + \".txt\")\n",
    "# preds, t1, t2 = model.classify(pred_file + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "\n",
    "# products\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    3844\n",
       "neg    2670\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    3398\n",
       "pos    3116\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.21%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
