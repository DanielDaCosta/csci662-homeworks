{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file, threshold=0, max_features=None):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "        # self.vocabulary = list(self.embedding_matrix.keys())\n",
    "        # self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        # self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features-1))\n",
    "\n",
    "        # Add to vocabulary the Out-of-Vocabulary token\n",
    "        return list(flattened_list_count_filter.keys()) + ['UNK']\n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        # Create Vocabulary\n",
    "        self.vocabulary = self.create_vocabulary(tokenized_text, self.threshold, self.max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)\n",
    "    \n",
    "    def sort_by_tfidf(self, tfidf_matrix, max_seq_length):\n",
    "        \"\"\"Sort input documents based on tf*idf score.\n",
    "        Return top \"max_seq_length\" words\n",
    "        :param: tfidf_matrix\n",
    "        :param: max_seq_length\n",
    "        :return: sentences ordered by TF-IDF score\n",
    "        \"\"\"\n",
    "        \n",
    "        # Indices of sorted matrix in descending order\n",
    "        indices = np.argsort(-tfidf_matrix, axis=1)\n",
    "        tfidf_matrix_sorted = []\n",
    "\n",
    "        # Create sorted matrix\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            # sentence in orderd version\n",
    "            tmp = [self.index2word[index] for index in indices[i][:max_seq_length]]\n",
    "            tfidf_matrix_sorted.append(tmp)\n",
    "    \n",
    "        return tfidf_matrix_sorted\n",
    "    \n",
    "    def get_features_tfidf(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothinf\n",
    "        return tf*idf_array\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from nn_layers import FeedForwardNetwork\n",
    "\n",
    "class NeuralModel(Model):\n",
    "    def __init__(self, embeddingfile,\n",
    "                 max_seq_length,\n",
    "                 hidden_units, minibatch_size,\n",
    "                 learning_rate,\n",
    "                 epochs,\n",
    "                 hidden_units_other_layers=[],\n",
    "                 tfidf=False,\n",
    "                 max_features=None,\n",
    "                 threshold=0,\n",
    "                 momentum=0,\n",
    "                 average_emb_sentence=False):\n",
    "        '''\n",
    "        :param embeddingfile: word embedding file\n",
    "        :param hidden_units: number of hidden units\n",
    "        :param minibatch_size: mini-batch size\n",
    "        :param learning_rate: learning_rate: learning\n",
    "        :param epochs: number of epochs to train for\n",
    "        :param hidden_units_other_layers (list): number of hidden units in each layer\n",
    "        :param tfidf: Enable TF-IDF ranking\n",
    "        :param threshold: TF-IDF Vocabulary size\n",
    "        :param momentum: TF-IDF Minimum word frequency required\n",
    "        :param average_emb_sentence: Compute the average of the embeddings in the sentence instead of concatenation\n",
    "        '''\n",
    "        # self.network = FeedForwardNetwork()\n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.average_emb_sentence = average_emb_sentence\n",
    "\n",
    "        self.hidden_units = [hidden_units] +  hidden_units_other_layers if len(hidden_units_other_layers) > 0 else [hidden_units]\n",
    "        # self.hidden_units = hidden_units if type(hidden_units) == list else [hidden_units] # list or int\n",
    "        self.n_hidden_layers = len(self.hidden_units)\n",
    "        self.weights = [None]*(self.n_hidden_layers + 1)\n",
    "        self.bias = [None]*(self.n_hidden_layers + 1)\n",
    "\n",
    "\n",
    "        self.Y_to_categorical = None\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = {}\n",
    "        # TF-IDF Sorting\n",
    "        self.tfidf = tfidf # enable sorting by tf-idf score\n",
    "        self.max_features = max_features\n",
    "        self.threshold = threshold\n",
    "        # Momentum\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def initialize_weights(self, n_inputs, n_output):\n",
    "        # weights = np.zeros((n_inputs, n_output))\n",
    "        # bias = np.zeros(n_output)\n",
    "        # np.random.seed(0)\n",
    "        weights = np.random.rand(n_inputs, n_output)\n",
    "        bias = np.random.rand(n_output)\n",
    "        return weights, bias\n",
    "    \n",
    "    def relu_function(self, A):\n",
    "        '''A = x*W + b\n",
    "\n",
    "        :return: Z = relut(x*A+b)\n",
    "        '''\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def relu_derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        S = np.maximum(epsilon, S)\n",
    "        S = np.minimum(1 - epsilon, S)\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z_i = np.dot(X, self.weights[0]) + self.bias[0]\n",
    "        A_i = self.relu_function(Z_i)\n",
    "        i = 0\n",
    "        if self.n_hidden_layers > 1:\n",
    "            for i in range(self.n_hidden_layers-1):\n",
    "                Z_i = np.dot(A_i, self.weights[i+1]) + self.bias[i+1]\n",
    "                A_i = self.relu_function(Z_i)\n",
    "\n",
    "            i = i + 1\n",
    "        Z_i = np.dot(A_i, self.weights[i+1]) + self.bias[i+1]\n",
    "        O = self.softmax(Z_i)\n",
    "\n",
    "        # Rows with highest probability\n",
    "        S_max = np.argmax(O, axis=1)\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "    def convert_to_embeddings(self, sentence, average_emb_sentence=False):\n",
    "        '''Convert sentence to embeddings.\n",
    "\n",
    "        :param average_sentence:  Compute the element-wise average of a list of sentence embedding\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "        if not average_emb_sentence:\n",
    "            if emb: # if there is a word\n",
    "                emb_concat = np.concatenate(emb, axis=0)\n",
    "            else:\n",
    "                emb_concat = []\n",
    "            # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "            if len(emb) < self.max_seq_length:\n",
    "                # Missing words\n",
    "                words_missing = self.max_seq_length - len(emb)\n",
    "                # print(words_missing)\n",
    "                emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        else:\n",
    "            # Compute average of the sentence\n",
    "            if len(emb) > 0:\n",
    "                stacked_arrays = np.vstack(emb)\n",
    "                emb_concat = np.mean(stacked_arrays, axis=0)\n",
    "            else:\n",
    "                emb_concat = np.zeros(self.embedding_dim)\n",
    "\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile, threshold=self.threshold, max_features=self.max_features)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        # Convert to OneHot for computing Loss\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim × features (u)\n",
    "        if not self.average_emb_sentence:\n",
    "            n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        else:\n",
    "            n_inputs = self.embedding_dim\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        if self.tfidf: # Truncate input to the max sequence length sorted by TF-IDF\n",
    "            tf_idf = features_ff_class.tf_idf(features_ff_class.tokenized_text)\n",
    "            trunc_tokenized_text = features_ff_class.sort_by_tfidf(\n",
    "                tf_idf,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "                features_ff_class.tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence, average_emb_sentence=self.average_emb_sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Weights\n",
    "        # Create W_a and b_a\n",
    "        # W_0[n_documents, hidden_units (u)]\n",
    "        # b_0[hidden_units (u)]\n",
    "\n",
    "        list_of_sizes = [n_inputs] + self.hidden_units + [num_labels]\n",
    "        for i in range(self.n_hidden_layers + 1):\n",
    "            weights, bias = self.initialize_weights(list_of_sizes[i], list_of_sizes[i+1])\n",
    "            self.weights[i] = weights\n",
    "            self.bias[i] = bias\n",
    "\n",
    "        # Initilze Momentum weights\n",
    "        prev_dW_i = [0] * (self.n_hidden_layers + 1)\n",
    "        prev_db_i = [0] * (self.n_hidden_layers + 1)\n",
    "\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        for n_epoch in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "                # List of outputs of each layer\n",
    "                # A[0] -> Input Layer\n",
    "                # A[.] => Hidden Layer\n",
    "                # A[n] -> Ouput Layer\n",
    "                A = [None]*(self.n_hidden_layers + 2) \n",
    "                Z = [None]*(self.n_hidden_layers + 2)\n",
    "                # ---------------- Input Layer --------------- #\n",
    "                A[0] = X_mini_batch\n",
    "                Z[0] = X_mini_batch\n",
    "\n",
    "                # ---------------- Hidden Layers --------------- #\n",
    "                for i in range(self.n_hidden_layers):\n",
    "                    # Z_i = np.dot(X_mini_batch, self.weights_i) + self.bias_i\n",
    "                    # A_i = relu(Z_i)\n",
    "                    Z_tmp = np.dot(A[i], self.weights[i]) + self.bias[i]\n",
    "                    Z[i+1] = Z_tmp\n",
    "                    A_tmp = self.relu_function(Z_tmp)\n",
    "                    A[i+1] = A_tmp\n",
    "                # ---------------- Hidden-to-Output Layer --------------- #\n",
    "\n",
    "                i = i + 1\n",
    "                # print(i)\n",
    "                Z_output_layer = np.dot(A[i], self.weights[self.n_hidden_layers]) + self.bias[self.n_hidden_layers]\n",
    "                Z[i+1] = Z_output_layer\n",
    "                A_output_layer = self.softmax(Z_output_layer)\n",
    "                A[i+1] = A_output_layer\n",
    "\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "\n",
    "                # Compute Gradients\n",
    "                # List of output gradients of each layer\n",
    "                # dZ[0] -> Input Layer\n",
    "                # dZ[.] => Hidden Layer\n",
    "                # dZ[n] -> Ouput Layer\n",
    "                dZ = [None] * (self.n_hidden_layers + 1)\n",
    "\n",
    "                # dW[previous_layer, next_layer]\n",
    "                dW = [None] * (self.n_hidden_layers + 1)\n",
    "                db = [None] * (self.n_hidden_layers + 1)\n",
    "                dZ[-1] = A[-1] - y_mini_batch\n",
    "                for i in range(self.n_hidden_layers, 0, -1):\n",
    "                    dW[i] = (1/minibatch_size)*np.dot(A[i].T, dZ[i])\n",
    "                    db[i] = (1/minibatch_size)*np.sum(dZ[i], axis=0, keepdims = True)\n",
    "                    dZ[i-1] = np.dot(dZ[i], self.weights[i].T)*self.relu_derivative(Z[i])\n",
    "\n",
    "                # print(dZ[i-1])\n",
    "                dW[0] = (1/minibatch_size)*np.dot(X_mini_batch.T, dZ[i-1])\n",
    "                db[0] = (1/minibatch_size)*np.sum(dZ[i-1], axis=0, keepdims = True)\n",
    "\n",
    "                # Update Weights\n",
    "                for i in range(self.n_hidden_layers + 1):\n",
    "                    self.weights[i] = self.weights[i] - (self.learning_rate*dW[i] + self.momentum*prev_dW_i[i])\n",
    "                    self.bias[i] = self.bias[i] - (self.learning_rate*db[i] + self.momentum*prev_db_i[i])\n",
    "                    # Momentum\n",
    "                    # Save previous gradients for Momentum\n",
    "                    prev_dW_i[i] =  self.learning_rate*dW[i] + self.momentum*prev_dW_i[i]\n",
    "                    prev_db_i[i] =  self.learning_rate*db[i] + self.momentum*prev_db_i[i]\n",
    "\n",
    "                ########\n",
    "                # Loss #\n",
    "                ########\n",
    "                # print(y_mini_batch)\n",
    "                mini_batch_loss.append(self.cross_entropy_loss(A[-1], y_mini_batch))\n",
    "\n",
    "            loss = np.mean(mini_batch_loss)\n",
    "            self.loss[n_epoch] = loss\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {n_epoch+1} - Loss: {loss}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        if self.tfidf:\n",
    "            tf_idf_inference = []\n",
    "            # Get features from inference file\n",
    "            for sentence in tokenized_text:\n",
    "                # Transform dataset to TF-IDF space\n",
    "                # Return features with format (1, size_vocabulary)\n",
    "                X_sentence = self.features_ff_class.get_features_tfidf(sentence, self.features_ff_class.idf)\n",
    "                tf_idf_inference.append(X_sentence)\n",
    "            tf_idf_inference = np.stack(tf_idf_inference)\n",
    "            trunc_tokenized_text = self.features_ff_class.sort_by_tfidf(\n",
    "                tf_idf_inference,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "                tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence, average_emb_sentence=self.average_emb_sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Make Prediction\n",
    "        y_test = self.predict(X_test)\n",
    "        preds_label = []\n",
    "        for y in y_test:\n",
    "            tmp = self.Y_to_categorical[y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "# train_file = \"datasets/products/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "# model_file_name = \"products.model\"\n",
    "# loss_file = \"datasets/products/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4) # 65.6%\n",
    "\n",
    "# Validation #\n",
    "\n",
    "\n",
    "# 1 Layer\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500) # 62.10%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=1000, threshold=2) # 66.23%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=1000, threshold=2, momentum=0.4) #66.17\n",
    "\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=50, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=1000, threshold=1, momentum=0.9, average_emb_sentence=True) # 70.86%\n",
    "\n",
    "# 2 Layers\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=4, hidden_units_other_layers=[4], minibatch_size=32, learning_rate=0.02, epochs=200) # 67.33\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=4, hidden_units_other_layers=[4], minibatch_size=32, learning_rate=0.02, epochs=200, tfidf=True, max_features=1000, threshold=0) # 59.01\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=4, hidden_units_other_layers=[4], minibatch_size=32, learning_rate=0.02, epochs=200, tfidf=True, max_features=1000, threshold=0, momentum=0.1)\n",
    "\n",
    "##BEST MODEL###\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=10, hidden_units_other_layers=[10], minibatch_size=32, learning_rate=0.02, epochs=200, tfidf=True, max_features=1000, threshold=1, momentum=0.4, average_emb_sentence=True) \n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=10, hidden_units_other_layers=[10, 10], minibatch_size=32, learning_rate=0.005, epochs=200, tfidf=True, max_features=1000, threshold=1, momentum=0.4, average_emb_sentence=True) \n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=10, hidden_units_other_layers=[10], minibatch_size=32, learning_rate=0.02, epochs=200, tfidf=False, max_features=1000, threshold=1, momentum=0.4, average_emb_sentence=True) \n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=256, learning_rate=0.1, epochs=500, tfidf=True, max_features=500, threshold=2) # 64%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=[15, 3], minibatch_size=32, learning_rate=0.002, epochs=100) # 65%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4)\n",
    "\n",
    "# 2 Layers\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[20, 10], minibatch_size=256, learning_rate=0.001, epochs=100, tfidf=True, max_features=500, threshold=2, momentum=0.9) # 59.01%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=[10, 10], minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4) # 59.01%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[10, 10], minibatch_size=32, learning_rate=0.0001, epochs=100) # 58.98%\n",
    "\n",
    "\n",
    "# 3 Layers\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[2, 2, 2], minibatch_size=128, learning_rate=0.2, momentum=0, epochs=100)  # Accuracy: 62.73%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[2, 2, 2], minibatch_size=128, learning_rate=0.2, momentum=0.1, epochs=100)  # Accuracy: 63.23%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.02, momentum=0.1, epochs=200)  # Accuracy: 65.77%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.02, momentum=0.1, epochs=200)  # Accuracy: 66.40%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.02, momentum=0.1, epochs=200)  # Accuracy: 68.27%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.015, momentum=0.1, epochs=200)\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=32, learning_rate=0.05, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=128, learning_rate=0.05, epochs=100, tfidf=True, threshold=2, max_features=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=100, minibatch_size=256, learning_rate=0.01, epochs=200) # 34.62%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=100, minibatch_size=256, learning_rate=0.005, epochs=200) # 35.58%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.003, epochs=500, momentum=0.9) # 37.82%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9) # 41.99%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=2, max_features=1000) # Accuracy: 44.55%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=0, max_features=10000) # Accuracy: 46.47%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[80, 50], minibatch_size=128, learning_rate=0.0001, epochs=500, momentum=0.9) # 28.21%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[5, 5, 5], minibatch_size=128, learning_rate=0.0001, epochs=500, momentum=0.9) \n",
    "\n",
    "\n",
    "# Validation #\n",
    "\n",
    "# 1 Layer\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=32, learning_rate=0.002, epochs=500) # Accuracy: 46.47%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=32, learning_rate=0.002, epochs=500, tfidf=True, threshold=0, max_features=10000) # Accuracy: 45.51%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=32, learning_rate=0.002, epochs=500, tfidf=True, threshold=0, max_features=10000,momentum=0.4) # Accuracy: 45.51%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=0, max_features=10000) # Accuracy: 46.47%\n",
    "\n",
    "# AVG Embedding\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=0, max_features=10000, average_emb_sentence=True) # Accuracy: 56.09%\n",
    "\n",
    "##BEST MODEL###\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=80, minibatch_size=64, learning_rate=0.0015, epochs=500, momentum=0.95, tfidf=True, threshold=1, max_features=1000, average_emb_sentence=True) # Accuracy: 65.06%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=80, hidden_units_other_layers=[10, 10], minibatch_size=64, learning_rate=0.0001, epochs=500, momentum=0.95, tfidf=True, threshold=1, max_features=1000, average_emb_sentence=True) # Accuracy: 65.06%\n",
    "\n",
    "# 2 Layer\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=50, hidden_units_other_layers=[50], minibatch_size=64, learning_rate=0.001, epochs=200) # 28.53%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=50, hidden_units_other_layers=[50], minibatch_size=64, learning_rate=0.001, epochs=200, tfidf=True, threshold=0, max_features=10000)  # 29.81%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=50, hidden_units_other_layers=[50], minibatch_size=64, learning_rate=0.001, epochs=200, tfidf=True, threshold=0, max_features=10000, momentum=0.2) #28.85%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=50, hidden_units_other_layers=[50], minibatch_size=32, learning_rate=0.002, epochs=300, tfidf=True, threshold=1, max_features=10000, momentum=0.5, average_emb_sentence=True) # Accuracy: 57.37%\n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100) # Accuracy: 65.77%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=9, minibatch_size=64, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 66.14%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=9, minibatch_size=64, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 67.24%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=[5, 5], minibatch_size=128, learning_rate=0.01, epochs=300, momentum=0.9)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[5, 5, 5], minibatch_size=32, learning_rate=0.01, epochs=300, momentum=0.9) # Accuracy: 63.33%\n",
    "\n",
    "# Validation\n",
    "# 1 Layer\n",
    "\n",
    "##BEST MODEL###\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=32, learning_rate=0.1, epochs=200, average_emb_sentence=True) # Accuracy: 65.77%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=200, tfidf=True, threshold=0, max_features=1000) # Accuracy: 45.60%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=200, tfidf=True, threshold=0, max_features=1000, momentum=0.2) # Accuracy: 45.60%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=50, minibatch_size=128, learning_rate=0.2, epochs=500, tfidf=True, threshold=1, max_features=1000, momentum=0.4, average_emb_sentence=True) # Accuracy: 60.27%\n",
    "\n",
    "# 2 Layer\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.025, epochs=400) # Accuracy: 61.37%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.025, epochs=400, tfidf=True, threshold=1, max_features=5000, momentum=0.4)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=5, hidden_units_other_layers=[5, 5], minibatch_size=128, learning_rate=0.2, epochs=400, tfidf=True, threshold=1, max_features=10000, momentum=0, average_emb_sentence=True) # Accuracy: 61.86%\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "train_file = \"datasets/odiya/train.txt\"\n",
    "emb_file = \"fasttext.wiki.300d.vec\"\n",
    "pred_file = \"datasets/odiya/val.test\"\n",
    "pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "model_file_name = \"odia.model\"\n",
    "loss_file = \"datasets/odiya/loss.txt\"\n",
    "\n",
    "# Validation\n",
    "# 1 Layer\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100) # Accuracy: 75.89%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100,  tfidf=True, threshold=2, max_features=1000) # Accuracy: 74.84%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100,  tfidf=True, threshold=2, max_features=1000, momentum=0.2) # Accuracy: 75.66%\n",
    "\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=40, minibatch_size=32, learning_rate=0.12, epochs=200,  tfidf=True, threshold=2, max_features=1000, momentum=0.4, average_emb_sentence=True) # Accuracy: 78.52%\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100, momentum=0.9) # Accuracy: 78.29%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=200, momentum=0.9) # Accuracy: 79.11%\n",
    "\n",
    "# 2 Layer\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.01, epochs=200) # Accuracy: 77.20%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.01, epochs=200, tfidf=True, threshold=1, max_features=1000) # Accuracy: 50.27%/\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.01, epochs=200, tfidf=True, threshold=1, max_features=1000, momentum=0.6) # Accuracy: 75.20%\n",
    "##BEST MODEL###\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=10, hidden_units_other_layers=[10], minibatch_size=128, learning_rate=0.01, epochs=200, tfidf=True, threshold=1, max_features=2000, momentum=0.9, average_emb_sentence=True)\n",
    "\n",
    "nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=10, hidden_units_other_layers=[10, 10], minibatch_size=128, learning_rate=0.0001, epochs=200, tfidf=True, threshold=1, max_features=2000, momentum=0.4, average_emb_sentence=True)\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.01, epochs=200, tfidf=True, threshold=1, max_features=1000, momentum=0.6, average_emb_sentence=False) # Accuracy: 50.27%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=[5, 5], minibatch_size=128, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 79.97%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[5, 5], minibatch_size=128, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 79.97%\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=5, hidden_units_other_layers=[5, 5], minibatch_size=32, learning_rate=0.001, epochs=200, momentum=0.9) # Accuracy: 81.32%\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 2.9633168168793698\n",
      "Epoch: 2 - Loss: 0.35326435225323183\n",
      "Epoch: 3 - Loss: 0.35325853458873785\n",
      "Epoch: 4 - Loss: 0.35325271327523877\n",
      "Epoch: 5 - Loss: 0.3532468884777687\n",
      "Epoch: 6 - Loss: 0.3532410607977945\n",
      "Epoch: 7 - Loss: 0.353235230300437\n",
      "Epoch: 8 - Loss: 0.3532293969090031\n",
      "Epoch: 9 - Loss: 0.3532235612113988\n",
      "Epoch: 10 - Loss: 0.35321772272170215\n",
      "Epoch: 11 - Loss: 0.35321188113960234\n",
      "Epoch: 12 - Loss: 0.3532060365459039\n",
      "Epoch: 13 - Loss: 0.3532001884944539\n",
      "Epoch: 14 - Loss: 0.35319433741872147\n",
      "Epoch: 15 - Loss: 0.3531884838676437\n",
      "Epoch: 16 - Loss: 0.35318262805290257\n",
      "Epoch: 17 - Loss: 0.3531767686086186\n",
      "Epoch: 18 - Loss: 0.35317090590484823\n",
      "Epoch: 19 - Loss: 0.35316503987619535\n",
      "Epoch: 20 - Loss: 0.3531591711328338\n",
      "Epoch: 21 - Loss: 0.3531532990014841\n",
      "Epoch: 22 - Loss: 0.3531474239972993\n",
      "Epoch: 23 - Loss: 0.35314154543655996\n",
      "Epoch: 24 - Loss: 0.353135664331243\n",
      "Epoch: 25 - Loss: 0.35312977955990993\n",
      "Epoch: 26 - Loss: 0.35312389129732785\n",
      "Epoch: 27 - Loss: 0.35311799968765933\n",
      "Epoch: 28 - Loss: 0.3531121046456068\n",
      "Epoch: 29 - Loss: 0.35310620681986754\n",
      "Epoch: 30 - Loss: 0.3531003053447993\n",
      "Epoch: 31 - Loss: 0.35309440009363113\n",
      "Epoch: 32 - Loss: 0.35308849056291336\n",
      "Epoch: 33 - Loss: 0.3530825760100089\n",
      "Epoch: 34 - Loss: 0.35307665710448394\n",
      "Epoch: 35 - Loss: 0.35307073434980435\n",
      "Epoch: 36 - Loss: 0.35306480773568744\n",
      "Epoch: 37 - Loss: 0.35305887804114267\n",
      "Epoch: 38 - Loss: 0.35305294490429334\n",
      "Epoch: 39 - Loss: 0.35304700716143966\n",
      "Epoch: 40 - Loss: 0.35304106508498073\n",
      "Epoch: 41 - Loss: 0.3530351194356534\n",
      "Epoch: 42 - Loss: 0.3530291705562871\n",
      "Epoch: 43 - Loss: 0.35302321813728726\n",
      "Epoch: 44 - Loss: 0.3530172617161406\n",
      "Epoch: 45 - Loss: 0.3530113011718829\n",
      "Epoch: 46 - Loss: 0.35300533634850784\n",
      "Epoch: 47 - Loss: 0.35299936834639745\n",
      "Epoch: 48 - Loss: 0.352993397437018\n",
      "Epoch: 49 - Loss: 0.35298742307439956\n",
      "Epoch: 50 - Loss: 0.3529814449209592\n",
      "Epoch: 51 - Loss: 0.3529754635907902\n",
      "Epoch: 52 - Loss: 0.3529694792224811\n",
      "Epoch: 53 - Loss: 0.3529634916742046\n",
      "Epoch: 54 - Loss: 0.35295750029058176\n",
      "Epoch: 55 - Loss: 0.3529515048074327\n",
      "Epoch: 56 - Loss: 0.3529455058306818\n",
      "Epoch: 57 - Loss: 0.35293950274122315\n",
      "Epoch: 58 - Loss: 0.3529334954767967\n",
      "Epoch: 59 - Loss: 0.35292748391061585\n",
      "Epoch: 60 - Loss: 0.3529214670946346\n",
      "Epoch: 61 - Loss: 0.3529154463258125\n",
      "Epoch: 62 - Loss: 0.352909421282629\n",
      "Epoch: 63 - Loss: 0.35290339181273817\n",
      "Epoch: 64 - Loss: 0.35289735886086454\n",
      "Epoch: 65 - Loss: 0.3528913220763935\n",
      "Epoch: 66 - Loss: 0.3528852809853713\n",
      "Epoch: 67 - Loss: 0.3528792357030666\n",
      "Epoch: 68 - Loss: 0.3528731858367595\n",
      "Epoch: 69 - Loss: 0.3528671306673707\n",
      "Epoch: 70 - Loss: 0.35286107085812546\n",
      "Epoch: 71 - Loss: 0.3528550078969146\n",
      "Epoch: 72 - Loss: 0.3528489403668858\n",
      "Epoch: 73 - Loss: 0.3528428680925403\n",
      "Epoch: 74 - Loss: 0.3528367917993956\n",
      "Epoch: 75 - Loss: 0.3528307103314706\n",
      "Epoch: 76 - Loss: 0.35282462442539736\n",
      "Epoch: 77 - Loss: 0.35281853486448467\n",
      "Epoch: 78 - Loss: 0.3528124416689656\n",
      "Epoch: 79 - Loss: 0.3528063441330566\n",
      "Epoch: 80 - Loss: 0.35280024117626807\n",
      "Epoch: 81 - Loss: 0.352794133090036\n",
      "Epoch: 82 - Loss: 0.3527880207075289\n",
      "Epoch: 83 - Loss: 0.35278190285893757\n",
      "Epoch: 84 - Loss: 0.35277578171669544\n",
      "Epoch: 85 - Loss: 0.35276965592418247\n",
      "Epoch: 86 - Loss: 0.35276352603315014\n",
      "Epoch: 87 - Loss: 0.35275739227455216\n",
      "Epoch: 88 - Loss: 0.35275125374921446\n",
      "Epoch: 89 - Loss: 0.3527451106004121\n",
      "Epoch: 90 - Loss: 0.352738964222128\n",
      "Epoch: 91 - Loss: 0.3527328135671064\n",
      "Epoch: 92 - Loss: 0.3527266578105422\n",
      "Epoch: 93 - Loss: 0.3527204970331236\n",
      "Epoch: 94 - Loss: 0.3527143331617815\n",
      "Epoch: 95 - Loss: 0.3527081649690068\n",
      "Epoch: 96 - Loss: 0.35270199248303774\n",
      "Epoch: 97 - Loss: 0.35269581589367555\n",
      "Epoch: 98 - Loss: 0.3526896344260876\n",
      "Epoch: 99 - Loss: 0.35268344699123777\n",
      "Epoch: 100 - Loss: 0.35267725311426107\n",
      "Epoch: 101 - Loss: 0.3526710544133485\n",
      "Epoch: 102 - Loss: 0.35266485072682535\n",
      "Epoch: 103 - Loss: 0.35265864171148\n",
      "Epoch: 104 - Loss: 0.3526524273539001\n",
      "Epoch: 105 - Loss: 0.3526462071820973\n",
      "Epoch: 106 - Loss: 0.3526399825033565\n",
      "Epoch: 107 - Loss: 0.3526337530214989\n",
      "Epoch: 108 - Loss: 0.3526275172516006\n",
      "Epoch: 109 - Loss: 0.35262127541078\n",
      "Epoch: 110 - Loss: 0.3526150275077938\n",
      "Epoch: 111 - Loss: 0.3526087742206628\n",
      "Epoch: 112 - Loss: 0.35260251411844407\n",
      "Epoch: 113 - Loss: 0.35259624848250215\n",
      "Epoch: 114 - Loss: 0.3525899785518332\n",
      "Epoch: 115 - Loss: 0.35258370344985246\n",
      "Epoch: 116 - Loss: 0.3525774220633236\n",
      "Epoch: 117 - Loss: 0.3525711350710636\n",
      "Epoch: 118 - Loss: 0.3525648425687932\n",
      "Epoch: 119 - Loss: 0.3525585452868235\n",
      "Epoch: 120 - Loss: 0.3525522423656721\n",
      "Epoch: 121 - Loss: 0.35254593378677224\n",
      "Epoch: 122 - Loss: 0.35253961937774847\n",
      "Epoch: 123 - Loss: 0.3525332985754493\n",
      "Epoch: 124 - Loss: 0.35252697178073633\n",
      "Epoch: 125 - Loss: 0.3525206392302806\n",
      "Epoch: 126 - Loss: 0.35251430167952474\n",
      "Epoch: 127 - Loss: 0.3525079592417342\n",
      "Epoch: 128 - Loss: 0.35250161074021974\n",
      "Epoch: 129 - Loss: 0.35249525672434084\n",
      "Epoch: 130 - Loss: 0.35248889649037635\n",
      "Epoch: 131 - Loss: 0.3524825301708828\n",
      "Epoch: 132 - Loss: 0.3524761577934535\n",
      "Epoch: 133 - Loss: 0.35246977839129295\n",
      "Epoch: 134 - Loss: 0.3524633923120489\n",
      "Epoch: 135 - Loss: 0.3524569995612445\n",
      "Epoch: 136 - Loss: 0.35245060011437473\n",
      "Epoch: 137 - Loss: 0.35244419339710537\n",
      "Epoch: 138 - Loss: 0.35243778123534436\n",
      "Epoch: 139 - Loss: 0.3524313629545857\n",
      "Epoch: 140 - Loss: 0.3524249399855666\n",
      "Epoch: 141 - Loss: 0.3524185122781028\n",
      "Epoch: 142 - Loss: 0.35241207997841706\n",
      "Epoch: 143 - Loss: 0.3524056423727596\n",
      "Epoch: 144 - Loss: 0.35239919802324793\n",
      "Epoch: 145 - Loss: 0.352392747318214\n",
      "Epoch: 146 - Loss: 0.3523862905437441\n",
      "Epoch: 147 - Loss: 0.35237982686112146\n",
      "Epoch: 148 - Loss: 0.3523733571162515\n",
      "Epoch: 149 - Loss: 0.3523668817513415\n",
      "Epoch: 150 - Loss: 0.35236040019710285\n",
      "Epoch: 151 - Loss: 0.35235391347804396\n",
      "Epoch: 152 - Loss: 0.3523474216328438\n",
      "Epoch: 153 - Loss: 0.35234092399829536\n",
      "Epoch: 154 - Loss: 0.35233442110526186\n",
      "Epoch: 155 - Loss: 0.3523279114632444\n",
      "Epoch: 156 - Loss: 0.3523213945748036\n",
      "Epoch: 157 - Loss: 0.352314870437476\n",
      "Epoch: 158 - Loss: 0.3523083402213959\n",
      "Epoch: 159 - Loss: 0.3523018035342458\n",
      "Epoch: 160 - Loss: 0.3522952627270972\n",
      "Epoch: 161 - Loss: 0.35228871506838466\n",
      "Epoch: 162 - Loss: 0.3522821575877505\n",
      "Epoch: 163 - Loss: 0.3522755920657391\n",
      "Epoch: 164 - Loss: 0.35226902204775046\n",
      "Epoch: 165 - Loss: 0.35226244639777116\n",
      "Epoch: 166 - Loss: 0.352255864634795\n",
      "Epoch: 167 - Loss: 0.3522492752418814\n",
      "Epoch: 168 - Loss: 0.35224267841273027\n",
      "Epoch: 169 - Loss: 0.3522360765600853\n",
      "Epoch: 170 - Loss: 0.35222946857035536\n",
      "Epoch: 171 - Loss: 0.3522228557449387\n",
      "Epoch: 172 - Loss: 0.3522162374813506\n",
      "Epoch: 173 - Loss: 0.35220961219061353\n",
      "Epoch: 174 - Loss: 0.35220297998735883\n",
      "Epoch: 175 - Loss: 0.35219634039780934\n",
      "Epoch: 176 - Loss: 0.3521896932677505\n",
      "Epoch: 177 - Loss: 0.35218304258897043\n",
      "Epoch: 178 - Loss: 0.35217639298920106\n",
      "Epoch: 179 - Loss: 0.35216973690543\n",
      "Epoch: 180 - Loss: 0.35216307268125063\n",
      "Epoch: 181 - Loss: 0.35215639869433374\n",
      "Epoch: 182 - Loss: 0.35214971786291605\n",
      "Epoch: 183 - Loss: 0.35214302907592776\n",
      "Epoch: 184 - Loss: 0.3521363330016162\n",
      "Epoch: 185 - Loss: 0.3521296299409114\n",
      "Epoch: 186 - Loss: 0.352122920634276\n",
      "Epoch: 187 - Loss: 0.35211620391241827\n",
      "Epoch: 188 - Loss: 0.3521094810256674\n",
      "Epoch: 189 - Loss: 0.3521027494136791\n",
      "Epoch: 190 - Loss: 0.35209600928305007\n",
      "Epoch: 191 - Loss: 0.35208926128903917\n",
      "Epoch: 192 - Loss: 0.35208250503282984\n",
      "Epoch: 193 - Loss: 0.3520757402660722\n",
      "Epoch: 194 - Loss: 0.3520689676472051\n",
      "Epoch: 195 - Loss: 0.35206218678187057\n",
      "Epoch: 196 - Loss: 0.3520553981709341\n",
      "Epoch: 197 - Loss: 0.35204860341117394\n",
      "Epoch: 198 - Loss: 0.35204180130153284\n",
      "Epoch: 199 - Loss: 0.35203499059840887\n",
      "Epoch: 200 - Loss: 0.35202817198250164\n",
      "CPU times: user 1min 13s, sys: 2.57 s, total: 1min 16s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = list(nn_model.loss.values())\n",
    "\n",
    "import csv\n",
    "with open(loss_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for item in loss:\n",
    "        writer.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = model.classify(pred_file + \".txt\")\n",
    "# preds, t1, t2 = model.classify(pred_file + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "\n",
    "# products\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           1345\n",
       "business          937\n",
       "entertainment     758\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports      3039\n",
       "business       1\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.28%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
