{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file, threshold=0, max_features=None):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "        # self.vocabulary = list(self.embedding_matrix.keys())\n",
    "        # self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        # self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features-1))\n",
    "\n",
    "        # Add to vocabulary the Out-of-Vocabulary token\n",
    "        return list(flattened_list_count_filter.keys()) + ['UNK']\n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        # Create Vocabulary\n",
    "        self.vocabulary = self.create_vocabulary(tokenized_text, self.threshold, self.max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.index2word = {i: word for i, word in enumerate(self.vocabulary, start=0)}\n",
    "\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)\n",
    "    \n",
    "    def sort_by_tfidf(self, tfidf_matrix, max_seq_length):\n",
    "        \"\"\"Sort input documents based on tf*idf score.\n",
    "        Return top \"max_seq_length\" words\n",
    "        :param: tfidf_matrix\n",
    "        :param: max_seq_length\n",
    "        :return: sentences ordered by TF-IDF score\n",
    "        \"\"\"\n",
    "        \n",
    "        # Indices of sorted matrix in descending order\n",
    "        indices = np.argsort(-tfidf_matrix, axis=1)\n",
    "        tfidf_matrix_sorted = []\n",
    "\n",
    "        # Create sorted matrix\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            # sentence in orderd version\n",
    "            tmp = [self.index2word[index] for index in indices[i][:max_seq_length]]\n",
    "            tfidf_matrix_sorted.append(tmp)\n",
    "    \n",
    "        return tfidf_matrix_sorted\n",
    "    \n",
    "    def get_features_tfidf(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothinf\n",
    "        return tf*idf_array\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from nn_layers import FeedForwardNetwork\n",
    "\n",
    "class NeuralModel(Model):\n",
    "    def __init__(self, embeddingfile,\n",
    "                 max_seq_length,\n",
    "                 hidden_units, minibatch_size,\n",
    "                 learning_rate,\n",
    "                 epochs,\n",
    "                 hidden_units_other_layers=[],\n",
    "                 tfidf=False,\n",
    "                 max_features=None,\n",
    "                 threshold=0,\n",
    "                 momentum=0):\n",
    "        '''\n",
    "        :param embeddingfile: word embedding file\n",
    "        :param hidden_units: number of hidden units\n",
    "        :param minibatch_size: mini-batch size\n",
    "        :param learning_rate: learning_rate: learning\n",
    "        :param epochs: number of epochs to train for\n",
    "        :param hidden_units_other_layers (list): number of hidden units in each layer\n",
    "        :param tfidf: Enable TF-IDF ranking\n",
    "        :param threshold: TF-IDF Vocabulary size\n",
    "        :param momentum: TF-IDF Minimum word frequency required\n",
    "        '''\n",
    "        # self.network = FeedForwardNetwork()\n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.hidden_units = [hidden_units] +  hidden_units_other_layers if len(hidden_units_other_layers) > 0 else [hidden_units]\n",
    "        # self.hidden_units = hidden_units if type(hidden_units) == list else [hidden_units] # list or int\n",
    "        self.n_hidden_layers = len(self.hidden_units)\n",
    "        self.weights = [None]*(self.n_hidden_layers + 1)\n",
    "        self.bias = [None]*(self.n_hidden_layers + 1)\n",
    "\n",
    "\n",
    "        self.Y_to_categorical = None\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = {}\n",
    "        # TF-IDF Sorting\n",
    "        self.tfidf = tfidf # enable sorting by tf-idf score\n",
    "        self.max_features = max_features\n",
    "        self.threshold = threshold\n",
    "        # Momentum\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def initialize_weights(self, n_inputs, n_output):\n",
    "        # weights = np.zeros((n_inputs, n_output))\n",
    "        # bias = np.zeros(n_output)\n",
    "        # np.random.seed(0)\n",
    "        weights = np.random.rand(n_inputs, n_output)\n",
    "        bias = np.random.rand(n_output)\n",
    "        return weights, bias\n",
    "    \n",
    "    def relu_function(self, A):\n",
    "        '''A = x*W + b\n",
    "\n",
    "        :return: Z = relut(x*A+b)\n",
    "        '''\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def relu_derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        S = np.maximum(epsilon, S)\n",
    "        S = np.minimum(1 - epsilon, S)\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z_i = np.dot(X, self.weights[0]) + self.bias[0]\n",
    "        A_i = self.relu_function(Z_i)\n",
    "        i = 0\n",
    "        if self.n_hidden_layers > 1:\n",
    "            for i in range(self.n_hidden_layers-1):\n",
    "                Z_i = np.dot(A_i, self.weights[i+1]) + self.bias[i+1]\n",
    "                A_i = self.relu_function(Z_i)\n",
    "\n",
    "            i = i + 1\n",
    "        Z_i = np.dot(A_i, self.weights[i+1]) + self.bias[i+1]\n",
    "        O = self.softmax(Z_i)\n",
    "\n",
    "        # Rows with highest probability\n",
    "        S_max = np.argmax(O, axis=1)\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "    def convert_to_embeddings(self, sentence):\n",
    "        '''Convert sentence to embeddings\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "            # try:\n",
    "        if emb: # if there is a word\n",
    "            emb_concat = np.concatenate(emb, axis=0)\n",
    "        else:\n",
    "            emb_concat = []\n",
    "        # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "        if len(emb) < self.max_seq_length:\n",
    "            # Missing words\n",
    "            words_missing = self.max_seq_length - len(emb)\n",
    "            # print(words_missing)\n",
    "            emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile, threshold=self.threshold, max_features=self.max_features)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        # Convert to OneHot for computing Loss\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim × features (u)\n",
    "        n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        if self.tfidf: # Truncate input to the max sequence length sorted by TF-IDF\n",
    "            tf_idf = features_ff_class.tf_idf(features_ff_class.tokenized_text)\n",
    "            trunc_tokenized_text = features_ff_class.sort_by_tfidf(\n",
    "                tf_idf,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "                features_ff_class.tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Weights\n",
    "        # Create W_a and b_a\n",
    "        # W_0[n_documents, hidden_units (u)]\n",
    "        # b_0[hidden_units (u)]\n",
    "\n",
    "        list_of_sizes = [n_inputs] + self.hidden_units + [num_labels]\n",
    "        for i in range(self.n_hidden_layers + 1):\n",
    "            weights, bias = self.initialize_weights(list_of_sizes[i], list_of_sizes[i+1])\n",
    "            self.weights[i] = weights\n",
    "            self.bias[i] = bias\n",
    "\n",
    "        # Initilze Momentum weights\n",
    "        prev_dW_i = [0] * (self.n_hidden_layers + 1)\n",
    "        prev_db_i = [0] * (self.n_hidden_layers + 1)\n",
    "\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        for n_epoch in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "                # List of outputs of each layer\n",
    "                # A[0] -> Input Layer\n",
    "                # A[.] => Hidden Layer\n",
    "                # A[n] -> Ouput Layer\n",
    "                A = [None]*(self.n_hidden_layers + 2) \n",
    "                Z = [None]*(self.n_hidden_layers + 2)\n",
    "                # ---------------- Input Layer --------------- #\n",
    "                A[0] = X_mini_batch\n",
    "                Z[0] = X_mini_batch\n",
    "\n",
    "                # ---------------- Hidden Layers --------------- #\n",
    "                for i in range(self.n_hidden_layers):\n",
    "                    # Z_i = np.dot(X_mini_batch, self.weights_i) + self.bias_i\n",
    "                    # A_i = relu(Z_i)\n",
    "                    Z_tmp = np.dot(A[i], self.weights[i]) + self.bias[i]\n",
    "                    Z[i+1] = Z_tmp\n",
    "                    A_tmp = self.relu_function(Z_tmp)\n",
    "                    A[i+1] = A_tmp\n",
    "                # ---------------- Hidden-to-Output Layer --------------- #\n",
    "\n",
    "                i = i + 1\n",
    "                # print(i)\n",
    "                Z_output_layer = np.dot(A[i], self.weights[self.n_hidden_layers]) + self.bias[self.n_hidden_layers]\n",
    "                Z[i+1] = Z_output_layer\n",
    "                A_output_layer = self.softmax(Z_output_layer)\n",
    "                A[i+1] = A_output_layer\n",
    "\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "\n",
    "                # Compute Gradients\n",
    "                # List of output gradients of each layer\n",
    "                # dZ[0] -> Input Layer\n",
    "                # dZ[.] => Hidden Layer\n",
    "                # dZ[n] -> Ouput Layer\n",
    "                dZ = [None] * (self.n_hidden_layers + 1)\n",
    "\n",
    "                # dW[previous_layer, next_layer]\n",
    "                dW = [None] * (self.n_hidden_layers + 1)\n",
    "                db = [None] * (self.n_hidden_layers + 1)\n",
    "                dZ[-1] = A[-1] - y_mini_batch\n",
    "                for i in range(self.n_hidden_layers, 0, -1):\n",
    "                    dW[i] = (1/minibatch_size)*np.dot(A[i].T, dZ[i])\n",
    "                    db[i] = (1/minibatch_size)*np.sum(dZ[i], axis=0, keepdims = True)\n",
    "                    dZ[i-1] = np.dot(dZ[i], self.weights[i].T)*self.relu_derivative(Z[i])\n",
    "\n",
    "                # print(dZ[i-1])\n",
    "                dW[0] = (1/minibatch_size)*np.dot(X_mini_batch.T, dZ[i-1])\n",
    "                db[0] = (1/minibatch_size)*np.sum(dZ[i-1], axis=0, keepdims = True)\n",
    "\n",
    "                # Update Weights\n",
    "                for i in range(self.n_hidden_layers + 1):\n",
    "                    self.weights[i] = self.weights[i] - (self.learning_rate*dW[i] + self.momentum*prev_dW_i[i])\n",
    "                    self.bias[i] = self.bias[i] - (self.learning_rate*db[i] + self.momentum*prev_db_i[i])\n",
    "                    # Momentum\n",
    "                    # Save previous gradients for Momentum\n",
    "                    prev_dW_i[i] =  self.learning_rate*dW[i] + self.momentum*prev_dW_i[i]\n",
    "                    prev_db_i[i] =  self.learning_rate*db[i] + self.momentum*prev_db_i[i]\n",
    "\n",
    "                ########\n",
    "                # Loss #\n",
    "                ########\n",
    "                # print(y_mini_batch)\n",
    "                mini_batch_loss.append(self.cross_entropy_loss(A[-1], y_mini_batch))\n",
    "\n",
    "            loss = np.mean(mini_batch_loss)\n",
    "            self.loss[n_epoch] = loss\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {n_epoch+1} - Loss: {loss}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        if self.tfidf:\n",
    "            tf_idf_inference = []\n",
    "            # Get features from inference file\n",
    "            for sentence in tokenized_text:\n",
    "                # Transform dataset to TF-IDF space\n",
    "                # Return features with format (1, size_vocabulary)\n",
    "                X_sentence = self.features_ff_class.get_features_tfidf(sentence, self.features_ff_class.idf)\n",
    "                tf_idf_inference.append(X_sentence)\n",
    "            tf_idf_inference = np.stack(tf_idf_inference)\n",
    "            trunc_tokenized_text = self.features_ff_class.sort_by_tfidf(\n",
    "                tf_idf_inference,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "        else:\n",
    "            # Truncate input to the max sequence length\n",
    "            trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "                tokenized_text,\n",
    "                self.max_seq_length\n",
    "            )\n",
    "\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Make Prediction\n",
    "        y_test = self.predict(X_test)\n",
    "        preds_label = []\n",
    "        for y in y_test:\n",
    "            tmp = self.Y_to_categorical[y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1593,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "train_file = \"datasets/products/train.txt\"\n",
    "emb_file = \"glove.6B.50d.txt\"\n",
    "pred_file = \"datasets/products/val.test\"\n",
    "pred_true_labels = \"datasets/products/val.txt\"\n",
    "model_file_name = \"products.model\"\n",
    "loss_file = \"datasets/products/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4) # 65.6%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=256, learning_rate=0.1, epochs=500, tfidf=True, max_features=500, threshold=2) # 64%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=[15, 3], minibatch_size=32, learning_rate=0.002, epochs=100) # 65%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4)\n",
    "\n",
    "# 2 Layers\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[20, 10], minibatch_size=256, learning_rate=0.001, epochs=100, tfidf=True, max_features=500, threshold=2, momentum=0.9) # 59.01%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=[10, 10], minibatch_size=256, learning_rate=0.2, epochs=500, tfidf=True, max_features=500, threshold=2, momentum=0.4) # 59.01%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[10, 10], minibatch_size=32, learning_rate=0.0001, epochs=100) # 58.98%\n",
    "\n",
    "\n",
    "# 3 Layers\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[2, 2, 2], minibatch_size=128, learning_rate=0.2, momentum=0, epochs=100)  # Accuracy: 62.73%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[2, 2, 2], minibatch_size=128, learning_rate=0.2, momentum=0.1, epochs=100)  # Accuracy: 63.23%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.02, momentum=0.1, epochs=200)  # Accuracy: 65.77%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.02, momentum=0.1, epochs=200)  # Accuracy: 66.40%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.02, momentum=0.1, epochs=200)  # Accuracy: 68.27%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[3, 3, 3], minibatch_size=32, learning_rate=0.015, momentum=0.1, epochs=200)\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=32, learning_rate=0.05, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=15, hidden_units=20, minibatch_size=128, learning_rate=0.05, epochs=100, tfidf=True, threshold=2, max_features=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=100, minibatch_size=256, learning_rate=0.01, epochs=200) # 34.62%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=100, minibatch_size=256, learning_rate=0.005, epochs=200) # 35.58%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.003, epochs=500, momentum=0.9) # 37.82%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9) # 41.99%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=2, max_features=1000) # Accuracy: 44.55%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=0, max_features=10000) # Accuracy: 46.47%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[80, 50], minibatch_size=128, learning_rate=0.0001, epochs=500, momentum=0.9) # 28.21%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=[5, 5, 5], minibatch_size=128, learning_rate=0.0001, epochs=500, momentum=0.9) \n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100) # Accuracy: 65.77%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=9, minibatch_size=64, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 66.14%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=9, minibatch_size=64, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 67.24%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=50, hidden_units=[5, 5], minibatch_size=128, learning_rate=0.01, epochs=300, momentum=0.9)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[5, 5, 5], minibatch_size=32, learning_rate=0.01, epochs=300, momentum=0.9) # Accuracy: 63.33%\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "train_file = \"datasets/odiya/train.txt\"\n",
    "emb_file = \"fasttext.wiki.300d.vec\"\n",
    "pred_file = \"datasets/odiya/val.test\"\n",
    "pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "model_file_name = \"odiya.model\"\n",
    "loss_file = \"datasets/odiya/loss.txt\"\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100) # Accuracy: 75.89%\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100, momentum=0.9) # Accuracy: 78.29%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=200, momentum=0.9) # Accuracy: 79.11%\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=30, hidden_units=[5, 5], minibatch_size=128, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 79.97%\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=[5, 5], minibatch_size=128, learning_rate=0.01, epochs=200, momentum=0.9) # Accuracy: 79.97%\n",
    "\n",
    "nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=5, hidden_units_other_layers=[5, 5], minibatch_size=32, learning_rate=0.001, epochs=200, momentum=0.9) # Accuracy: 81.32%\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.46432380620485036\n",
      "Epoch: 2 - Loss: 0.3601050147990227\n",
      "Epoch: 3 - Loss: 0.3520441453113394\n",
      "Epoch: 4 - Loss: 0.3481383380287288\n",
      "Epoch: 5 - Loss: 0.3452040425771486\n",
      "Epoch: 6 - Loss: 0.3423645871945432\n",
      "Epoch: 7 - Loss: 0.3394223154098657\n",
      "Epoch: 8 - Loss: 0.33628925993864084\n",
      "Epoch: 9 - Loss: 0.3329733599273262\n",
      "Epoch: 10 - Loss: 0.3294680359940737\n",
      "Epoch: 11 - Loss: 0.3256844073067642\n",
      "Epoch: 12 - Loss: 0.32161035602554194\n",
      "Epoch: 13 - Loss: 0.3173306884703431\n",
      "Epoch: 14 - Loss: 0.31285058435797725\n",
      "Epoch: 15 - Loss: 0.308173288758248\n",
      "Epoch: 16 - Loss: 0.30331985969927633\n",
      "Epoch: 17 - Loss: 0.2984199636790532\n",
      "Epoch: 18 - Loss: 0.29349749444967144\n",
      "Epoch: 19 - Loss: 0.2886392533705296\n",
      "Epoch: 20 - Loss: 0.2839791411036632\n",
      "Epoch: 21 - Loss: 0.27942314615763253\n",
      "Epoch: 22 - Loss: 0.27497709903496426\n",
      "Epoch: 23 - Loss: 0.2706210994666953\n",
      "Epoch: 24 - Loss: 0.2663419824084664\n",
      "Epoch: 25 - Loss: 0.2622856969843712\n",
      "Epoch: 26 - Loss: 0.2583566469714966\n",
      "Epoch: 27 - Loss: 0.2545992276216042\n",
      "Epoch: 28 - Loss: 0.2509932042064922\n",
      "Epoch: 29 - Loss: 0.24750889144146532\n",
      "Epoch: 30 - Loss: 0.24415883345903855\n",
      "Epoch: 31 - Loss: 0.24092043352402492\n",
      "Epoch: 32 - Loss: 0.23775661826349961\n",
      "Epoch: 33 - Loss: 0.2346444772791872\n",
      "Epoch: 34 - Loss: 0.23158790851169295\n",
      "Epoch: 35 - Loss: 0.22868317164491855\n",
      "Epoch: 36 - Loss: 0.22587348870767954\n",
      "Epoch: 37 - Loss: 0.22310316164464342\n",
      "Epoch: 38 - Loss: 0.22041294967569663\n",
      "Epoch: 39 - Loss: 0.21779816533857674\n",
      "Epoch: 40 - Loss: 0.21534418546165343\n",
      "Epoch: 41 - Loss: 0.2128772916119868\n",
      "Epoch: 42 - Loss: 0.2105188903598161\n",
      "Epoch: 43 - Loss: 0.2082501696701121\n",
      "Epoch: 44 - Loss: 0.20609429328614826\n",
      "Epoch: 45 - Loss: 0.20395447394688876\n",
      "Epoch: 46 - Loss: 0.20192919535803197\n",
      "Epoch: 47 - Loss: 0.19989400631236504\n",
      "Epoch: 48 - Loss: 0.19787862364096057\n",
      "Epoch: 49 - Loss: 0.19601527768863486\n",
      "Epoch: 50 - Loss: 0.19428225292119344\n",
      "Epoch: 51 - Loss: 0.19264724375879055\n",
      "Epoch: 52 - Loss: 0.19108229686578043\n",
      "Epoch: 53 - Loss: 0.18959130638695718\n",
      "Epoch: 54 - Loss: 0.1880925116553801\n",
      "Epoch: 55 - Loss: 0.18664434749267975\n",
      "Epoch: 56 - Loss: 0.18529862894736743\n",
      "Epoch: 57 - Loss: 0.18397848168102907\n",
      "Epoch: 58 - Loss: 0.1827435583592541\n",
      "Epoch: 59 - Loss: 0.18155791453385395\n",
      "Epoch: 60 - Loss: 0.1803720259371331\n",
      "Epoch: 61 - Loss: 0.17925117382108455\n",
      "Epoch: 62 - Loss: 0.17820324031812737\n",
      "Epoch: 63 - Loss: 0.17717555321409928\n",
      "Epoch: 64 - Loss: 0.17622270978085317\n",
      "Epoch: 65 - Loss: 0.17525863920385923\n",
      "Epoch: 66 - Loss: 0.17434573868282002\n",
      "Epoch: 67 - Loss: 0.17344295331257448\n",
      "Epoch: 68 - Loss: 0.17254751833274015\n",
      "Epoch: 69 - Loss: 0.17166235723215814\n",
      "Epoch: 70 - Loss: 0.17079813163507104\n",
      "Epoch: 71 - Loss: 0.1699796921319817\n",
      "Epoch: 72 - Loss: 0.16916986959513694\n",
      "Epoch: 73 - Loss: 0.1683675560266555\n",
      "Epoch: 74 - Loss: 0.16758059438193607\n",
      "Epoch: 75 - Loss: 0.1668323826198364\n",
      "Epoch: 76 - Loss: 0.16613180736223224\n",
      "Epoch: 77 - Loss: 0.16543050039803417\n",
      "Epoch: 78 - Loss: 0.16475199711869562\n",
      "Epoch: 79 - Loss: 0.164106884165399\n",
      "Epoch: 80 - Loss: 0.1634925416929369\n",
      "Epoch: 81 - Loss: 0.16286161485365497\n",
      "Epoch: 82 - Loss: 0.16223642246941458\n",
      "Epoch: 83 - Loss: 0.16160344178444871\n",
      "Epoch: 84 - Loss: 0.16102889557359623\n",
      "Epoch: 85 - Loss: 0.16043425985083032\n",
      "Epoch: 86 - Loss: 0.15982518709337315\n",
      "Epoch: 87 - Loss: 0.1592664830850439\n",
      "Epoch: 88 - Loss: 0.1587027108918895\n",
      "Epoch: 89 - Loss: 0.15814104885690583\n",
      "Epoch: 90 - Loss: 0.15762330778045525\n",
      "Epoch: 91 - Loss: 0.15707354350322034\n",
      "Epoch: 92 - Loss: 0.1565724953327443\n",
      "Epoch: 93 - Loss: 0.15603131641837537\n",
      "Epoch: 94 - Loss: 0.15553351544851393\n",
      "Epoch: 95 - Loss: 0.15504620838213995\n",
      "Epoch: 96 - Loss: 0.15454782467957673\n",
      "Epoch: 97 - Loss: 0.15407404115902162\n",
      "Epoch: 98 - Loss: 0.15359212087904503\n",
      "Epoch: 99 - Loss: 0.15310166549560392\n",
      "Epoch: 100 - Loss: 0.15266540644561005\n",
      "Epoch: 101 - Loss: 0.15220442330561973\n",
      "Epoch: 102 - Loss: 0.15178246812640145\n",
      "Epoch: 103 - Loss: 0.15133762488589753\n",
      "Epoch: 104 - Loss: 0.15091895778604345\n",
      "Epoch: 105 - Loss: 0.1504717700009272\n",
      "Epoch: 106 - Loss: 0.15006695003347137\n",
      "Epoch: 107 - Loss: 0.14963323841500867\n",
      "Epoch: 108 - Loss: 0.14923984059435555\n",
      "Epoch: 109 - Loss: 0.14883096524287767\n",
      "Epoch: 110 - Loss: 0.14844482225088942\n",
      "Epoch: 111 - Loss: 0.14803466472937843\n",
      "Epoch: 112 - Loss: 0.14763313059054595\n",
      "Epoch: 113 - Loss: 0.1472379517546686\n",
      "Epoch: 114 - Loss: 0.1468475277508394\n",
      "Epoch: 115 - Loss: 0.1464936987141208\n",
      "Epoch: 116 - Loss: 0.14610194214473474\n",
      "Epoch: 117 - Loss: 0.14574086129669764\n",
      "Epoch: 118 - Loss: 0.1453889918307188\n",
      "Epoch: 119 - Loss: 0.14503070885825045\n",
      "Epoch: 120 - Loss: 0.14466968187289095\n",
      "Epoch: 121 - Loss: 0.1443271071590369\n",
      "Epoch: 122 - Loss: 0.14398012531149526\n",
      "Epoch: 123 - Loss: 0.14364067806331085\n",
      "Epoch: 124 - Loss: 0.1433097839437751\n",
      "Epoch: 125 - Loss: 0.14296084877441753\n",
      "Epoch: 126 - Loss: 0.1426022823596191\n",
      "Epoch: 127 - Loss: 0.14225477020581956\n",
      "Epoch: 128 - Loss: 0.1419308516763125\n",
      "Epoch: 129 - Loss: 0.14161973171174147\n",
      "Epoch: 130 - Loss: 0.141298200980999\n",
      "Epoch: 131 - Loss: 0.14098714030628437\n",
      "Epoch: 132 - Loss: 0.14064192812098833\n",
      "Epoch: 133 - Loss: 0.14033444880997975\n",
      "Epoch: 134 - Loss: 0.1400306339520257\n",
      "Epoch: 135 - Loss: 0.13967771860712117\n",
      "Epoch: 136 - Loss: 0.13938304389944386\n",
      "Epoch: 137 - Loss: 0.1390719285408358\n",
      "Epoch: 138 - Loss: 0.13877103506484978\n",
      "Epoch: 139 - Loss: 0.13847497539138404\n",
      "Epoch: 140 - Loss: 0.13814357316677936\n",
      "Epoch: 141 - Loss: 0.13784085250087055\n",
      "Epoch: 142 - Loss: 0.13755232642690002\n",
      "Epoch: 143 - Loss: 0.13726283701994116\n",
      "Epoch: 144 - Loss: 0.13699119419918926\n",
      "Epoch: 145 - Loss: 0.13669534474216805\n",
      "Epoch: 146 - Loss: 0.13640501890268728\n",
      "Epoch: 147 - Loss: 0.1361640889876726\n",
      "Epoch: 148 - Loss: 0.13586853734424725\n",
      "Epoch: 149 - Loss: 0.13558134326021837\n",
      "Epoch: 150 - Loss: 0.13531323467742803\n",
      "Epoch: 151 - Loss: 0.13501389700101407\n",
      "Epoch: 152 - Loss: 0.1347477455708058\n",
      "Epoch: 153 - Loss: 0.13445805427258667\n",
      "Epoch: 154 - Loss: 0.13420787818759874\n",
      "Epoch: 155 - Loss: 0.13395183191871274\n",
      "Epoch: 156 - Loss: 0.13365927298718347\n",
      "Epoch: 157 - Loss: 0.13337364818555936\n",
      "Epoch: 158 - Loss: 0.13309793780194454\n",
      "Epoch: 159 - Loss: 0.1328385532090535\n",
      "Epoch: 160 - Loss: 0.1325709510982125\n",
      "Epoch: 161 - Loss: 0.13229596402243815\n",
      "Epoch: 162 - Loss: 0.1320292213898987\n",
      "Epoch: 163 - Loss: 0.131777347731143\n",
      "Epoch: 164 - Loss: 0.13151363680530515\n",
      "Epoch: 165 - Loss: 0.1312574087728332\n",
      "Epoch: 166 - Loss: 0.13101364922676448\n",
      "Epoch: 167 - Loss: 0.13076128194890052\n",
      "Epoch: 168 - Loss: 0.13050059062642205\n",
      "Epoch: 169 - Loss: 0.13024002359616801\n",
      "Epoch: 170 - Loss: 0.12998468844338604\n",
      "Epoch: 171 - Loss: 0.12972407082100457\n",
      "Epoch: 172 - Loss: 0.1294955590370831\n",
      "Epoch: 173 - Loss: 0.1292517859277733\n",
      "Epoch: 174 - Loss: 0.12902548883176954\n",
      "Epoch: 175 - Loss: 0.12874843523726479\n",
      "Epoch: 176 - Loss: 0.12852127321315113\n",
      "Epoch: 177 - Loss: 0.12826876584103294\n",
      "Epoch: 178 - Loss: 0.12800223999594978\n",
      "Epoch: 179 - Loss: 0.12776844898364134\n",
      "Epoch: 180 - Loss: 0.1275145971817666\n",
      "Epoch: 181 - Loss: 0.12728195305124243\n",
      "Epoch: 182 - Loss: 0.127047169143668\n",
      "Epoch: 183 - Loss: 0.12682592654213268\n",
      "Epoch: 184 - Loss: 0.1265769981287169\n",
      "Epoch: 185 - Loss: 0.12635588605183198\n",
      "Epoch: 186 - Loss: 0.12610945520860184\n",
      "Epoch: 187 - Loss: 0.12585856160684503\n",
      "Epoch: 188 - Loss: 0.12563346029091899\n",
      "Epoch: 189 - Loss: 0.12537780284918892\n",
      "Epoch: 190 - Loss: 0.1251600714164441\n",
      "Epoch: 191 - Loss: 0.12494024943918312\n",
      "Epoch: 192 - Loss: 0.12471797071080298\n",
      "Epoch: 193 - Loss: 0.12448333852638485\n",
      "Epoch: 194 - Loss: 0.12424334211354854\n",
      "Epoch: 195 - Loss: 0.12405841382037035\n",
      "Epoch: 196 - Loss: 0.12379579703903705\n",
      "Epoch: 197 - Loss: 0.12359499021757803\n",
      "Epoch: 198 - Loss: 0.12336937626907286\n",
      "Epoch: 199 - Loss: 0.12314785775128924\n",
      "Epoch: 200 - Loss: 0.12291653881514349\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1595,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1596,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1597,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = model.classify(pred_file + \".txt\")\n",
    "# preds, t1, t2 = model.classify(pred_file + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1598,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1599,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1600,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "\n",
    "# products\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1601,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           1345\n",
       "business          937\n",
       "entertainment     758\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 1602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           1373\n",
       "business          894\n",
       "entertainment     773\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 1603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.32%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
