{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass\n",
    "\n",
    "\n",
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkTorch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, n_labels):\n",
    "        super(NeuralNetworkTorch, self).__init__()\n",
    "        self.weights_1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.weights_2 = nn.Linear(hidden_units, n_labels)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Apply softmax along the output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Input-to-Hidden Layer --------------- #\n",
    "        x = self.weights_1(x)\n",
    "        x = self.relu(x)\n",
    "        # ---------------- Hidden-to-Output Layer --------------- #\n",
    "        x = self.weights_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "\n",
    "class NeuralModel_Torch(Model):\n",
    "    def __init__(self, embeddingfile, max_seq_length, hidden_units, minibatch_size, learning_rate, epochs, adam=False): \n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_units = hidden_units\n",
    "        # Layers\n",
    "        self.model_torch = None\n",
    "        self.Y_to_categorical = None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.adam = adam\n",
    "        self.loss = {}\n",
    "        \n",
    "    def convert_to_embeddings(self, sentence):\n",
    "        '''Convert sentence to embeddings\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "            # try:\n",
    "        if emb: # if there is a word\n",
    "            emb_concat = np.concatenate(emb, axis=0)\n",
    "        else:\n",
    "            emb_concat = []\n",
    "        # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "        if len(emb) < self.max_seq_length:\n",
    "            # Missing words\n",
    "            words_missing = self.max_seq_length - len(emb)\n",
    "            # print(words_missing)\n",
    "            emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        Y = np.array(Y)\n",
    "        # Convert to OneHot for computing Loss\n",
    "        # Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim × features (u)\n",
    "        n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "            features_ff_class.tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Torch Model\n",
    "        self.model_torch = NeuralNetworkTorch(n_inputs, self.hidden_units, num_labels)\n",
    "        # Optimzer\n",
    "        if self.adam:  \n",
    "            optimizer = optim.Adam(self.model_torch.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            optimizer = optim.SGD(self.model_torch.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        #################\n",
    "        # Torch Tensors #\n",
    "        #################\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation = Y[permutation]\n",
    "\n",
    "\n",
    "        # Torch Tensors\n",
    "        X_permutation = torch.tensor(X_permutation, dtype=torch.float32)\n",
    "        Y_permutation = torch.tensor(Y_permutation)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model_torch.forward(X_mini_batch)\n",
    "\n",
    "                loss = self.criterion(outputs, y_mini_batch)\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "   \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                mini_batch_loss.append(loss.item())\n",
    "\n",
    "            \n",
    "            self.loss[i] = np.mean(mini_batch_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {self.loss[i]}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "            tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Convert to tensor\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "        # Make Prediction\n",
    "        preds_label = []\n",
    "        with torch.no_grad():\n",
    "            predicted = self.model_torch(X_test)\n",
    "            _, y_test = torch.max(predicted, 1)\n",
    "            for y in y_test:\n",
    "                tmp = self.Y_to_categorical[y.item()] # Convert to original class\n",
    "                preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "train_file = \"datasets/products/train.txt\"\n",
    "emb_file = \"glove.6B.50d.txt\"\n",
    "pred_file = \"datasets/products/val.test\"\n",
    "pred_true_labels = \"datasets/products/val.txt\"\n",
    "model_file_name = \"torch.products.model\"\n",
    "loss_file = \"datasets/products/loss.txt\"\n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"torch.4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100) # 39\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=5, minibatch_size=32, learning_rate=0.001, epochs=300, adam=True) # 43%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, minibatch_size=32, learning_rate=0.01, epochs=300, adam=True) # 45%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, minibatch_size=32, learning_rate=0.015, epochs=300, adam=True) # 45.51%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, minibatch_size=32, learning_rate=0.005, epochs=500, adam=True) # 45.51%\n",
    "\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=0, max_features=10000) # Accuracy: 46.47%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, adam=False) # Accuracy: 33.33%\n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"torch.questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "# train_file = \"datasets/odiya/train.txt\"\n",
    "# emb_file = \"fasttext.wiki.300d.vec\"\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "# model_file_name = \"torch.odiya.model\"\n",
    "# loss_file = \"datasets/odiya/loss.txt\"\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.6579930233809114\n",
      "Epoch: 2 - Loss: 0.635890508100299\n",
      "Epoch: 3 - Loss: 0.6301713169718082\n",
      "Epoch: 4 - Loss: 0.6268997912392295\n",
      "Epoch: 5 - Loss: 0.6247328416335802\n",
      "Epoch: 6 - Loss: 0.622332417928368\n",
      "Epoch: 7 - Loss: 0.6207420110336842\n",
      "Epoch: 8 - Loss: 0.6188695454158666\n",
      "Epoch: 9 - Loss: 0.6169649226533854\n",
      "Epoch: 10 - Loss: 0.6153154452519914\n",
      "Epoch: 11 - Loss: 0.613259307592193\n",
      "Epoch: 12 - Loss: 0.6115361777551335\n",
      "Epoch: 13 - Loss: 0.6096675588309399\n",
      "Epoch: 14 - Loss: 0.6079360321986894\n",
      "Epoch: 15 - Loss: 0.6062596385830019\n",
      "Epoch: 16 - Loss: 0.6043611472369703\n",
      "Epoch: 17 - Loss: 0.602632923469953\n",
      "Epoch: 18 - Loss: 0.6009644783713335\n",
      "Epoch: 19 - Loss: 0.5994811976248501\n",
      "Epoch: 20 - Loss: 0.5979908450615187\n",
      "Epoch: 21 - Loss: 0.5966084120829412\n",
      "Epoch: 22 - Loss: 0.5953003016717595\n",
      "Epoch: 23 - Loss: 0.5940346993185991\n",
      "Epoch: 24 - Loss: 0.5926057237788943\n",
      "Epoch: 25 - Loss: 0.5917622514297626\n",
      "Epoch: 26 - Loss: 0.5908374864996576\n",
      "Epoch: 27 - Loss: 0.5900613744200373\n",
      "Epoch: 28 - Loss: 0.5896899424081931\n",
      "Epoch: 29 - Loss: 0.5889571746434171\n",
      "Epoch: 30 - Loss: 0.5885196169095537\n",
      "Epoch: 31 - Loss: 0.5872407772058358\n",
      "Epoch: 32 - Loss: 0.5866763857610386\n",
      "Epoch: 33 - Loss: 0.5853238692312884\n",
      "Epoch: 34 - Loss: 0.5845928986745378\n",
      "Epoch: 35 - Loss: 0.5832056678145942\n",
      "Epoch: 36 - Loss: 0.5828716528196276\n",
      "Epoch: 37 - Loss: 0.5819982035028423\n",
      "Epoch: 38 - Loss: 0.5810392950210104\n",
      "Epoch: 39 - Loss: 0.5795773969106147\n",
      "Epoch: 40 - Loss: 0.5789690218820163\n",
      "Epoch: 41 - Loss: 0.5775268362709349\n",
      "Epoch: 42 - Loss: 0.5768277668879808\n",
      "Epoch: 43 - Loss: 0.5765268416126813\n",
      "Epoch: 44 - Loss: 0.5753450776901713\n",
      "Epoch: 45 - Loss: 0.5741938537249536\n",
      "Epoch: 46 - Loss: 0.5740969871450787\n",
      "Epoch: 47 - Loss: 0.5729793638539461\n",
      "Epoch: 48 - Loss: 0.5732153880815565\n",
      "Epoch: 49 - Loss: 0.573624796955132\n",
      "Epoch: 50 - Loss: 0.5713458607533227\n",
      "Epoch: 51 - Loss: 0.5703252938627466\n",
      "Epoch: 52 - Loss: 0.5701680310664733\n",
      "Epoch: 53 - Loss: 0.5715389290104614\n",
      "Epoch: 54 - Loss: 0.5725073228584476\n",
      "Epoch: 55 - Loss: 0.5694692699090104\n",
      "Epoch: 56 - Loss: 0.569762372678043\n",
      "Epoch: 57 - Loss: 0.5696655203228348\n",
      "Epoch: 58 - Loss: 0.5695194636020192\n",
      "Epoch: 59 - Loss: 0.5699709539398825\n",
      "Epoch: 60 - Loss: 0.5684418374775377\n",
      "Epoch: 61 - Loss: 0.5697321327551742\n",
      "Epoch: 62 - Loss: 0.5680953438662313\n",
      "Epoch: 63 - Loss: 0.5689518684624163\n",
      "Epoch: 64 - Loss: 0.5680413413267195\n",
      "Epoch: 65 - Loss: 0.5705783097656226\n",
      "Epoch: 66 - Loss: 0.5685441913414586\n",
      "Epoch: 67 - Loss: 0.5686281502978202\n",
      "Epoch: 68 - Loss: 0.569177134088212\n",
      "Epoch: 69 - Loss: 0.5686966485772396\n",
      "Epoch: 70 - Loss: 0.5675594619446737\n",
      "Epoch: 71 - Loss: 0.5665066431270787\n",
      "Epoch: 72 - Loss: 0.5665640261641309\n",
      "Epoch: 73 - Loss: 0.5670630320083876\n",
      "Epoch: 74 - Loss: 0.5675304900649135\n",
      "Epoch: 75 - Loss: 0.5671489709359736\n",
      "Epoch: 76 - Loss: 0.5672589468809724\n",
      "Epoch: 77 - Loss: 0.5662001527891568\n",
      "Epoch: 78 - Loss: 0.5657505600364662\n",
      "Epoch: 79 - Loss: 0.5670351301599865\n",
      "Epoch: 80 - Loss: 0.5666464277937369\n",
      "Epoch: 81 - Loss: 0.5647297035697048\n",
      "Epoch: 82 - Loss: 0.5638028319627961\n",
      "Epoch: 83 - Loss: 0.5640379346221502\n",
      "Epoch: 84 - Loss: 0.563641954601908\n",
      "Epoch: 85 - Loss: 0.564631009833213\n",
      "Epoch: 86 - Loss: 0.5637968044339513\n",
      "Epoch: 87 - Loss: 0.5632776162375702\n",
      "Epoch: 88 - Loss: 0.5637952445474871\n",
      "Epoch: 89 - Loss: 0.5624415292330315\n",
      "Epoch: 90 - Loss: 0.5629592436222942\n",
      "Epoch: 91 - Loss: 0.5624287265941409\n",
      "Epoch: 92 - Loss: 0.5617024693752359\n",
      "Epoch: 93 - Loss: 0.561885718809315\n",
      "Epoch: 94 - Loss: 0.5612160893305679\n",
      "Epoch: 95 - Loss: 0.5616630462781051\n",
      "Epoch: 96 - Loss: 0.5623201889494446\n",
      "Epoch: 97 - Loss: 0.5614453138383619\n",
      "Epoch: 98 - Loss: 0.5624009373729214\n",
      "Epoch: 99 - Loss: 0.5614563330916539\n",
      "Epoch: 100 - Loss: 0.5613381264034224\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel_Torch.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9485e-01, 1.6644e-12, 5.1513e-03, 1.0587e-15],\n",
      "        [1.3973e-08, 0.0000e+00, 1.0000e+00, 1.3683e-09],\n",
      "        [5.1195e-09, 8.0907e-10, 2.4653e-15, 1.0000e+00],\n",
      "        ...,\n",
      "        [1.0000e+00, 1.3591e-32, 1.0462e-08, 6.7035e-21],\n",
      "        [6.0829e-04, 9.1669e-03, 4.3853e-02, 9.4637e-01],\n",
      "        [4.9278e-15, 0.0000e+00, 1.0000e+00, 2.5279e-20]])\n"
     ]
    }
   ],
   "source": [
    "preds= model.classify(pred_file + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, i = torch.max(prob, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos.dec    94\n",
       "neg.tru    83\n",
       "pos.tru    70\n",
       "neg.dec    65\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset.true_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred   \n",
       "pos.tru    89\n",
       "neg.dec    88\n",
       "neg.tru    80\n",
       "pos.dec    55\n",
       "dtype: int64"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.83%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
