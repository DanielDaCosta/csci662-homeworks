{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass\n",
    "\n",
    "\n",
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkTorch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, n_labels):\n",
    "        super(NeuralNetworkTorch, self).__init__()\n",
    "        self.weights_1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.weights_2 = nn.Linear(hidden_units, n_labels)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Apply softmax along the output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Input-to-Hidden Layer --------------- #\n",
    "        x = self.weights_1(x)\n",
    "        x = self.relu(x)\n",
    "        # ---------------- Hidden-to-Output Layer --------------- #\n",
    "        x = self.weights_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "\n",
    "class NeuralModel_Torch(Model):\n",
    "    def __init__(self, embeddingfile, max_seq_length, hidden_units, minibatch_size, learning_rate, epochs): \n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_units = hidden_units\n",
    "        # Layers\n",
    "        self.model_torch = None\n",
    "        self.Y_to_categorical = None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = {}\n",
    "        \n",
    "    def convert_to_embeddings(self, sentence):\n",
    "        '''Convert sentence to embeddings\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "            # try:\n",
    "        if emb: # if there is a word\n",
    "            emb_concat = np.concatenate(emb, axis=0)\n",
    "        else:\n",
    "            emb_concat = []\n",
    "        # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "        if len(emb) < self.max_seq_length:\n",
    "            # Missing words\n",
    "            words_missing = self.max_seq_length - len(emb)\n",
    "            # print(words_missing)\n",
    "            emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        Y = np.array(Y)\n",
    "        # Convert to OneHot for computing Loss\n",
    "        # Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim × features (u)\n",
    "        n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "            features_ff_class.tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Torch Model\n",
    "        self.model_torch = NeuralNetworkTorch(n_inputs, self.hidden_units, num_labels)\n",
    "        # Optimzer\n",
    "        optimizer = optim.SGD(self.model_torch.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "        #################\n",
    "        # Torch Tensors #\n",
    "        #################\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation = Y[permutation]\n",
    "\n",
    "\n",
    "        # Torch Tensors\n",
    "        X_permutation = torch.tensor(X_permutation, dtype=torch.float32)\n",
    "        Y_permutation = torch.tensor(Y_permutation)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "                outputs = self.model_torch(X_mini_batch)\n",
    "\n",
    "                loss = self.criterion(outputs, y_mini_batch)\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                mini_batch_loss.append(loss.item())\n",
    "\n",
    "            \n",
    "            self.loss[i] = np.mean(mini_batch_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {self.loss[i]}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "            tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Convert to tensor\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "        # Make Prediction\n",
    "        preds_label = []\n",
    "        with torch.no_grad():\n",
    "            predicted = self.model_torch(X_test)\n",
    "            _, y_test = torch.max(predicted, 1)\n",
    "            for y in y_test:\n",
    "                tmp = self.Y_to_categorical[y.item()] # Convert to original class\n",
    "                preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "train_file = \"datasets/products/train.txt\"\n",
    "emb_file = \"glove.6B.50d.txt\"\n",
    "pred_file = \"datasets/products/val.test\"\n",
    "pred_true_labels = \"datasets/products/val.txt\"\n",
    "model_file_name = \"torch.products.model\"\n",
    "loss_file = \"datasets/products/loss.txt\"\n",
    "\n",
    "nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"torch.4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"torch.questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "# train_file = \"datasets/odiya/train.txt\"\n",
    "# emb_file = \"fasttext.wiki.300d.vec\"\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "# model_file_name = \"torch.odiya.model\"\n",
    "# loss_file = \"datasets/odiya/loss.txt\"\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.6493482973677981\n",
      "Epoch: 2 - Loss: 0.6323923690918765\n",
      "Epoch: 3 - Loss: 0.626327877871098\n",
      "Epoch: 4 - Loss: 0.6226238647121594\n",
      "Epoch: 5 - Loss: 0.6191641749048525\n",
      "Epoch: 6 - Loss: 0.6162016637851856\n",
      "Epoch: 7 - Loss: 0.6136381490830264\n",
      "Epoch: 8 - Loss: 0.6112433095285498\n",
      "Epoch: 9 - Loss: 0.6092542092858648\n",
      "Epoch: 10 - Loss: 0.6071236630158922\n",
      "Epoch: 11 - Loss: 0.6047104695457622\n",
      "Epoch: 12 - Loss: 0.6018602465193695\n",
      "Epoch: 13 - Loss: 0.599180631988619\n",
      "Epoch: 14 - Loss: 0.5976826925950548\n",
      "Epoch: 15 - Loss: 0.5960224956091196\n",
      "Epoch: 16 - Loss: 0.5937833718361298\n",
      "Epoch: 17 - Loss: 0.5920294069439356\n",
      "Epoch: 18 - Loss: 0.5904260096008792\n",
      "Epoch: 19 - Loss: 0.5886665507328291\n",
      "Epoch: 20 - Loss: 0.5871475106733708\n",
      "Epoch: 21 - Loss: 0.5866939762618644\n",
      "Epoch: 22 - Loss: 0.5847412821339684\n",
      "Epoch: 23 - Loss: 0.5828933919134316\n",
      "Epoch: 24 - Loss: 0.5824078328404689\n",
      "Epoch: 25 - Loss: 0.5808292337356169\n",
      "Epoch: 26 - Loss: 0.5800598656107312\n",
      "Epoch: 27 - Loss: 0.5784858658635543\n",
      "Epoch: 28 - Loss: 0.5789726588257983\n",
      "Epoch: 29 - Loss: 0.5784376913053126\n",
      "Epoch: 30 - Loss: 0.5766052632975432\n",
      "Epoch: 31 - Loss: 0.576055695162229\n",
      "Epoch: 32 - Loss: 0.5756383547022299\n",
      "Epoch: 33 - Loss: 0.5750145323803089\n",
      "Epoch: 34 - Loss: 0.5739928713605448\n",
      "Epoch: 35 - Loss: 0.5736079039749192\n",
      "Epoch: 36 - Loss: 0.5726754134418043\n",
      "Epoch: 37 - Loss: 0.5716946895502828\n",
      "Epoch: 38 - Loss: 0.5718514518869435\n",
      "Epoch: 39 - Loss: 0.5703817926301546\n",
      "Epoch: 40 - Loss: 0.5709874987602234\n",
      "Epoch: 41 - Loss: 0.5721986697129676\n",
      "Epoch: 42 - Loss: 0.5720804346485372\n",
      "Epoch: 43 - Loss: 0.5711819918609105\n",
      "Epoch: 44 - Loss: 0.5701503123608104\n",
      "Epoch: 45 - Loss: 0.5701107575483849\n",
      "Epoch: 46 - Loss: 0.5681354189577278\n",
      "Epoch: 47 - Loss: 0.568603267077288\n",
      "Epoch: 48 - Loss: 0.5670458022070809\n",
      "Epoch: 49 - Loss: 0.5672968543380316\n",
      "Epoch: 50 - Loss: 0.5664723334136916\n",
      "Epoch: 51 - Loss: 0.5660387525894891\n",
      "Epoch: 52 - Loss: 0.565251968682178\n",
      "Epoch: 53 - Loss: 0.5655451373819924\n",
      "Epoch: 54 - Loss: 0.5639206726126875\n",
      "Epoch: 55 - Loss: 0.565268733091881\n",
      "Epoch: 56 - Loss: 0.5643386417005691\n",
      "Epoch: 57 - Loss: 0.564034861509054\n",
      "Epoch: 58 - Loss: 0.5638204216225747\n",
      "Epoch: 59 - Loss: 0.5632883226578952\n",
      "Epoch: 60 - Loss: 0.563748415920632\n",
      "Epoch: 61 - Loss: 0.5632165415886721\n",
      "Epoch: 62 - Loss: 0.5634074064485866\n",
      "Epoch: 63 - Loss: 0.563418927660749\n",
      "Epoch: 64 - Loss: 0.5621373479717349\n",
      "Epoch: 65 - Loss: 0.5626770080598585\n",
      "Epoch: 66 - Loss: 0.562700998709977\n",
      "Epoch: 67 - Loss: 0.5622674375223967\n",
      "Epoch: 68 - Loss: 0.5644088338123509\n",
      "Epoch: 69 - Loss: 0.5620598455879586\n",
      "Epoch: 70 - Loss: 0.5613883881115475\n",
      "Epoch: 71 - Loss: 0.5624513868539611\n",
      "Epoch: 72 - Loss: 0.5618848658778185\n",
      "Epoch: 73 - Loss: 0.5635647376256486\n",
      "Epoch: 74 - Loss: 0.5618490187302689\n",
      "Epoch: 75 - Loss: 0.5613924076220741\n",
      "Epoch: 76 - Loss: 0.5627838869036341\n",
      "Epoch: 77 - Loss: 0.5618162802391988\n",
      "Epoch: 78 - Loss: 0.5610954611213661\n",
      "Epoch: 79 - Loss: 0.5597060798136003\n",
      "Epoch: 80 - Loss: 0.5585347067359036\n",
      "Epoch: 81 - Loss: 0.5614048352636443\n",
      "Epoch: 82 - Loss: 0.5603585593905186\n",
      "Epoch: 83 - Loss: 0.559491461296023\n",
      "Epoch: 84 - Loss: 0.5596701367135428\n",
      "Epoch: 85 - Loss: 0.5578158743908069\n",
      "Epoch: 86 - Loss: 0.5581349602871877\n",
      "Epoch: 87 - Loss: 0.5591283823083515\n",
      "Epoch: 88 - Loss: 0.5588758010074405\n",
      "Epoch: 89 - Loss: 0.557321960282472\n",
      "Epoch: 90 - Loss: 0.55703918685211\n",
      "Epoch: 91 - Loss: 0.5562662713001111\n",
      "Epoch: 92 - Loss: 0.5570114895975663\n",
      "Epoch: 93 - Loss: 0.5572591016994664\n",
      "Epoch: 94 - Loss: 0.5570472815651104\n",
      "Epoch: 95 - Loss: 0.5555747721458505\n",
      "Epoch: 96 - Loss: 0.557350079011332\n",
      "Epoch: 97 - Loss: 0.5552177774028544\n",
      "Epoch: 98 - Loss: 0.5543274788403072\n",
      "Epoch: 99 - Loss: 0.5569174944257443\n",
      "Epoch: 100 - Loss: 0.5550796199429986\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel_Torch.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.classify(pred_file + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products\n",
    "pred_file = \"datasets/products/val.test\"\n",
    "pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.34%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
