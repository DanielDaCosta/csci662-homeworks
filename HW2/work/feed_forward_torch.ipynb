{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass\n",
    "\n",
    "\n",
    "class Features_FeedForward(Features):\n",
    "\n",
    "    def __init__(self, input_file, embedding_file):\n",
    "        super(Features_FeedForward, self).__init__(input_file)\n",
    "        self.embedding_matrix = self.read_embedding_file(embedding_file) # Need to save EmbeddingMatrix values for inference\n",
    "\n",
    "    def adjust_max_seq_length(self, tokenized_text, max_seq_length):\n",
    "        \"\"\"Adjust size of data input to the max sequence length\n",
    "        :param tokenized_text: data input\n",
    "        :param max_seq_length: the max sequence length\n",
    "        :return list: truncated sentences\n",
    "        \"\"\"\n",
    "        new_tokenized_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            new_tokenized_text.append(sentence[:max_seq_length])\n",
    "        return new_tokenized_text\n",
    "\n",
    "        \n",
    "    def read_embedding_file(self, embedding_file):\n",
    "        '''Read embedding file\n",
    "\n",
    "        :param embedding_file (str):\n",
    "        :return: dict: embedding matrix\n",
    "        '''\n",
    "\n",
    "        embedding_matrix = dict()\n",
    "        try: \n",
    "            with open(embedding_file, \"r\") as file:\n",
    "                for line in file:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    word_embedding = np.array([float(emb) for emb in values[1:]])\n",
    "                    embedding_matrix[word] = word_embedding\n",
    "            return embedding_matrix\n",
    "        except OSError as e:\n",
    "            print(\"Embedding file \" + embedding_file + \" is not available, please input the right parth to the file.\")\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def get_features(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to word embeeding values.\n",
    "        :param tokenized_sentence\n",
    "        :return feature weights\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "            except: # read UNK token embedding \n",
    "                word_emb = self.embedding_matrix[\"UNK\"]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        \n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkTorch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, n_labels):\n",
    "        super(NeuralNetworkTorch, self).__init__()\n",
    "        self.n_hidden_layers = len(hidden_units)\n",
    "        self.activation = nn.ReLU()\n",
    "        # List of domensions\n",
    "        self.list_of_sizes = [input_dim] + hidden_units + [n_labels]\n",
    "\n",
    "        self.weights = nn.ModuleList([nn.Linear(self.list_of_sizes[i], self.list_of_sizes[i+1]) for i in range(self.n_hidden_layers)])\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_units[-1], n_labels)\n",
    "        # Last actvatio is a Softmax\n",
    "        self.softmax = nn.Softmax(dim=1) # Apply softmax along the output dimension\n",
    "        \n",
    "        # self.weights_1 = nn.Linear(input_dim, hidden_units)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.weights_2 = nn.Linear(hidden_units, n_labels)\n",
    "        # self.softmax = nn.Softmax(dim=1)  # Apply softmax along the output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Input-to-Hidden Layer --------------- #\n",
    "\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            x = self.weights[i](x)\n",
    "            x = self.activation(x)\n",
    "        # ---------------- Hidden-to-Output Layer --------------- #\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "\n",
    "class NeuralModel_Torch(Model):\n",
    "    def __init__(self, embeddingfile, max_seq_length, hidden_units, minibatch_size, learning_rate, epochs, hidden_units_other_layers=[],adam=False, average_emb_sentence=False): \n",
    "        self.embeddingfile = embeddingfile\n",
    "        self.embedding_dim = None\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_units = [hidden_units] +  hidden_units_other_layers if len(hidden_units_other_layers) > 0 else [hidden_units]\n",
    "        self.average_emb_sentence = average_emb_sentence\n",
    "        # Layers\n",
    "        self.model_torch = None\n",
    "        self.Y_to_categorical = None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.features_ff_class = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.adam = adam\n",
    "        self.loss = {}\n",
    "        \n",
    "    def convert_to_embeddings(self, sentence, average_emb_sentence=False):\n",
    "        '''Convert sentence to embeddings.\n",
    "\n",
    "        :param average_sentence:  Compute the element-wise average of a list of sentence embedding\n",
    "        '''\n",
    "        emb = self.features_ff_class.get_features(sentence)\n",
    "        if not average_emb_sentence:\n",
    "            if emb: # if there is a word\n",
    "                emb_concat = np.concatenate(emb, axis=0)\n",
    "            else:\n",
    "                emb_concat = []\n",
    "            # If you need padding words (i.e., your input is too short), use a vector of zeroes\n",
    "            if len(emb) < self.max_seq_length:\n",
    "                # Missing words\n",
    "                words_missing = self.max_seq_length - len(emb)\n",
    "                # print(words_missing)\n",
    "                emb_concat = np.pad(emb_concat, (0, words_missing*self.embedding_dim), 'constant')\n",
    "        else:\n",
    "            # Compute average of the sentence\n",
    "            if len(emb) > 0:\n",
    "                stacked_arrays = np.vstack(emb)\n",
    "                emb_concat = np.mean(stacked_arrays, axis=0)\n",
    "            else:\n",
    "                emb_concat = np.zeros(self.embedding_dim)\n",
    "\n",
    "        return emb_concat\n",
    "\n",
    "    \n",
    "    def train(self, input_file, verbose=False):\n",
    "\n",
    "        # Read dataset and create vocabulary\n",
    "        features_ff_class = Features_FeedForward(input_file, self.embeddingfile)\n",
    "        self.features_ff_class = features_ff_class\n",
    "        num_labels = len(features_ff_class.labelset)\n",
    "\n",
    "        # Convert Y from categorical to integers values\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_ff_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_ff_class.labels]\n",
    "        Y = np.array(Y)\n",
    "        # Convert to OneHot for computing Loss\n",
    "        # Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        # Get embedding dim\n",
    "        self.embedding_dim = list(features_ff_class.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Number of sentences\n",
    "        sample_size = len(features_ff_class.tokenized_text)\n",
    "\n",
    "        # X_train: shape: 50f or 300f-dim × features (u)\n",
    "        if not self.average_emb_sentence:\n",
    "            n_inputs = self.max_seq_length*self.embedding_dim # number of features\n",
    "        else:\n",
    "            n_inputs = self.embedding_dim\n",
    "        X_train = np.zeros((sample_size, n_inputs))\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = features_ff_class.adjust_max_seq_length(\n",
    "            features_ff_class.tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "\n",
    "        # Convert to embeddings with zero-padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence, average_emb_sentence=self.average_emb_sentence)\n",
    "            X_train[i] = sentence_emb\n",
    "\n",
    "        minibatch_size = self.minibatch_size\n",
    "\n",
    "        # Initialize Torch Model\n",
    "        self.model_torch = NeuralNetworkTorch(n_inputs, self.hidden_units, num_labels)\n",
    "        # Optimzer\n",
    "        if self.adam:  \n",
    "            optimizer = optim.Adam(self.model_torch.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            optimizer = optim.SGD(self.model_torch.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        #################\n",
    "        # Torch Tensors #\n",
    "        #################\n",
    "        # Permutate the dataset to increase randomness\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        # X_train[n_documents, n_features]\n",
    "        X_permutation = X_train[permutation]\n",
    "        Y_permutation = Y[permutation]\n",
    "\n",
    "\n",
    "        # Torch Tensors\n",
    "        X_permutation = torch.tensor(X_permutation, dtype=torch.float32)\n",
    "        Y_permutation = torch.tensor(Y_permutation)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Mini-batch_size Implementation\n",
    "            mini_batch_loss = []\n",
    "            for j in range(0, sample_size, minibatch_size):\n",
    "                X_mini_batch = X_permutation[j:j+minibatch_size]\n",
    "                y_mini_batch = Y_permutation[j:j+minibatch_size]\n",
    "\n",
    "                ##########################################################\n",
    "                # ---------------------FORWARD PASS--------------------- #\n",
    "                ##########################################################\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model_torch.forward(X_mini_batch)\n",
    "\n",
    "                loss = self.criterion(outputs, y_mini_batch)\n",
    "\n",
    "                ##########################################################\n",
    "                # -------------------BACKWARD PASS---------------------- #\n",
    "                ##########################################################\n",
    "   \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                mini_batch_loss.append(loss.item())\n",
    "\n",
    "            \n",
    "            self.loss[i] = np.mean(mini_batch_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {self.loss[i]}\")\n",
    "\n",
    "    def classify(self, input_file):\n",
    "        # Read Input File\n",
    "        tokenized_text = self.features_ff_class.read_inference_file(input_file)\n",
    "\n",
    "        # Truncate input to the max sequence length\n",
    "        trunc_tokenized_text = self.features_ff_class.adjust_max_seq_length(\n",
    "            tokenized_text,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "        X_test = []\n",
    "        # Convert to embeddings with zero padding\n",
    "        for i, sentence in enumerate(trunc_tokenized_text):\n",
    "            sentence_emb = self.convert_to_embeddings(sentence, average_emb_sentence=self.average_emb_sentence)\n",
    "            X_test.append(sentence_emb)\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Convert to tensor\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "        # Make Prediction\n",
    "        preds_label = []\n",
    "        with torch.no_grad():\n",
    "            predicted = self.model_torch(X_test)\n",
    "            _, y_test = torch.max(predicted, 1)\n",
    "            for y in y_test:\n",
    "                tmp = self.Y_to_categorical[y.item()] # Convert to original class\n",
    "                preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Products #\n",
    "############\n",
    "\n",
    "# train_file = \"datasets/products/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "# model_file_name = \"torch.products.model\"\n",
    "# loss_file = \"datasets/products/loss.txt\"\n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# 1 Layers\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=15, hidden_units=10, minibatch_size=256, learning_rate=0.2, epochs=500)#, tfidf=True, max_features=1000, threshold=2) # 66.33%\n",
    "##BEST MODEL###\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=60, hidden_units=20, minibatch_size=256, learning_rate=0.2, epochs=300, average_emb_sentence=True)\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, hidden_units_other_layers=[5, 5], minibatch_size=32, learning_rate=0.001, epochs=200, average_emb_sentence=True) \n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, hidden_units_other_layers=[10], minibatch_size=32, learning_rate=0.02, epochs=200, average_emb_sentence=True) \n",
    "\n",
    "# 2 Layers\n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=4, hidden_units_other_layers=[4], minibatch_size=32, learning_rate=0.02, epochs=200)\n",
    "\n",
    "########\n",
    "# 4dim #\n",
    "########\n",
    "\n",
    "# train_file = \"datasets/4dim/train.txt\"\n",
    "# emb_file = \"glove.6B.50d.txt\"\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "# model_file_name = \"torch.4dim.model\"\n",
    "# loss_file = \"datasets/4dim/loss.txt\"\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100) # 39\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=5, minibatch_size=32, learning_rate=0.001, epochs=300, adam=True) # 43%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, minibatch_size=32, learning_rate=0.01, epochs=300, adam=True) # 45%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, minibatch_size=32, learning_rate=0.015, epochs=300, adam=True) # 45.51%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=10, minibatch_size=32, learning_rate=0.005, epochs=500, adam=True) # 45.51%\n",
    "\n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, momentum=0.9, tfidf=True, threshold=0, max_features=10000) # Accuracy: 46.47%\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=256, learning_rate=0.0015, epochs=500, adam=False) # Accuracy: 33.33%\n",
    "\n",
    "# 1 Layer\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=32, learning_rate=0.002, epochs=500)\n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=100, minibatch_size=32, learning_rate=0.0001, epochs=500, average_emb_sentence=True, adam=True)\n",
    "##BEST MODEL###\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=100, hidden_units=30, minibatch_size=32, learning_rate=0.001, epochs=500, average_emb_sentence=True, adam=True)\n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=80, hidden_units_other_layers=[10, 10], minibatch_size=64, learning_rate=0.0001, epochs=500, average_emb_sentence=True) # Accuracy: 65.06%\n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=80, minibatch_size=32, learning_rate=0.0015, epochs=500, average_emb_sentence=True, adam=False)\n",
    "\n",
    "# max_seq_length=40, hidden_units=80, minibatch_size=64, learning_rate=0.0015, epochs=500, momentum=0.95, tfidf=True\n",
    "# 2 Layer\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=50, hidden_units_other_layers=[50], minibatch_size=64, learning_rate=0.001, epochs=200)\n",
    "#############\n",
    "# questions #\n",
    "#############\n",
    "\n",
    "# train_file = \"datasets/questions/train.txt\"\n",
    "# emb_file = \"ufvytar.100d.txt\"\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "# model_file_name = \"torch.questions.model\"\n",
    "# loss_file = \"datasets/questions/loss.txt\"\n",
    "# # nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "\n",
    "# # Validation\n",
    "# # 1 Layer\n",
    "##BEST MODEL###\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=200)\n",
    "\n",
    "# 2 Layer\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=50, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.025, epochs=400)\n",
    "\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=40, hidden_units=5, hidden_units_other_layers=[5, 5], minibatch_size=128, learning_rate=0.2, epochs=400, average_emb_sentence=True) # Accuracy: 61.86%\n",
    "#########\n",
    "# odiya #\n",
    "#########\n",
    "\n",
    "train_file = \"datasets/odiya/train.txt\"\n",
    "emb_file = \"fasttext.wiki.300d.vec\"\n",
    "pred_file = \"datasets/odiya/val.test\"\n",
    "pred_true_labels = \"datasets/odiya/val.txt\"\n",
    "model_file_name = \"torch.odia.model\"\n",
    "loss_file = \"datasets/odiya/loss.txt\"\n",
    "# # # nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# # # 1 Layer\n",
    "# # # nn_model = NeuralModel_Torch(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# # # 2 Layer\n",
    "# ##BEST MODEL###\n",
    "# nn_model = NeuralModel_Torch(emb_file, max_seq_length=30, hidden_units=5, hidden_units_other_layers=[5], minibatch_size=128, learning_rate=0.01, epochs=200) \n",
    "\n",
    "nn_model = NeuralModel_Torch(emb_file, max_seq_length=30, hidden_units=5, hidden_units_other_layers=[5, 5], minibatch_size=128, learning_rate=0.01, epochs=200) \n",
    "\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=10, minibatch_size=64, learning_rate=0.35, epochs=100)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=20, hidden_units=10, minibatch_size=64, learning_rate=0.1, epochs=1000)\n",
    "# nn_model = NeuralModel(emb_file, max_seq_length=10, hidden_units=5, minibatch_size=32, learning_rate=0.1, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 1.092742357755962\n",
      "Epoch: 2 - Loss: 1.0883695351450067\n",
      "Epoch: 3 - Loss: 1.0845312545174046\n",
      "Epoch: 4 - Loss: 1.0808350600694354\n",
      "Epoch: 5 - Loss: 1.077128681383635\n",
      "Epoch: 6 - Loss: 1.0737557235517\n",
      "Epoch: 7 - Loss: 1.07076062905161\n",
      "Epoch: 8 - Loss: 1.0681435886182282\n",
      "Epoch: 9 - Loss: 1.065899618048417\n",
      "Epoch: 10 - Loss: 1.0638974139564916\n",
      "Epoch: 11 - Loss: 1.0620200922614649\n",
      "Epoch: 12 - Loss: 1.0601877789748342\n",
      "Epoch: 13 - Loss: 1.0582967582501863\n",
      "Epoch: 14 - Loss: 1.05633418810995\n",
      "Epoch: 15 - Loss: 1.0542912633795487\n",
      "Epoch: 16 - Loss: 1.0521641066199854\n",
      "Epoch: 17 - Loss: 1.0498961022025661\n",
      "Epoch: 18 - Loss: 1.0473855972290038\n",
      "Epoch: 19 - Loss: 1.044520233806811\n",
      "Epoch: 20 - Loss: 1.0411638535951313\n",
      "Epoch: 21 - Loss: 1.037117970617194\n",
      "Epoch: 22 - Loss: 1.0320707333715338\n",
      "Epoch: 23 - Loss: 1.025720475221935\n",
      "Epoch: 24 - Loss: 1.0180110730622944\n",
      "Epoch: 25 - Loss: 1.00897431185371\n",
      "Epoch: 26 - Loss: 0.9987533418755782\n",
      "Epoch: 27 - Loss: 0.9876566190468637\n",
      "Epoch: 28 - Loss: 0.9761372666609914\n",
      "Epoch: 29 - Loss: 0.9646896236821225\n",
      "Epoch: 30 - Loss: 0.9537565024275528\n",
      "Epoch: 31 - Loss: 0.9436712710480941\n",
      "Epoch: 32 - Loss: 0.9346128043375517\n",
      "Epoch: 33 - Loss: 0.9266169729985689\n",
      "Epoch: 34 - Loss: 0.9195903596125151\n",
      "Epoch: 35 - Loss: 0.9133765697479248\n",
      "Epoch: 36 - Loss: 0.9078005746791237\n",
      "Epoch: 37 - Loss: 0.9027135572935405\n",
      "Epoch: 38 - Loss: 0.898027021006534\n",
      "Epoch: 39 - Loss: 0.8936397615231966\n",
      "Epoch: 40 - Loss: 0.8894551484208358\n",
      "Epoch: 41 - Loss: 0.8854174181034691\n",
      "Epoch: 42 - Loss: 0.881482846485941\n",
      "Epoch: 43 - Loss: 0.8776142013700385\n",
      "Epoch: 44 - Loss: 0.8738120857037996\n",
      "Epoch: 45 - Loss: 0.870074220707542\n",
      "Epoch: 46 - Loss: 0.8663894245499059\n",
      "Epoch: 47 - Loss: 0.8627358687551399\n",
      "Epoch: 48 - Loss: 0.8590843552037289\n",
      "Epoch: 49 - Loss: 0.855416686911332\n",
      "Epoch: 50 - Loss: 0.851707619114926\n",
      "Epoch: 51 - Loss: 0.8479824292032342\n",
      "Epoch: 52 - Loss: 0.8442289559464705\n",
      "Epoch: 53 - Loss: 0.8403873330668399\n",
      "Epoch: 54 - Loss: 0.8364134135999177\n",
      "Epoch: 55 - Loss: 0.8322881504109031\n",
      "Epoch: 56 - Loss: 0.8281218924020466\n",
      "Epoch: 57 - Loss: 0.8238559020192999\n",
      "Epoch: 58 - Loss: 0.8195369682813946\n",
      "Epoch: 59 - Loss: 0.8151884480526573\n",
      "Epoch: 60 - Loss: 0.8108245868431895\n",
      "Epoch: 61 - Loss: 0.8064351527314437\n",
      "Epoch: 62 - Loss: 0.8020501632439463\n",
      "Epoch: 63 - Loss: 0.7976489280399524\n",
      "Epoch: 64 - Loss: 0.7932378078761854\n",
      "Epoch: 65 - Loss: 0.7888385063723514\n",
      "Epoch: 66 - Loss: 0.7844435654188457\n",
      "Epoch: 67 - Loss: 0.7800809050861158\n",
      "Epoch: 68 - Loss: 0.7757740836394461\n",
      "Epoch: 69 - Loss: 0.7715501440198798\n",
      "Epoch: 70 - Loss: 0.7674367346261677\n",
      "Epoch: 71 - Loss: 0.7634232351654454\n",
      "Epoch: 72 - Loss: 0.759545920397106\n",
      "Epoch: 73 - Loss: 0.7558020259204664\n",
      "Epoch: 74 - Loss: 0.7521946605883146\n",
      "Epoch: 75 - Loss: 0.7487526605003758\n",
      "Epoch: 76 - Loss: 0.7454652823899922\n",
      "Epoch: 77 - Loss: 0.7423418954799049\n",
      "Epoch: 78 - Loss: 0.7393769571655675\n",
      "Epoch: 79 - Loss: 0.7365599952246014\n",
      "Epoch: 80 - Loss: 0.7338831638035022\n",
      "Epoch: 81 - Loss: 0.7313598193620381\n",
      "Epoch: 82 - Loss: 0.7289818048477172\n",
      "Epoch: 83 - Loss: 0.7267220986516852\n",
      "Epoch: 84 - Loss: 0.7245786409629019\n",
      "Epoch: 85 - Loss: 0.7225293648870368\n",
      "Epoch: 86 - Loss: 0.7205637423615706\n",
      "Epoch: 87 - Loss: 0.7186910685740019\n",
      "Epoch: 88 - Loss: 0.7168898908715499\n",
      "Epoch: 89 - Loss: 0.715156604741749\n",
      "Epoch: 90 - Loss: 0.7134853657923247\n",
      "Epoch: 91 - Loss: 0.7118753376759981\n",
      "Epoch: 92 - Loss: 0.7103157752438596\n",
      "Epoch: 93 - Loss: 0.7088157051487973\n",
      "Epoch: 94 - Loss: 0.7073662312407243\n",
      "Epoch: 95 - Loss: 0.705971033949601\n",
      "Epoch: 96 - Loss: 0.704607303518998\n",
      "Epoch: 97 - Loss: 0.7033094255547775\n",
      "Epoch: 98 - Loss: 0.7020493664239582\n",
      "Epoch: 99 - Loss: 0.7008506417274475\n",
      "Epoch: 100 - Loss: 0.6996638956822847\n",
      "Epoch: 101 - Loss: 0.698537153946726\n",
      "Epoch: 102 - Loss: 0.6974344491958618\n",
      "Epoch: 103 - Loss: 0.6963725585686533\n",
      "Epoch: 104 - Loss: 0.6953417840756868\n",
      "Epoch: 105 - Loss: 0.6943311302285445\n",
      "Epoch: 106 - Loss: 0.6933517550167284\n",
      "Epoch: 107 - Loss: 0.6923938249286853\n",
      "Epoch: 108 - Loss: 0.6914610028266907\n",
      "Epoch: 109 - Loss: 0.69054173670317\n",
      "Epoch: 110 - Loss: 0.6896477661634747\n",
      "Epoch: 111 - Loss: 0.6887718439102173\n",
      "Epoch: 112 - Loss: 0.6879403302544042\n",
      "Epoch: 113 - Loss: 0.6871171167022303\n",
      "Epoch: 114 - Loss: 0.6863110247411226\n",
      "Epoch: 115 - Loss: 0.6855237860428659\n",
      "Epoch: 116 - Loss: 0.6847554740152861\n",
      "Epoch: 117 - Loss: 0.6839974635525754\n",
      "Epoch: 118 - Loss: 0.6832602940107647\n",
      "Epoch: 119 - Loss: 0.6825388651145132\n",
      "Epoch: 120 - Loss: 0.681821526351728\n",
      "Epoch: 121 - Loss: 0.681129651320608\n",
      "Epoch: 122 - Loss: 0.6804420847641794\n",
      "Epoch: 123 - Loss: 0.6797778399367082\n",
      "Epoch: 124 - Loss: 0.679118075496272\n",
      "Epoch: 125 - Loss: 0.678462204807683\n",
      "Epoch: 126 - Loss: 0.6778280841676813\n",
      "Epoch: 127 - Loss: 0.6772005696045725\n",
      "Epoch: 128 - Loss: 0.6765726936490912\n",
      "Epoch: 129 - Loss: 0.6759657081804777\n",
      "Epoch: 130 - Loss: 0.6753652547535144\n",
      "Epoch: 131 - Loss: 0.6747698727406953\n",
      "Epoch: 132 - Loss: 0.6741783110718979\n",
      "Epoch: 133 - Loss: 0.6736083670666343\n",
      "Epoch: 134 - Loss: 0.6730376042817768\n",
      "Epoch: 135 - Loss: 0.6724905246182492\n",
      "Epoch: 136 - Loss: 0.6719417741424158\n",
      "Epoch: 137 - Loss: 0.6714114164051257\n",
      "Epoch: 138 - Loss: 0.6708783996732611\n",
      "Epoch: 139 - Loss: 0.6703499580684461\n",
      "Epoch: 140 - Loss: 0.669826076532665\n",
      "Epoch: 141 - Loss: 0.6693085532439382\n",
      "Epoch: 142 - Loss: 0.6688002580090573\n",
      "Epoch: 143 - Loss: 0.6682859954081084\n",
      "Epoch: 144 - Loss: 0.6677902554210864\n",
      "Epoch: 145 - Loss: 0.6672903487556859\n",
      "Epoch: 146 - Loss: 0.6668020530750877\n",
      "Epoch: 147 - Loss: 0.6663212682071485\n",
      "Epoch: 148 - Loss: 0.665857364002027\n",
      "Epoch: 149 - Loss: 0.6653943193586249\n",
      "Epoch: 150 - Loss: 0.6649443344066017\n",
      "Epoch: 151 - Loss: 0.6644967694031565\n",
      "Epoch: 152 - Loss: 0.6640467888430546\n",
      "Epoch: 153 - Loss: 0.6636103128132067\n",
      "Epoch: 154 - Loss: 0.6631545926395216\n",
      "Epoch: 155 - Loss: 0.6627216088144403\n",
      "Epoch: 156 - Loss: 0.6623015993519833\n",
      "Epoch: 157 - Loss: 0.6618850149606403\n",
      "Epoch: 158 - Loss: 0.6614740459542525\n",
      "Epoch: 159 - Loss: 0.6610718727111816\n",
      "Epoch: 160 - Loss: 0.6606697333486456\n",
      "Epoch: 161 - Loss: 0.6602828232865584\n",
      "Epoch: 162 - Loss: 0.6598946000400342\n",
      "Epoch: 163 - Loss: 0.6595046677087483\n",
      "Epoch: 164 - Loss: 0.6591288986958955\n",
      "Epoch: 165 - Loss: 0.6587535588364852\n",
      "Epoch: 166 - Loss: 0.6583900508127715\n",
      "Epoch: 167 - Loss: 0.6580120193330865\n",
      "Epoch: 168 - Loss: 0.6576600018300508\n",
      "Epoch: 169 - Loss: 0.6572993466728612\n",
      "Epoch: 170 - Loss: 0.656949277928001\n",
      "Epoch: 171 - Loss: 0.6566052468199479\n",
      "Epoch: 172 - Loss: 0.6562661936408595\n",
      "Epoch: 173 - Loss: 0.6559400056537829\n",
      "Epoch: 174 - Loss: 0.655621726261942\n",
      "Epoch: 175 - Loss: 0.6552847285019724\n",
      "Epoch: 176 - Loss: 0.6549789002067165\n",
      "Epoch: 177 - Loss: 0.6546715077600981\n",
      "Epoch: 178 - Loss: 0.6543720213990463\n",
      "Epoch: 179 - Loss: 0.6540602997729653\n",
      "Epoch: 180 - Loss: 0.6537601201157821\n",
      "Epoch: 181 - Loss: 0.6534679463035182\n",
      "Epoch: 182 - Loss: 0.6531724798051934\n",
      "Epoch: 183 - Loss: 0.6528768909604926\n",
      "Epoch: 184 - Loss: 0.6525927248754\n",
      "Epoch: 185 - Loss: 0.6523105037839789\n",
      "Epoch: 186 - Loss: 0.652022511708109\n",
      "Epoch: 187 - Loss: 0.6517237280544482\n",
      "Epoch: 188 - Loss: 0.6514330506324768\n",
      "Epoch: 189 - Loss: 0.6511582431040313\n",
      "Epoch: 190 - Loss: 0.6508874937107688\n",
      "Epoch: 191 - Loss: 0.6506137540465907\n",
      "Epoch: 192 - Loss: 0.6503414480309737\n",
      "Epoch: 193 - Loss: 0.6500811445085626\n",
      "Epoch: 194 - Loss: 0.6498206264094303\n",
      "Epoch: 195 - Loss: 0.6495550055252878\n",
      "Epoch: 196 - Loss: 0.6492956770093817\n",
      "Epoch: 197 - Loss: 0.6490278720855713\n",
      "Epoch: 198 - Loss: 0.648770262693104\n",
      "Epoch: 199 - Loss: 0.6485043111600374\n",
      "Epoch: 200 - Loss: 0.6482509500101993\n"
     ]
    }
   ],
   "source": [
    "nn_model.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel_Torch.load_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= model.classify(pred_file + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products\n",
    "# pred_file = \"datasets/products/val.test\"\n",
    "# pred_true_labels = \"datasets/products/val.txt\"\n",
    "\n",
    "# 4dim\n",
    "\n",
    "# pred_file = \"datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"datasets/4dim/val.txt\"\n",
    "\n",
    "\n",
    "# questions\n",
    "# pred_file = \"datasets/questions/val.test\"\n",
    "# pred_true_labels = \"datasets/questions/val.txt\"\n",
    "\n",
    "# odiya\n",
    "\n",
    "# pred_file = \"datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"datasets/odiya/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           1345\n",
       "business          937\n",
       "entertainment     758\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset.true_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred         \n",
       "sports           1437\n",
       "business          900\n",
       "entertainment     703\n",
       "dtype: int64"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.51%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
