{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string \n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    return text.split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Model(object, metaclass=ABCMeta):\n",
    "    def __init__(self, model_file):\n",
    "        self.model_file = model_file\n",
    "\n",
    "    def save_model(self, model):\n",
    "        with open(self.model_file, \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open(self.model_file, \"rb\") as file:\n",
    "            model = pickle.load(file)\n",
    "        return model\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, input_file):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def classify(self, input_file, model):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NaiveBayes is a generative classifier based on the Naive assumption that features are independent from each other\n",
    "P(w1, w2, ..., wn|y) = P(w1|y) P(w2|y) ... P(wn|y)\n",
    "Thus argmax_{y} (P(y|w1,w2, ... wn)) can be modeled as argmax_{y} P(w1|y) P(w2|y) ... P(wn|y) P(y) using Bayes Rule\n",
    "and P(w1, w2, ... ,wn) is constant with respect to argmax_{y} \n",
    "Please refer to lecture notes Chapter 4 for more details\n",
    "\"\"\"\n",
    "\n",
    "from work.Model import *\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "class NaiveBayes(Model):\n",
    "    \n",
    "    def train(self, input_file):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        ## TODO write your code here\n",
    "        x = 3\n",
    "        breakpoint()\n",
    "        model = None\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return model\n",
    "    \n",
    "    def read_input_file(self):\n",
    "\n",
    "        with open(self.model_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, labels = map(list, zip(*data_split))\n",
    "\n",
    "        tokenized_text = [tokenize(text.lower()) for text in texts]\n",
    "\n",
    "        return tokenized_text, labels\n",
    "\n",
    "    def create_vocabulary(self, tokenized_text):\n",
    "\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "        vocabulary = list(flattened_list_count.keys())\n",
    "        return vocabulary\n",
    "\n",
    "    def count_frequency_word_label(self, sentences, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param sentences (list[list]): sentecnces tokenized\n",
    "        :param labels (list): list of labels\n",
    "        :return: count(c_j, w_i) refers to the count of word w_i in documents with label c_j\n",
    "                _sum_{i=1}^{V}{count(c_j, w_i)} sum of the counts of each word in our vocabulary in class c_j \n",
    "                 count(c_j) refers to the count of label c_j \n",
    "        \"\"\"\n",
    "        count_word_label = []\n",
    "        count_words_per_label = defaultdict(int)\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            for token in sentence:\n",
    "                count_word_label.append((token, label))\n",
    "                count_words_per_label[label] += 1\n",
    "            \n",
    "        # count_word_label = [(token, label) for sentence, label in zip(sentences, labels) for token in sentence]\n",
    "        count_label = Counter(labels)\n",
    "        return Counter(count_word_label), count_words_per_label, count_label\n",
    "\n",
    "    def compute_probability(self, sentence_tokenized, label, count_word, count_words_label, count_label, n_documents):\n",
    "        total_prob = 0.0\n",
    "\n",
    "        # Compute log(P(w_i|c_j)\n",
    "        for word in sentence_tokenized:\n",
    "            word_label = (word, label)\n",
    "            prob = count_word[word_label]/count_words_label[label]\n",
    "            if(prob == 0):\n",
    "                print(word_label)\n",
    "            prob_log = np.log(prob)\n",
    "            total_prob += prob_log # save total\n",
    "\n",
    "        # add prior probability\n",
    "        total_prob += np.log(count_label[label]/n_documents)\n",
    "        return total_prob\n",
    "        \n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_file: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\" \n",
    "        ## TODO write your code here\n",
    "        preds = None\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(\"work/datasets/4dim.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, labels = nb.read_input_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word, count_words_label, count_label =nb.count_frequency_word_label(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_documents = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample_1, label_1 = text[1], 'pos.dec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('twin', 'pos.dec')\n",
      "('brilliant', 'pos.dec')\n",
      "('quicker', 'pos.dec')\n",
      "('opposite', 'pos.dec')\n",
      "('grabbing', 'pos.dec')\n",
      "('croissant', 'pos.dec')\n",
      "('bagel', 'pos.dec')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/6vxh_8k15_n5m7tsxrjkzsvm0000gn/T/ipykernel_46274/159099992.py:77: RuntimeWarning: divide by zero encountered in log\n",
      "  prob_log = np.log(prob)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.compute_probability(text_sample_1, label_1, count_word, count_words_label, count_label, n_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1274.5210730012898"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.compute_probability(text_sample_1, label_1, count_word, count_words_label, count_label, n_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = Features(\"work/datasets/4dim.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Features' object has no attribute 'data_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feat\u001b[39m.\u001b[39;49mdata_split\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Features' object has no attribute 'data_split'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
