{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    return text.lower().split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file, has_label=True):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        # if has_label:\n",
    "        print('####### HAS LABEL')\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "        self.labelset = list(set(self.labels))\n",
    "        # else:\n",
    "        #     print('####### DOES NOT HAVE LABEL')\n",
    "        #     texts = data\n",
    "        #     self.labels = None\n",
    "        #     self.labelset = None\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features_NB(Features):\n",
    "\n",
    "    def __init__(self, model_file, has_label=True):\n",
    "        super(Features_NB, self).__init__(model_file, has_label)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text)\n",
    "        self.size_vocab = len(self.vocabulary)\n",
    "        self.feature_weights = self.generate_feature_weights(laplace_smoothing=True)\n",
    "\n",
    "    def read_input_file(self, input_file):\n",
    "\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "\n",
    "    def count_frequency_word_label(self, sentences, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param sentences (list[list]): sentences tokenized\n",
    "        :param labels (list): list of labels\n",
    "        :return: count(c_j, w_i) refers to the count of word w_i in documents with label c_j\n",
    "                _sum_{i=1}^{V}{count(c_j, w_i)} sum of the counts of each word in our vocabulary in class c_j \n",
    "                 count(c_j) refers to the count of label c_j \n",
    "        \"\"\"\n",
    "        count_word_label = []\n",
    "        count_words_per_label = defaultdict(int)\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            for token in sentence:\n",
    "                count_word_label.append((token, label))\n",
    "                count_words_per_label[label] += 1\n",
    "            \n",
    "        # count_word_label = [(token, label) for sentence, label in zip(sentences, labels) for token in sentence]\n",
    "        count_label = Counter(labels)\n",
    "        return Counter(count_word_label), count_words_per_label, count_label\n",
    "\n",
    "    def create_vocabulary(self, tokenized_text):\n",
    "\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "        vocabulary = list(flattened_list_count.keys())\n",
    "        return vocabulary\n",
    "    \n",
    "    def generate_feature_weights(self, laplace_smoothing=True):\n",
    "        # Vocabulary\n",
    "        size_vocab = len(self.vocabulary)\n",
    "\n",
    "        # Maximum Likelihood Estimates\n",
    "        count_word, count_words_label, count_label = self.count_frequency_word_label(self.tokenized_text, self.labels)\n",
    "\n",
    "        # Feature Weights\n",
    "        feature_weights = defaultdict()\n",
    "        feature_weights[\"count_word\"] = count_word\n",
    "        feature_weights[\"count_words_label\"] = count_words_label\n",
    "        feature_weights[\"count_label\"] = count_label\n",
    "\n",
    "        # Generate feature weights => P(w_i|c_i)\n",
    "        \n",
    "        # for word_label in count_word.keys():\n",
    "        #     word, label = word_label\n",
    "        #     if laplace_smoothing:\n",
    "        #         prob = (count_word[word_label] + 1)/(count_words_label[label] + size_vocab)\n",
    "        #     else:\n",
    "        #         prob = count_word[word_label]/count_words_label[label]\n",
    "        #     feature_weights[word_label] = np.log(prob)\n",
    "        # Generate feature weights, apriori probability: P(c_i)\n",
    "        # for label in self.labels:\n",
    "        #     feature_weights[label] = np.log(count_label[label]/len(self.tokenized_text))\n",
    "\n",
    "        return feature_weights\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, target_label, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "\n",
    "        #         # Read Input File\n",
    "        # tokenized_text, labels = self.read_input_file(input_file)\n",
    "        # # Vocabulary\n",
    "        # vocabulary = self.create_vocabulary(tokenized_text)\n",
    "        # # Maximum Likelihood Estimates\n",
    "        # count_word, count_words_label, count_label = self.count_frequency_word_label(tokenized_text, labels)\n",
    "\n",
    "        # Compute log(P(w_i|c_j)\n",
    "        features = []\n",
    "        total_prob = 0.0\n",
    "        for word in tokenized:\n",
    "            word_label = (word, target_label)\n",
    "            # Laplace Smoothing\n",
    "            prob = (model.feature_weights[\"count_word\"][word_label] + 1)/(model.feature_weights[\"count_words_label\"][target_label] + model.size_vocab)\n",
    "            prob_log = np.log(prob)\n",
    "            total_prob += prob_log # save total\n",
    "\n",
    "        # add prior probability\n",
    "        n_documents = len(model.tokenized_text)\n",
    "        total_prob += np.log(model.feature_weights[\"count_label\"][target_label]/n_documents)\n",
    "        return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Model(object, metaclass=ABCMeta):\n",
    "    def __init__(self, model_file):\n",
    "        self.model_file = model_file\n",
    "\n",
    "    def save_model(self, model):\n",
    "        with open(self.model_file, \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open(self.model_file, \"rb\") as file:\n",
    "            model = pickle.load(file)\n",
    "        return model\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, input_file):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def classify(self, input_file, model):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NaiveBayes is a generative classifier based on the Naive assumption that features are independent from each other\n",
    "P(w1, w2, ..., wn|y) = P(w1|y) P(w2|y) ... P(wn|y)\n",
    "Thus argmax_{y} (P(y|w1,w2, ... wn)) can be modeled as argmax_{y} P(w1|y) P(w2|y) ... P(wn|y) P(y) using Bayes Rule\n",
    "and P(w1, w2, ... ,wn) is constant with respect to argmax_{y} \n",
    "Please refer to lecture notes Chapter 4 for more details\n",
    "\"\"\"\n",
    "\n",
    "from work.Model import *\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "class NaiveBayes(Model):\n",
    "\n",
    "    def __init__(self, model_file):\n",
    "        super(NaiveBayes, self).__init__(model_file)\n",
    "\n",
    "    \n",
    "    def train(self, input_file):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "\n",
    "        features_naive_bayes = Features_NB(input_file, True)\n",
    "\n",
    "\n",
    "        # # Read Input File\n",
    "        # tokenized_text, labels = self.read_input_file(input_file)\n",
    "        # # Vocabulary\n",
    "        # vocabulary = self.create_vocabulary(tokenized_text)\n",
    "        # # Maximum Likelihood Estimates\n",
    "        # count_word, count_words_label, count_label = self.count_frequency_word_label(tokenized_text, labels)\n",
    "        \n",
    "\n",
    "        # ## TODO write your code here\n",
    "        # x = 3\n",
    "        # breakpoint()\n",
    "        # model = None\n",
    "\n",
    "        # model = {\n",
    "        #     \"count_word\": count_word,\n",
    "        #     \"count_words_label\": count_words_label,\n",
    "        #     \"count_label\": count_label,\n",
    "        #     \"n_documents\": len(labels),\n",
    "        #     \"vocabulary\": vocabulary\n",
    "        # }\n",
    "        # ## Save the model\n",
    "        self.save_model(features_naive_bayes)\n",
    "    \n",
    "    # def read_input_file(self, model_file):\n",
    "\n",
    "    #     with open(model_file) as file:\n",
    "    #         data = file.read().splitlines()\n",
    "\n",
    "    #     data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "    #     texts, labels = map(list, zip(*data_split))\n",
    "\n",
    "    #     tokenized_text = [tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    #     return tokenized_text, labels\n",
    "\n",
    "    # def create_vocabulary(self, tokenized_text):\n",
    "\n",
    "    #     # Append everything together in a dictionary\n",
    "    #     flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "    #     flattened_list_count = Counter(flattened_list)\n",
    "    #     vocabulary = list(flattened_list_count.keys())\n",
    "    #     return vocabulary\n",
    "\n",
    "    # def count_frequency_word_label(self, sentences, labels):\n",
    "        \n",
    "    #     \"\"\"\n",
    "    #     :param sentences (list[list]): sentences tokenized\n",
    "    #     :param labels (list): list of labels\n",
    "    #     :return: count(c_j, w_i) refers to the count of word w_i in documents with label c_j\n",
    "    #             _sum_{i=1}^{V}{count(c_j, w_i)} sum of the counts of each word in our vocabulary in class c_j \n",
    "    #              count(c_j) refers to the count of label c_j \n",
    "    #     \"\"\"\n",
    "    #     count_word_label = []\n",
    "    #     count_words_per_label = defaultdict(int)\n",
    "    #     for sentence, label in zip(sentences, labels):\n",
    "    #         for token in sentence:\n",
    "    #             count_word_label.append((token, label))\n",
    "    #             count_words_per_label[label] += 1\n",
    "            \n",
    "    #     # count_word_label = [(token, label) for sentence, label in zip(sentences, labels) for token in sentence]\n",
    "    #     count_label = Counter(labels)\n",
    "    #     return Counter(count_word_label), count_words_per_label, count_label\n",
    "\n",
    "    # def compute_probability(self,\n",
    "    #     sentence_tokenized,\n",
    "    #     label,\n",
    "    #     count_word,\n",
    "    #     count_words_label,\n",
    "    #     count_label,\n",
    "    #     n_documents,\n",
    "    #     size_vocab,\n",
    "    #     laplace_smoothig=True):\n",
    "    #     total_prob = 0.0\n",
    "\n",
    "    #     # Compute log(P(w_i|c_j)\n",
    "    #     for word in sentence_tokenized:\n",
    "    #         word_label = (word, label)\n",
    "    #         if laplace_smoothig:\n",
    "    #             prob = (count_word[word_label] + 1)/(count_words_label[label] + size_vocab)\n",
    "    #         else:\n",
    "    #             prob = count_word[word_label]/count_words_label[label]\n",
    "    #         prob_log = np.log(prob)\n",
    "    #         total_prob += prob_log # save total\n",
    "\n",
    "    #     # add prior probability\n",
    "    #     total_prob += np.log(count_label[label]/n_documents)\n",
    "    #     return total_prob\n",
    "        \n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_file: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\" \n",
    "\n",
    "\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = model.read_input_file(input_file)\n",
    "\n",
    "        # Compute P(sentence|c_i) for each of c_i\n",
    "        # labels = model['count_label'].keys()\n",
    "        # count_word = model['count_word']\n",
    "        # count_words_label = model['count_words_label']\n",
    "        # count_label = model['count_label']\n",
    "        # n_documents = model['n_documents']\n",
    "        # size_vocab = len(model['vocabulary'])\n",
    "        \n",
    "\n",
    "        preds = []\n",
    "        for sentence in tokenized_text:\n",
    "            class_predictions = defaultdict()\n",
    "            for label in set(model.labels):\n",
    "                class_predictions[label] = model.get_features(sentence, label, model)\n",
    "            # Find the class with the highest value\n",
    "            class_with_highest_value = max(class_predictions, key=lambda k: class_predictions[k])\n",
    "            preds.append(class_with_highest_value)\n",
    "                #self.compute_probability(\n",
    "        #             sentence, label, count_word,\n",
    "        #             count_words_label, count_label,\n",
    "        #             n_documents, size_vocab)\n",
    "                \n",
    "            # Find the key with the highest value\n",
    "            # key_with_highest_value = max(class_predictions, key=lambda k: class_predictions[k])\n",
    "        #     preds.append(key_with_highest_value)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(\"nb.4dim.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### HAS LABEL\n"
     ]
    }
   ],
   "source": [
    "feat = Features_NB(\"work/datasets/4dim.train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### HAS LABEL\n"
     ]
    }
   ],
   "source": [
    "nb.train(\"work/datasets/4dim.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_documents = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_sample_1, label_1 = text[1], 'pos.dec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = nb.load_model()\n",
    "val_dataset = \"work/datasets/4dim.train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb.classify(val_dataset, loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"work/datasets/4dim.train\" + \".pred.txt\"\n",
    "\n",
    "## Save the predictions: one label prediction per line\n",
    "with open(output_file, \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
