{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'g' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     69\u001b[0m X_test \u001b[39m=\u001b[39m [[\u001b[39m\"\u001b[39m\u001b[39mgood\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mservice\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     70\u001b[0m           [\u001b[39m\"\u001b[39m\u001b[39mpoor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mproduct\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbad\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m     72\u001b[0m clf \u001b[39m=\u001b[39m MultinomialNaiveBayes(alpha\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     74\u001b[0m predictions \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m i, prediction \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(predictions):\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mMultinomialNaiveBayes.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m class_docs:\n\u001b[1;32m     32\u001b[0m         \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m doc:\n\u001b[0;32m---> 33\u001b[0m             word_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocabulary\u001b[39m.\u001b[39;49mindex(word)\n\u001b[1;32m     34\u001b[0m             word_counts_per_class[i][word_idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[39m# Apply Laplace smoothing to word counts\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'g' is not in list"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Laplace smoothing parameter\n",
    "        self.classes = None\n",
    "        self.class_priors = None\n",
    "        self.word_probs = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate class priors and vocabulary size\n",
    "        self.classes, class_counts = np.unique(y, return_counts=True)\n",
    "        num_classes = len(self.classes)\n",
    "        total_samples = len(y)\n",
    "        \n",
    "        self.class_priors = class_counts / total_samples\n",
    "\n",
    "        # Create a vocabulary from the training data\n",
    "        self.vocabulary = set()\n",
    "        for doc in X:\n",
    "            self.vocabulary.update(doc)\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        \n",
    "        # Initialize count matrices for word occurrences\n",
    "        word_counts_per_class = np.zeros((num_classes, self.vocabulary_size))\n",
    "\n",
    "        # Count word occurrences for each class\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            class_docs = X[y == cls]\n",
    "            for doc in class_docs:\n",
    "                for word in doc:\n",
    "                    word_idx = self.vocabulary.index(word)\n",
    "                    word_counts_per_class[i][word_idx] += 1\n",
    "\n",
    "        # Apply Laplace smoothing to word counts\n",
    "        smoothed_counts = word_counts_per_class + self.alpha\n",
    "        class_word_counts = np.sum(smoothed_counts, axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate word probabilities for each class\n",
    "        word_probs = smoothed_counts / class_word_counts\n",
    "\n",
    "        self.word_probs = word_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for doc in X:\n",
    "            class_scores = np.log(self.class_priors)  # Initialize with class priors\n",
    "            for word in doc:\n",
    "                if word in self.vocabulary:\n",
    "                    word_idx = self.vocabulary.index(word)\n",
    "                    class_scores += np.log(self.word_probs[:, word_idx])\n",
    "            if not np.isnan(class_scores).all():\n",
    "                predicted_class = self.classes[np.argmax(class_scores)]\n",
    "            else:\n",
    "                # Handle the case when all class_scores are NaN\n",
    "                # You can choose to assign a default class or handle it differently\n",
    "                predicted_class = \"unknown\"\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "X_train = [[\"good\", \"product\", \"great\"],\n",
    "           [\"poor\", \"service\"],\n",
    "           [\"awesome\", \"experience\", \"good\"],\n",
    "           [\"poor\", \"product\"]]\n",
    "y_train = [\"positive\", \"negative\", \"positive\", \"negative\"]\n",
    "\n",
    "X_test = [[\"good\", \"service\"],\n",
    "          [\"poor\", \"product\", \"bad\"]]\n",
    "\n",
    "clf = MultinomialNaiveBayes(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Example {i+1}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
