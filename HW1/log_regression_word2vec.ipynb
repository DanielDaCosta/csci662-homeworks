{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = KeyedVectors.load(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_embeddings[\"computer\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    # 5) Remove Stop Words\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_word2vec(tokenized_sentence, embedding_matrix):\n",
    "    \"\"\"Convert sentence to Word2Vec Embeeding.\n",
    "    Each sentence is represented by the average of all of word embeddings \n",
    "    of that sentence.\n",
    "    \"\"\"\n",
    "    sentence_embedding = []\n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        # get embedding of word if exists\n",
    "        try:\n",
    "            word_emb = embedding_matrix[word]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Compute average of the sentence\n",
    "    if len(sentence_embedding) > 0:\n",
    "        stacked_arrays = np.vstack(sentence_embedding)\n",
    "        elementwise_average = np.mean(stacked_arrays, axis=0)\n",
    "    else:\n",
    "        elementwise_average = np.zeros(embedding_matrix.shape[1])\n",
    "    return elementwise_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Logistic Regression Features #\n",
    "################################\n",
    "\n",
    "class Features_LR_Word2Vec(Features):\n",
    "\n",
    "    def __init__(self, model_file, embedding_matrix):\n",
    "        super(Features_LR_Word2Vec, self).__init__(model_file)\n",
    "        self.embedding_matrix = embedding_matrix # Need to save IDF values for inference\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def get_features_word2vec(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to Word2Vec Embeeding.\n",
    "        Each sentence is represented by the average of all of word embeddings \n",
    "        of that sentence.\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "                sentence_embedding.append(word_emb)\n",
    "            except: # remove Out-of-Vocabulary words\n",
    "                pass\n",
    "        \n",
    "        # Compute average of the sentence\n",
    "        if len(sentence_embedding) > 0:\n",
    "            stacked_arrays = np.vstack(sentence_embedding)\n",
    "            elementwise_average = np.mean(stacked_arrays, axis=0)\n",
    "        else:\n",
    "            elementwise_average = np.zeros(self.embedding_matrix.vector_size)\n",
    "        return elementwise_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Refer to Chapter 5 for more details on how to implement a LogisticRegression\n",
    "\"\"\"\n",
    "from work.Model import *\n",
    "\n",
    "class LogisticRegressionWord2Vec(Model):\n",
    "    def __init__(self, model_file, learning_rate=None, epochs=None, batch_size=None, embedding_matrix=None):\n",
    "        super(LogisticRegressionWord2Vec, self).__init__(model_file)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.Y_to_categorical = None # Map Y label to numerical\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "\n",
    "    def initialize_weights(self, num_features, num_labels):\n",
    "        self.weights = np.zeros((num_features, num_labels))\n",
    "        self.bias = np.zeros(num_labels)\n",
    "        # np.random.seed(0)\n",
    "        # self.weights = np.random.rand(num_features, num_labels)\n",
    "        # self.bias = np.random.rand(num_labels)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"Sigmoid function for binary classification\n",
    "\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return 1/(1+e^{-Z})\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "        \n",
    "    def predict_prob(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of shape [num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "        return S\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate Binary cross-entropy\n",
    "        \"\"\"\n",
    "        return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n",
    "\n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "\n",
    "            # Rows with highest probability\n",
    "            S_max = np.argmax(S, axis=1)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "            # Rows with highest probability\n",
    "            S_max = [1 if i > 0.5 else 0 for i in S]\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "\n",
    "    def train(self, input_file, verbose=False):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset and create vocabulary\n",
    "        features_lr_class = Features_LR_Word2Vec(input_file, self.embedding_matrix)\n",
    "        embedding_size = self.embedding_matrix.vector_size\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in enumerate(features_lr_class.tokenized_text):\n",
    "            tmp = features_lr_class.get_features_word2vec(sentence)\n",
    "            updated_text.append(tmp)\n",
    "        \n",
    "        # Transform dataset to Word2Vec space\n",
    "        # Return features with format (n_documents, embedding_size=300)\n",
    "        X = np.array(updated_text)\n",
    "        \n",
    "        # Y\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_lr_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_lr_class.labels]\n",
    "\n",
    "        # Initialize Weights\n",
    "        sample_size = len(features_lr_class.tokenized_text)\n",
    "        n_features = embedding_size\n",
    "        num_labels = len(features_lr_class.labelset)\n",
    "\n",
    "\n",
    "        # Check if it's multinomial or binary classification\n",
    "        if num_labels == 2:\n",
    "            multinomial = False\n",
    "            num_labels = 1 # Only one column to reference 0 or 1\n",
    "        else:\n",
    "            multinomial = True\n",
    "\n",
    "        self.initialize_weights(n_features, num_labels)\n",
    "\n",
    "        # One Hot encoded Y\n",
    "        if multinomial:\n",
    "            Y_onehot = self.OneHot(Y, num_labels)\n",
    "        else:\n",
    "            Y_onehot = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        X_permutation = X[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # Batch_size implementation\n",
    "            for j in range(0, sample_size, batch_size):\n",
    "                X_mini_batch = X_permutation[j:j+batch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+batch_size]\n",
    "\n",
    "                # Z = softmax(X*W + b)\n",
    "                prob = self.predict_prob(X_mini_batch, self.weights, self.bias, multinomial)\n",
    "\n",
    "                # dL/dW\n",
    "                grad_w = (1/batch_size)*np.dot(X_mini_batch.T, prob - y_mini_batch)\n",
    "                grad_b =  (1/batch_size)*np.sum(prob - y_mini_batch, axis=0)\n",
    "\n",
    "            # # break            \n",
    "            # dL/dW\n",
    "                # grad_w = (1/sample_size)*np.dot(X.T, prob - Y_onehot)\n",
    "                # grad_b =  (1/sample_size)*np.sum(prob - Y_onehot, axis=0)\n",
    "\n",
    "                self.weights = self.weights - (self.learning_rate*grad_w)\n",
    "                self.bias = self.bias - (self.learning_rate*grad_b)\n",
    "\n",
    "            # Computing cross-entropy loss\n",
    "            if multinomial:\n",
    "                loss = self.cross_entropy_loss(prob, y_mini_batch)\n",
    "            else:\n",
    "                loss = self.binary_cross_entropy_loss(prob, y_mini_batch)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "        model = {\n",
    "            \"feature_weights\": {\n",
    "                \"weights\": self.weights,\n",
    "                \"bias\": self.bias,\n",
    "                \"Y_to_categorical\": self.Y_to_categorical\n",
    "            },\n",
    "            \"Feature\": features_lr_class\n",
    "        }\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return X, Y_onehot, prob, \n",
    "\n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_fixle: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\"\n",
    "\n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_LR_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_LR_class.read_inference_file(input_file)\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in enumerate(tokenized_text):\n",
    "            tmp = Feature_LR_class.get_features_word2vec(sentence)\n",
    "            updated_text.append(tmp)\n",
    "\n",
    "        tokenized_text = updated_text\n",
    "        \n",
    "\n",
    "        X = np.vstack(tokenized_text)\n",
    "\n",
    "        print(X.shape)\n",
    "\n",
    "        # Prediction\n",
    "        multinomial = True if len(feature_weights['Y_to_categorical'].keys()) > 2 else False\n",
    "        preds_numerical = self.predict(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        # Map indexes to Categorical space\n",
    "        preds_label = []\n",
    "        probs = self.predict_prob(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        for y in preds_numerical:\n",
    "            tmp = feature_weights['Y_to_categorical'][y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label, probs, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.3455683356205985\n",
      "Epoch: 2 - Loss: 0.3388484446076833\n",
      "Epoch: 3 - Loss: 0.3333682782564171\n",
      "Epoch: 4 - Loss: 0.3287315025056436\n",
      "Epoch: 5 - Loss: 0.32471381844996877\n",
      "Epoch: 6 - Loss: 0.3211664071541381\n",
      "Epoch: 7 - Loss: 0.3179870322245108\n",
      "Epoch: 8 - Loss: 0.31510333073549973\n",
      "Epoch: 9 - Loss: 0.31246266812888995\n",
      "Epoch: 10 - Loss: 0.3100257911866595\n",
      "Epoch: 11 - Loss: 0.30776274499194595\n",
      "Epoch: 12 - Loss: 0.305650178889223\n",
      "Epoch: 13 - Loss: 0.3036695282891939\n",
      "Epoch: 14 - Loss: 0.301805762717811\n",
      "Epoch: 15 - Loss: 0.30004650815682354\n",
      "Epoch: 16 - Loss: 0.29838142164776377\n",
      "Epoch: 17 - Loss: 0.2968017388352722\n",
      "Epoch: 18 - Loss: 0.29529994187029457\n",
      "Epoch: 19 - Loss: 0.29386951222571733\n",
      "Epoch: 20 - Loss: 0.29250474417142036\n",
      "Epoch: 21 - Loss: 0.29120060209778403\n",
      "Epoch: 22 - Loss: 0.2899526098988806\n",
      "Epoch: 23 - Loss: 0.2887567640602617\n",
      "Epoch: 24 - Loss: 0.28760946447092123\n",
      "Epoch: 25 - Loss: 0.28650745863812266\n",
      "Epoch: 26 - Loss: 0.28544779615368454\n",
      "Epoch: 27 - Loss: 0.28442779109232535\n",
      "Epoch: 28 - Loss: 0.28344499061913614\n",
      "Epoch: 29 - Loss: 0.28249714851418156\n",
      "Epoch: 30 - Loss: 0.2815822026359469\n",
      "Epoch: 31 - Loss: 0.28069825557549016\n",
      "Epoch: 32 - Loss: 0.2798435579232972\n",
      "Epoch: 33 - Loss: 0.2790164936976398\n",
      "Epoch: 34 - Loss: 0.27821556757851623\n",
      "Epoch: 35 - Loss: 0.27743939366347137\n",
      "Epoch: 36 - Loss: 0.2766866855168168\n",
      "Epoch: 37 - Loss: 0.2759562473263924\n",
      "Epoch: 38 - Loss: 0.275246966015213\n",
      "Epoch: 39 - Loss: 0.274557804181459\n",
      "Epoch: 40 - Loss: 0.27388779376100986\n",
      "Epoch: 41 - Loss: 0.27323603032334365\n",
      "Epoch: 42 - Loss: 0.27260166792508167\n",
      "Epoch: 43 - Loss: 0.27198391445644604\n",
      "Epoch: 44 - Loss: 0.27138202742494694\n",
      "Epoch: 45 - Loss: 0.2707953101281359\n",
      "Epoch: 46 - Loss: 0.27022310817355405\n",
      "Epoch: 47 - Loss: 0.26966480630930795\n",
      "Epoch: 48 - Loss: 0.26911982553321206\n",
      "Epoch: 49 - Loss: 0.2685876204522763\n",
      "Epoch: 50 - Loss: 0.26806767686762584\n",
      "Epoch: 51 - Loss: 0.2675595095627833\n",
      "Epoch: 52 - Loss: 0.26706266027571773\n",
      "Epoch: 53 - Loss: 0.2665766958372156\n",
      "Epoch: 54 - Loss: 0.2661012064600069\n",
      "Epoch: 55 - Loss: 0.2656358041647289\n",
      "Epoch: 56 - Loss: 0.26518012133026087\n",
      "Epoch: 57 - Loss: 0.26473380935724017\n",
      "Epoch: 58 - Loss: 0.2642965374347017\n",
      "Epoch: 59 - Loss: 0.2638679914007878\n",
      "Epoch: 60 - Loss: 0.26344787268936126\n",
      "Epoch: 61 - Loss: 0.2630358973551529\n",
      "Epoch: 62 - Loss: 0.2626317951707785\n",
      "Epoch: 63 - Loss: 0.2622353087895921\n",
      "Epoch: 64 - Loss: 0.26184619296891004\n",
      "Epoch: 65 - Loss: 0.2614642138486442\n",
      "Epoch: 66 - Loss: 0.2610891482808397\n",
      "Epoch: 67 - Loss: 0.26072078320601927\n",
      "Epoch: 68 - Loss: 0.26035891507260683\n",
      "Epoch: 69 - Loss: 0.2600033492960302\n",
      "Epoch: 70 - Loss: 0.2596538997544065\n",
      "Epoch: 71 - Loss: 0.25931038831798\n",
      "Epoch: 72 - Loss: 0.2589726444097287\n",
      "Epoch: 73 - Loss: 0.2586405045947752\n",
      "Epoch: 74 - Loss: 0.25831381219643873\n",
      "Epoch: 75 - Loss: 0.2579924169369463\n",
      "Epoch: 76 - Loss: 0.25767617460098574\n",
      "Epoch: 77 - Loss: 0.2573649467204331\n",
      "Epoch: 78 - Loss: 0.25705860027872296\n",
      "Epoch: 79 - Loss: 0.25675700743345464\n",
      "Epoch: 80 - Loss: 0.25646004525593913\n",
      "Epoch: 81 - Loss: 0.2561675954864963\n",
      "Epoch: 82 - Loss: 0.25587954430440357\n",
      "Epoch: 83 - Loss: 0.25559578211148537\n",
      "Epoch: 84 - Loss: 0.25531620332840926\n",
      "Epoch: 85 - Loss: 0.25504070620282726\n",
      "Epoch: 86 - Loss: 0.25476919262856673\n",
      "Epoch: 87 - Loss: 0.2545015679751348\n",
      "Epoch: 88 - Loss: 0.25423774092685497\n",
      "Epoch: 89 - Loss: 0.2539776233310071\n",
      "Epoch: 90 - Loss: 0.2537211300543856\n",
      "Epoch: 91 - Loss: 0.25346817884773654\n",
      "Epoch: 92 - Loss: 0.2532186902175693\n",
      "Epoch: 93 - Loss: 0.25297258730488015\n",
      "Epoch: 94 - Loss: 0.25272979577035254\n",
      "Epoch: 95 - Loss: 0.25249024368563344\n",
      "Epoch: 96 - Loss: 0.25225386143031214\n",
      "Epoch: 97 - Loss: 0.25202058159425267\n",
      "Epoch: 98 - Loss: 0.2517903388849574\n",
      "Epoch: 99 - Loss: 0.2515630700396589\n",
      "Epoch: 100 - Loss: 0.25133871374186034\n",
      "Epoch: 101 - Loss: 0.25111721054206093\n",
      "Epoch: 102 - Loss: 0.25089850278242265\n",
      "Epoch: 103 - Loss: 0.2506825345251493\n",
      "Epoch: 104 - Loss: 0.25046925148436516\n",
      "Epoch: 105 - Loss: 0.2502586009612923\n",
      "Epoch: 106 - Loss: 0.2500505317825423\n",
      "Epoch: 107 - Loss: 0.24984499424134488\n",
      "Epoch: 108 - Loss: 0.2496419400415533\n",
      "Epoch: 109 - Loss: 0.24944132224427007\n",
      "Epoch: 110 - Loss: 0.2492430952169519\n",
      "Epoch: 111 - Loss: 0.24904721458485804\n",
      "Epoch: 112 - Loss: 0.2488536371847165\n",
      "Epoch: 113 - Loss: 0.24866232102048894\n",
      "Epoch: 114 - Loss: 0.24847322522112336\n",
      "Epoch: 115 - Loss: 0.24828631000018989\n",
      "Epoch: 116 - Loss: 0.24810153661730117\n",
      "Epoch: 117 - Loss: 0.24791886734122534\n",
      "Epoch: 118 - Loss: 0.24773826541460386\n",
      "Epoch: 119 - Loss: 0.24755969502019312\n",
      "Epoch: 120 - Loss: 0.2473831212485519\n",
      "Epoch: 121 - Loss: 0.24720851006710257\n",
      "Epoch: 122 - Loss: 0.2470358282904969\n",
      "Epoch: 123 - Loss: 0.2468650435522227\n",
      "Epoch: 124 - Loss: 0.24669612427738932\n",
      "Epoch: 125 - Loss: 0.2465290396566354\n",
      "Epoch: 126 - Loss: 0.24636375962110346\n",
      "Epoch: 127 - Loss: 0.24620025481843136\n",
      "Epoch: 128 - Loss: 0.24603849658971044\n",
      "Epoch: 129 - Loss: 0.24587845694736613\n",
      "Epoch: 130 - Loss: 0.24572010855391616\n",
      "Epoch: 131 - Loss: 0.24556342470156647\n",
      "Epoch: 132 - Loss: 0.2454083792926051\n",
      "Epoch: 133 - Loss: 0.24525494682055768\n",
      "Epoch: 134 - Loss: 0.24510310235206978\n",
      "Epoch: 135 - Loss: 0.24495282150948264\n",
      "Epoch: 136 - Loss: 0.24480408045407098\n",
      "Epoch: 137 - Loss: 0.2446568558699136\n",
      "Epoch: 138 - Loss: 0.24451112494836838\n",
      "Epoch: 139 - Loss: 0.24436686537312477\n",
      "Epoch: 140 - Loss: 0.24422405530580818\n",
      "Epoch: 141 - Loss: 0.2440826733721131\n",
      "Epoch: 142 - Loss: 0.24394269864844068\n",
      "Epoch: 143 - Loss: 0.24380411064902\n",
      "Epoch: 144 - Loss: 0.24366688931349234\n",
      "Epoch: 145 - Loss: 0.24353101499493737\n",
      "Epoch: 146 - Loss: 0.24339646844832497\n",
      "Epoch: 147 - Loss: 0.2432632308193723\n",
      "Epoch: 148 - Loss: 0.24313128363379088\n",
      "Epoch: 149 - Loss: 0.2430006087869068\n",
      "Epoch: 150 - Loss: 0.24287118853363907\n",
      "Epoch: 151 - Loss: 0.24274300547882066\n",
      "Epoch: 152 - Loss: 0.24261604256784938\n",
      "Epoch: 153 - Loss: 0.24249028307765422\n",
      "Epoch: 154 - Loss: 0.2423657106079655\n",
      "Epoch: 155 - Loss: 0.24224230907287514\n",
      "Epoch: 156 - Loss: 0.24212006269267797\n",
      "Epoch: 157 - Loss: 0.24199895598597984\n",
      "Epoch: 158 - Loss: 0.2418789737620651\n",
      "Epoch: 159 - Loss: 0.24176010111351118\n",
      "Epoch: 160 - Loss: 0.2416423234090418\n",
      "Epoch: 161 - Loss: 0.2415256262866093\n",
      "Epoch: 162 - Loss: 0.2414099956466971\n",
      "Epoch: 163 - Loss: 0.24129541764583418\n",
      "Epoch: 164 - Loss: 0.2411818786903138\n",
      "Epoch: 165 - Loss: 0.2410693654301078\n",
      "Epoch: 166 - Loss: 0.2409578647529705\n",
      "Epoch: 167 - Loss: 0.2408473637787242\n",
      "Epoch: 168 - Loss: 0.24073784985371946\n",
      "Epoch: 169 - Loss: 0.2406293105454648\n",
      "Epoch: 170 - Loss: 0.24052173363741836\n",
      "Epoch: 171 - Loss: 0.2404151071239364\n",
      "Epoch: 172 - Loss: 0.24030941920537283\n",
      "Epoch: 173 - Loss: 0.24020465828332419\n",
      "Epoch: 174 - Loss: 0.24010081295601507\n",
      "Epoch: 175 - Loss: 0.2399978720138188\n",
      "Epoch: 176 - Loss: 0.23989582443490853\n",
      "Epoch: 177 - Loss: 0.23979465938103495\n",
      "Epoch: 178 - Loss: 0.23969436619342477\n",
      "Epoch: 179 - Loss: 0.2395949343887971\n",
      "Epoch: 180 - Loss: 0.23949635365549296\n",
      "Epoch: 181 - Loss: 0.2393986138497143\n",
      "Epoch: 182 - Loss: 0.23930170499186865\n",
      "Epoch: 183 - Loss: 0.2392056172630157\n",
      "Epoch: 184 - Loss: 0.2391103410014127\n",
      "Epoch: 185 - Loss: 0.23901586669915523\n",
      "Epoch: 186 - Loss: 0.23892218499891005\n",
      "Epoch: 187 - Loss: 0.2388292866907367\n",
      "Epoch: 188 - Loss: 0.23873716270899595\n",
      "Epoch: 189 - Loss: 0.23864580412934058\n",
      "Epoch: 190 - Loss: 0.23855520216578788\n",
      "Epoch: 191 - Loss: 0.23846534816786893\n",
      "Epoch: 192 - Loss: 0.23837623361785423\n",
      "Epoch: 193 - Loss: 0.23828785012805168\n",
      "Epoch: 194 - Loss: 0.2382001894381758\n",
      "Epoch: 195 - Loss: 0.2381132434127848\n",
      "Epoch: 196 - Loss: 0.23802700403878432\n",
      "Epoch: 197 - Loss: 0.23794146342299508\n",
      "Epoch: 198 - Loss: 0.23785661378978284\n",
      "Epoch: 199 - Loss: 0.23777244747874846\n",
      "Epoch: 200 - Loss: 0.2376889569424764\n",
      "Epoch: 201 - Loss: 0.2376061347443396\n",
      "Epoch: 202 - Loss: 0.23752397355635935\n",
      "Epoch: 203 - Loss: 0.2374424661571181\n",
      "Epoch: 204 - Loss: 0.23736160542972418\n",
      "Epoch: 205 - Loss: 0.2372813843598259\n",
      "Epoch: 206 - Loss: 0.23720179603367456\n",
      "Epoch: 207 - Loss: 0.23712283363623432\n",
      "Epoch: 208 - Loss: 0.2370444904493378\n",
      "Epoch: 209 - Loss: 0.2369667598498857\n",
      "Epoch: 210 - Loss: 0.23688963530808937\n",
      "Epoch: 211 - Loss: 0.23681311038575573\n",
      "Epoch: 212 - Loss: 0.23673717873461167\n",
      "Epoch: 213 - Loss: 0.2366618340946686\n",
      "Epoch: 214 - Loss: 0.23658707029262463\n",
      "Epoch: 215 - Loss: 0.23651288124030423\n",
      "Epoch: 216 - Loss: 0.23643926093313358\n",
      "Epoch: 217 - Loss: 0.2363662034486512\n",
      "Epoch: 218 - Loss: 0.23629370294505228\n",
      "Epoch: 219 - Loss: 0.2362217536597666\n",
      "Epoch: 220 - Loss: 0.23615034990806785\n",
      "Epoch: 221 - Loss: 0.2360794860817147\n",
      "Epoch: 222 - Loss: 0.23600915664762215\n",
      "Epoch: 223 - Loss: 0.23593935614656217\n",
      "Epoch: 224 - Loss: 0.23587007919189354\n",
      "Epoch: 225 - Loss: 0.23580132046831914\n",
      "Epoch: 226 - Loss: 0.23573307473067048\n",
      "Epoch: 227 - Loss: 0.23566533680271906\n",
      "Epoch: 228 - Loss: 0.2355981015760132\n",
      "Epoch: 229 - Loss: 0.23553136400873956\n",
      "Epoch: 230 - Loss: 0.2354651191246098\n",
      "Epoch: 231 - Loss: 0.2353993620117704\n",
      "Epoch: 232 - Loss: 0.23533408782173562\n",
      "Epoch: 233 - Loss: 0.2352692917683433\n",
      "Epoch: 234 - Loss: 0.2352049691267322\n",
      "Epoch: 235 - Loss: 0.23514111523234094\n",
      "Epoch: 236 - Loss: 0.23507772547992747\n",
      "Epoch: 237 - Loss: 0.23501479532260916\n",
      "Epoch: 238 - Loss: 0.2349523202709222\n",
      "Epoch: 239 - Loss: 0.23489029589190058\n",
      "Epoch: 240 - Loss: 0.23482871780817363\n",
      "Epoch: 241 - Loss: 0.23476758169708198\n",
      "Epoch: 242 - Loss: 0.23470688328981093\n",
      "Epoch: 243 - Loss: 0.23464661837054201\n",
      "Epoch: 244 - Loss: 0.23458678277562042\n",
      "Epoch: 245 - Loss: 0.23452737239274002\n",
      "Epoch: 246 - Loss: 0.23446838316014343\n",
      "Epoch: 247 - Loss: 0.23440981106583877\n",
      "Epoch: 248 - Loss: 0.2343516521468311\n",
      "Epoch: 249 - Loss: 0.2342939024883692\n",
      "Epoch: 250 - Loss: 0.23423655822320652\n",
      "Epoch: 251 - Loss: 0.234179615530877\n",
      "Epoch: 252 - Loss: 0.23412307063698398\n",
      "Epoch: 253 - Loss: 0.23406691981250352\n",
      "Epoch: 254 - Loss: 0.2340111593731006\n",
      "Epoch: 255 - Loss: 0.2339557856784579\n",
      "Epoch: 256 - Loss: 0.23390079513161788\n",
      "Epoch: 257 - Loss: 0.23384618417833677\n",
      "Epoch: 258 - Loss: 0.23379194930645072\n",
      "Epoch: 259 - Loss: 0.23373808704525398\n",
      "Epoch: 260 - Loss: 0.2336845939648878\n",
      "Epoch: 261 - Loss: 0.23363146667574178\n",
      "Epoch: 262 - Loss: 0.233578701827865\n",
      "Epoch: 263 - Loss: 0.23352629611038866\n",
      "Epoch: 264 - Loss: 0.233474246250959\n",
      "Epoch: 265 - Loss: 0.23342254901518014\n",
      "Epoch: 266 - Loss: 0.23337120120606783\n",
      "Epoch: 267 - Loss: 0.23332019966351172\n",
      "Epoch: 268 - Loss: 0.23326954126374844\n",
      "Epoch: 269 - Loss: 0.23321922291884323\n",
      "Epoch: 270 - Loss: 0.23316924157618116\n",
      "Epoch: 271 - Loss: 0.23311959421796713\n",
      "Epoch: 272 - Loss: 0.23307027786073473\n",
      "Epoch: 273 - Loss: 0.23302128955486356\n",
      "Epoch: 274 - Loss: 0.23297262638410499\n",
      "Epoch: 275 - Loss: 0.23292428546511648\n",
      "Epoch: 276 - Loss: 0.23287626394700311\n",
      "Epoch: 277 - Loss: 0.23282855901086794\n",
      "Epoch: 278 - Loss: 0.2327811678693692\n",
      "Epoch: 279 - Loss: 0.23273408776628599\n",
      "Epoch: 280 - Loss: 0.23268731597609016\n",
      "Epoch: 281 - Loss: 0.2326408498035265\n",
      "Epoch: 282 - Loss: 0.2325946865831994\n",
      "Epoch: 283 - Loss: 0.23254882367916663\n",
      "Epoch: 284 - Loss: 0.23250325848453976\n",
      "Epoch: 285 - Loss: 0.23245798842109142\n",
      "Epoch: 286 - Loss: 0.2324130109388689\n",
      "Epoch: 287 - Loss: 0.23236832351581435\n",
      "Epoch: 288 - Loss: 0.23232392365739105\n",
      "Epoch: 289 - Loss: 0.23227980889621547\n",
      "Epoch: 290 - Loss: 0.23223597679169625\n",
      "Epoch: 291 - Loss: 0.23219242492967798\n",
      "Epoch: 292 - Loss: 0.23214915092209157\n",
      "Epoch: 293 - Loss: 0.2321061524066097\n",
      "Epoch: 294 - Loss: 0.23206342704630842\n",
      "Epoch: 295 - Loss: 0.23202097252933318\n",
      "Epoch: 296 - Loss: 0.2319787865685714\n",
      "Epoch: 297 - Loss: 0.2319368669013294\n",
      "Epoch: 298 - Loss: 0.23189521128901444\n",
      "Epoch: 299 - Loss: 0.23185381751682244\n",
      "Epoch: 300 - Loss: 0.23181268339342997\n",
      "Epoch: 301 - Loss: 0.23177180675069145\n",
      "Epoch: 302 - Loss: 0.2317311854433405\n",
      "Epoch: 303 - Loss: 0.23169081734869698\n",
      "Epoch: 304 - Loss: 0.23165070036637725\n",
      "Epoch: 305 - Loss: 0.23161083241801048\n",
      "Epoch: 306 - Loss: 0.23157121144695733\n",
      "Epoch: 307 - Loss: 0.23153183541803504\n",
      "Epoch: 308 - Loss: 0.23149270231724536\n",
      "Epoch: 309 - Loss: 0.23145381015150682\n",
      "Epoch: 310 - Loss: 0.23141515694839168\n",
      "Epoch: 311 - Loss: 0.23137674075586614\n",
      "Epoch: 312 - Loss: 0.23133855964203498\n",
      "Epoch: 313 - Loss: 0.23130061169488955\n",
      "Epoch: 314 - Loss: 0.23126289502206024\n",
      "Epoch: 315 - Loss: 0.2312254077505718\n",
      "Epoch: 316 - Loss: 0.23118814802660315\n",
      "Epoch: 317 - Loss: 0.23115111401524993\n",
      "Epoch: 318 - Loss: 0.23111430390029142\n",
      "Epoch: 319 - Loss: 0.23107771588396023\n",
      "Epoch: 320 - Loss: 0.23104134818671554\n",
      "Epoch: 321 - Loss: 0.23100519904701988\n",
      "Epoch: 322 - Loss: 0.2309692667211191\n",
      "Epoch: 323 - Loss: 0.23093354948282518\n",
      "Epoch: 324 - Loss: 0.23089804562330282\n",
      "Epoch: 325 - Loss: 0.2308627534508588\n",
      "Epoch: 326 - Loss: 0.23082767129073392\n",
      "Epoch: 327 - Loss: 0.23079279748489914\n",
      "Epoch: 328 - Loss: 0.23075813039185342\n",
      "Epoch: 329 - Loss: 0.2307236683864248\n",
      "Epoch: 330 - Loss: 0.23068940985957517\n",
      "Epoch: 331 - Loss: 0.23065535321820615\n",
      "Epoch: 332 - Loss: 0.23062149688496966\n",
      "Epoch: 333 - Loss: 0.23058783929807958\n",
      "Epoch: 334 - Loss: 0.23055437891112687\n",
      "Epoch: 335 - Loss: 0.2305211141928972\n",
      "Epoch: 336 - Loss: 0.23048804362719102\n",
      "Epoch: 337 - Loss: 0.2304551657126461\n",
      "Epoch: 338 - Loss: 0.23042247896256274\n",
      "Epoch: 339 - Loss: 0.23038998190473137\n",
      "Epoch: 340 - Loss: 0.2303576730812623\n",
      "Epoch: 341 - Loss: 0.23032555104841826\n",
      "Epoch: 342 - Loss: 0.2302936143764488\n",
      "Epoch: 343 - Loss: 0.23026186164942727\n",
      "Epoch: 344 - Loss: 0.23023029146509014\n",
      "Epoch: 345 - Loss: 0.230198902434678\n",
      "Epoch: 346 - Loss: 0.23016769318277933\n",
      "Epoch: 347 - Loss: 0.23013666234717603\n",
      "Epoch: 348 - Loss: 0.2301058085786912\n",
      "Epoch: 349 - Loss: 0.23007513054103904\n",
      "Epoch: 350 - Loss: 0.2300446269106766\n",
      "Epoch: 351 - Loss: 0.2300142963766576\n",
      "Epoch: 352 - Loss: 0.2299841376404884\n",
      "Epoch: 353 - Loss: 0.22995414941598544\n",
      "Epoch: 354 - Loss: 0.2299243304291355\n",
      "Epoch: 355 - Loss: 0.22989467941795652\n",
      "Epoch: 356 - Loss: 0.22986519513236156\n",
      "Epoch: 357 - Loss: 0.22983587633402372\n",
      "Epoch: 358 - Loss: 0.22980672179624317\n",
      "Epoch: 359 - Loss: 0.2297777303038159\n",
      "Epoch: 360 - Loss: 0.22974890065290446\n",
      "Epoch: 361 - Loss: 0.22972023165090985\n",
      "Epoch: 362 - Loss: 0.22969172211634534\n",
      "Epoch: 363 - Loss: 0.22966337087871244\n",
      "Epoch: 364 - Loss: 0.22963517677837766\n",
      "Epoch: 365 - Loss: 0.2296071386664517\n",
      "Epoch: 366 - Loss: 0.22957925540466909\n",
      "Epoch: 367 - Loss: 0.22955152586527106\n",
      "Epoch: 368 - Loss: 0.2295239489308881\n",
      "Epoch: 369 - Loss: 0.2294965234944255\n",
      "Epoch: 370 - Loss: 0.2294692484589496\n",
      "Epoch: 371 - Loss: 0.22944212273757558\n",
      "Epoch: 372 - Loss: 0.22941514525335707\n",
      "Epoch: 373 - Loss: 0.22938831493917658\n",
      "Epoch: 374 - Loss: 0.22936163073763804\n",
      "Epoch: 375 - Loss: 0.2293350916009602\n",
      "Epoch: 376 - Loss: 0.22930869649087118\n",
      "Epoch: 377 - Loss: 0.22928244437850553\n",
      "Epoch: 378 - Loss: 0.22925633424430095\n",
      "Epoch: 379 - Loss: 0.22923036507789765\n",
      "Epoch: 380 - Loss: 0.22920453587803843\n",
      "Epoch: 381 - Loss: 0.22917884565246985\n",
      "Epoch: 382 - Loss: 0.22915329341784516\n",
      "Epoch: 383 - Loss: 0.229127878199628\n",
      "Epoch: 384 - Loss: 0.22910259903199756\n",
      "Epoch: 385 - Loss: 0.22907745495775486\n",
      "Epoch: 386 - Loss: 0.22905244502823008\n",
      "Epoch: 387 - Loss: 0.22902756830319135\n",
      "Epoch: 388 - Loss: 0.22900282385075413\n",
      "Epoch: 389 - Loss: 0.22897821074729272\n",
      "Epoch: 390 - Loss: 0.2289537280773513\n",
      "Epoch: 391 - Loss: 0.22892937493355792\n",
      "Epoch: 392 - Loss: 0.2289051504165379\n",
      "Epoch: 393 - Loss: 0.22888105363482947\n",
      "Epoch: 394 - Loss: 0.22885708370479985\n",
      "Epoch: 395 - Loss: 0.2288332397505625\n",
      "Epoch: 396 - Loss: 0.2288095209038953\n",
      "Epoch: 397 - Loss: 0.22878592630416014\n",
      "Epoch: 398 - Loss: 0.22876245509822268\n",
      "Epoch: 399 - Loss: 0.22873910644037423\n",
      "Epoch: 400 - Loss: 0.22871587949225333\n",
      "Epoch: 401 - Loss: 0.22869277342276928\n",
      "Epoch: 402 - Loss: 0.22866978740802604\n",
      "Epoch: 403 - Loss: 0.22864692063124747\n",
      "Epoch: 404 - Loss: 0.2286241722827027\n",
      "Epoch: 405 - Loss: 0.22860154155963366\n",
      "Epoch: 406 - Loss: 0.22857902766618202\n",
      "Epoch: 407 - Loss: 0.22855662981331828\n",
      "Epoch: 408 - Loss: 0.22853434721877094\n",
      "Epoch: 409 - Loss: 0.2285121791069568\n",
      "Epoch: 410 - Loss: 0.22849012470891195\n",
      "Epoch: 411 - Loss: 0.22846818326222396\n",
      "Epoch: 412 - Loss: 0.228446354010964\n",
      "Epoch: 413 - Loss: 0.22842463620562134\n",
      "Epoch: 414 - Loss: 0.22840302910303656\n",
      "Epoch: 415 - Loss: 0.2283815319663375\n",
      "Epoch: 416 - Loss: 0.2283601440648746\n",
      "Epoch: 417 - Loss: 0.22833886467415784\n",
      "Epoch: 418 - Loss: 0.2283176930757939\n",
      "Epoch: 419 - Loss: 0.22829662855742425\n",
      "Epoch: 420 - Loss: 0.22827567041266414\n",
      "Epoch: 421 - Loss: 0.2282548179410422\n",
      "Epoch: 422 - Loss: 0.22823407044794028\n",
      "Epoch: 423 - Loss: 0.228213427244535\n",
      "Epoch: 424 - Loss: 0.22819288764773885\n",
      "Epoch: 425 - Loss: 0.2281724509801429\n",
      "Epoch: 426 - Loss: 0.22815211656995968\n",
      "Epoch: 427 - Loss: 0.22813188375096669\n",
      "Epoch: 428 - Loss: 0.228111751862451\n",
      "Epoch: 429 - Loss: 0.228091720249154\n",
      "Epoch: 430 - Loss: 0.22807178826121702\n",
      "Epoch: 431 - Loss: 0.22805195525412778\n",
      "Epoch: 432 - Loss: 0.2280322205886668\n",
      "Epoch: 433 - Loss: 0.22801258363085533\n",
      "Epoch: 434 - Loss: 0.22799304375190318\n",
      "Epoch: 435 - Loss: 0.22797360032815728\n",
      "Epoch: 436 - Loss: 0.2279542527410512\n",
      "Epoch: 437 - Loss: 0.2279350003770548\n",
      "Epoch: 438 - Loss: 0.22791584262762465\n",
      "Epoch: 439 - Loss: 0.22789677888915497\n",
      "Epoch: 440 - Loss: 0.22787780856292938\n",
      "Epoch: 441 - Loss: 0.2278589310550728\n",
      "Epoch: 442 - Loss: 0.22784014577650372\n",
      "Epoch: 443 - Loss: 0.2278214521428882\n",
      "Epoch: 444 - Loss: 0.22780284957459296\n",
      "Epoch: 445 - Loss: 0.2277843374966395\n",
      "Epoch: 446 - Loss: 0.2277659153386593\n",
      "Epoch: 447 - Loss: 0.2277475825348488\n",
      "Epoch: 448 - Loss: 0.2277293385239251\n",
      "Epoch: 449 - Loss: 0.22771118274908228\n",
      "Epoch: 450 - Loss: 0.2276931146579481\n",
      "Epoch: 451 - Loss: 0.22767513370254142\n",
      "Epoch: 452 - Loss: 0.22765723933922954\n",
      "Epoch: 453 - Loss: 0.22763943102868653\n",
      "Epoch: 454 - Loss: 0.22762170823585215\n",
      "Epoch: 455 - Loss: 0.22760407042989042\n",
      "Epoch: 456 - Loss: 0.22758651708414965\n",
      "Epoch: 457 - Loss: 0.2275690476761221\n",
      "Epoch: 458 - Loss: 0.2275516616874047\n",
      "Epoch: 459 - Loss: 0.22753435860365967\n",
      "Epoch: 460 - Loss: 0.22751713791457606\n",
      "Epoch: 461 - Loss: 0.22749999911383142\n",
      "Epoch: 462 - Loss: 0.22748294169905384\n",
      "Epoch: 463 - Loss: 0.22746596517178486\n",
      "Epoch: 464 - Loss: 0.22744906903744191\n",
      "Epoch: 465 - Loss: 0.22743225280528243\n",
      "Epoch: 466 - Loss: 0.22741551598836698\n",
      "Epoch: 467 - Loss: 0.22739885810352392\n",
      "Epoch: 468 - Loss: 0.22738227867131391\n",
      "Epoch: 469 - Loss: 0.2273657772159946\n",
      "Epoch: 470 - Loss: 0.22734935326548622\n",
      "Epoch: 471 - Loss: 0.22733300635133738\n",
      "Epoch: 472 - Loss: 0.2273167360086911\n",
      "Epoch: 473 - Loss: 0.22730054177625122\n",
      "Epoch: 474 - Loss: 0.22728442319624922\n",
      "Epoch: 475 - Loss: 0.22726837981441164\n",
      "Epoch: 476 - Loss: 0.22725241117992734\n",
      "Epoch: 477 - Loss: 0.22723651684541565\n",
      "Epoch: 478 - Loss: 0.22722069636689443\n",
      "Epoch: 479 - Loss: 0.2272049493037487\n",
      "Epoch: 480 - Loss: 0.22718927521869983\n",
      "Epoch: 481 - Loss: 0.2271736736777743\n",
      "Epoch: 482 - Loss: 0.22715814425027372\n",
      "Epoch: 483 - Loss: 0.2271426865087446\n",
      "Epoch: 484 - Loss: 0.22712730002894863\n",
      "Epoch: 485 - Loss: 0.22711198438983296\n",
      "Epoch: 486 - Loss: 0.22709673917350157\n",
      "Epoch: 487 - Loss: 0.227081563965186\n",
      "Epoch: 488 - Loss: 0.22706645835321707\n",
      "Epoch: 489 - Loss: 0.22705142192899666\n",
      "Epoch: 490 - Loss: 0.2270364542869697\n",
      "Epoch: 491 - Loss: 0.22702155502459667\n",
      "Epoch: 492 - Loss: 0.2270067237423261\n",
      "Epoch: 493 - Loss: 0.22699196004356773\n",
      "Epoch: 494 - Loss: 0.22697726353466563\n",
      "Epoch: 495 - Loss: 0.22696263382487178\n",
      "Epoch: 496 - Loss: 0.22694807052631985\n",
      "Epoch: 497 - Loss: 0.22693357325399943\n",
      "Epoch: 498 - Loss: 0.22691914162573001\n",
      "Epoch: 499 - Loss: 0.22690477526213618\n",
      "Epoch: 500 - Loss: 0.22689047378662186\n"
     ]
    }
   ],
   "source": [
    "# questions\n",
    "# train_file = \"work/datasets/questions/train.txt\"\n",
    "# pred_file = \"work/datasets/questions/val.test\"\n",
    "# pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "# model_file_name = \"logreg.questions.model\"\n",
    "# # model_LR = LogisticRegression(model_file_name, learning_rate=0.1, epochs=1000, threshold=0, max_features=10)\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.15, epochs=500, threshold=0, max_features=150)\n",
    "# X, Y, prob  = model_LR.train(train_file, batch_size=32, verbose=True)\n",
    "\n",
    "# odiya\n",
    "# train_file = \"work/datasets/odiya/train.txt\"\n",
    "# pred_file = \"work/datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "# model_file_name = \"logreg.odiya.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.01, epochs=1000, threshold=10, max_features=1000)\n",
    "# X, Y, prob  = model_LR.train(train_file, batch_size=256, verbose=True)\n",
    "\n",
    "\n",
    "# 4dim\n",
    "train_file = \"work/datasets/4dim/train.txt\"\n",
    "pred_file = \"work/datasets/4dim/val.test\"\n",
    "pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "model_file_name = \"word2vec_log.4dim.model\"\n",
    "model_LR = LogisticRegressionWord2Vec(model_file_name, learning_rate=0.9, epochs=500, batch_size=128, embedding_matrix=model_embeddings)\n",
    "X, Y, prob  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "\n",
    "# #Products\n",
    "# train_file = \"work/datasets/products/train.txt\"\n",
    "# pred_file = \"work/datasets/products/val.test\"\n",
    "# pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "# model_file_name = \"word2vec_log.products.model\"\n",
    "# # model_LR = LogisticRegression(model_file_name, learning_rate=0.9, epochs=1000, threshold=2, max_features=500)\n",
    "# #80% of the dataset\n",
    "# model_LR = LogisticRegressionWord2Vec(model_file_name, learning_rate=0.9, epochs=100, batch_size=16, embedding_matrix=model_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.3455683356205985\n",
      "Epoch: 2 - Loss: 0.3388484446076833\n",
      "Epoch: 3 - Loss: 0.3333682782564171\n",
      "Epoch: 4 - Loss: 0.3287315025056436\n",
      "Epoch: 5 - Loss: 0.32471381844996877\n",
      "Epoch: 6 - Loss: 0.3211664071541381\n",
      "Epoch: 7 - Loss: 0.3179870322245108\n",
      "Epoch: 8 - Loss: 0.31510333073549973\n",
      "Epoch: 9 - Loss: 0.31246266812888995\n",
      "Epoch: 10 - Loss: 0.3100257911866595\n",
      "Epoch: 11 - Loss: 0.30776274499194595\n",
      "Epoch: 12 - Loss: 0.305650178889223\n",
      "Epoch: 13 - Loss: 0.3036695282891939\n",
      "Epoch: 14 - Loss: 0.301805762717811\n",
      "Epoch: 15 - Loss: 0.30004650815682354\n",
      "Epoch: 16 - Loss: 0.29838142164776377\n",
      "Epoch: 17 - Loss: 0.2968017388352722\n",
      "Epoch: 18 - Loss: 0.29529994187029457\n",
      "Epoch: 19 - Loss: 0.29386951222571733\n",
      "Epoch: 20 - Loss: 0.29250474417142036\n",
      "Epoch: 21 - Loss: 0.29120060209778403\n",
      "Epoch: 22 - Loss: 0.2899526098988806\n",
      "Epoch: 23 - Loss: 0.2887567640602617\n",
      "Epoch: 24 - Loss: 0.28760946447092123\n",
      "Epoch: 25 - Loss: 0.28650745863812266\n",
      "Epoch: 26 - Loss: 0.28544779615368454\n",
      "Epoch: 27 - Loss: 0.28442779109232535\n",
      "Epoch: 28 - Loss: 0.28344499061913614\n",
      "Epoch: 29 - Loss: 0.28249714851418156\n",
      "Epoch: 30 - Loss: 0.2815822026359469\n",
      "Epoch: 31 - Loss: 0.28069825557549016\n",
      "Epoch: 32 - Loss: 0.2798435579232972\n",
      "Epoch: 33 - Loss: 0.2790164936976398\n",
      "Epoch: 34 - Loss: 0.27821556757851623\n",
      "Epoch: 35 - Loss: 0.27743939366347137\n",
      "Epoch: 36 - Loss: 0.2766866855168168\n",
      "Epoch: 37 - Loss: 0.2759562473263924\n",
      "Epoch: 38 - Loss: 0.275246966015213\n",
      "Epoch: 39 - Loss: 0.274557804181459\n",
      "Epoch: 40 - Loss: 0.27388779376100986\n",
      "Epoch: 41 - Loss: 0.27323603032334365\n",
      "Epoch: 42 - Loss: 0.27260166792508167\n",
      "Epoch: 43 - Loss: 0.27198391445644604\n",
      "Epoch: 44 - Loss: 0.27138202742494694\n",
      "Epoch: 45 - Loss: 0.2707953101281359\n",
      "Epoch: 46 - Loss: 0.27022310817355405\n",
      "Epoch: 47 - Loss: 0.26966480630930795\n",
      "Epoch: 48 - Loss: 0.26911982553321206\n",
      "Epoch: 49 - Loss: 0.2685876204522763\n",
      "Epoch: 50 - Loss: 0.26806767686762584\n",
      "Epoch: 51 - Loss: 0.2675595095627833\n",
      "Epoch: 52 - Loss: 0.26706266027571773\n",
      "Epoch: 53 - Loss: 0.2665766958372156\n",
      "Epoch: 54 - Loss: 0.2661012064600069\n",
      "Epoch: 55 - Loss: 0.2656358041647289\n",
      "Epoch: 56 - Loss: 0.26518012133026087\n",
      "Epoch: 57 - Loss: 0.26473380935724017\n",
      "Epoch: 58 - Loss: 0.2642965374347017\n",
      "Epoch: 59 - Loss: 0.2638679914007878\n",
      "Epoch: 60 - Loss: 0.26344787268936126\n",
      "Epoch: 61 - Loss: 0.2630358973551529\n",
      "Epoch: 62 - Loss: 0.2626317951707785\n",
      "Epoch: 63 - Loss: 0.2622353087895921\n",
      "Epoch: 64 - Loss: 0.26184619296891004\n",
      "Epoch: 65 - Loss: 0.2614642138486442\n",
      "Epoch: 66 - Loss: 0.2610891482808397\n",
      "Epoch: 67 - Loss: 0.26072078320601927\n",
      "Epoch: 68 - Loss: 0.26035891507260683\n",
      "Epoch: 69 - Loss: 0.2600033492960302\n",
      "Epoch: 70 - Loss: 0.2596538997544065\n",
      "Epoch: 71 - Loss: 0.25931038831798\n",
      "Epoch: 72 - Loss: 0.2589726444097287\n",
      "Epoch: 73 - Loss: 0.2586405045947752\n",
      "Epoch: 74 - Loss: 0.25831381219643873\n",
      "Epoch: 75 - Loss: 0.2579924169369463\n",
      "Epoch: 76 - Loss: 0.25767617460098574\n",
      "Epoch: 77 - Loss: 0.2573649467204331\n",
      "Epoch: 78 - Loss: 0.25705860027872296\n",
      "Epoch: 79 - Loss: 0.25675700743345464\n",
      "Epoch: 80 - Loss: 0.25646004525593913\n",
      "Epoch: 81 - Loss: 0.2561675954864963\n",
      "Epoch: 82 - Loss: 0.25587954430440357\n",
      "Epoch: 83 - Loss: 0.25559578211148537\n",
      "Epoch: 84 - Loss: 0.25531620332840926\n",
      "Epoch: 85 - Loss: 0.25504070620282726\n",
      "Epoch: 86 - Loss: 0.25476919262856673\n",
      "Epoch: 87 - Loss: 0.2545015679751348\n",
      "Epoch: 88 - Loss: 0.25423774092685497\n",
      "Epoch: 89 - Loss: 0.2539776233310071\n",
      "Epoch: 90 - Loss: 0.2537211300543856\n",
      "Epoch: 91 - Loss: 0.25346817884773654\n",
      "Epoch: 92 - Loss: 0.2532186902175693\n",
      "Epoch: 93 - Loss: 0.25297258730488015\n",
      "Epoch: 94 - Loss: 0.25272979577035254\n",
      "Epoch: 95 - Loss: 0.25249024368563344\n",
      "Epoch: 96 - Loss: 0.25225386143031214\n",
      "Epoch: 97 - Loss: 0.25202058159425267\n",
      "Epoch: 98 - Loss: 0.2517903388849574\n",
      "Epoch: 99 - Loss: 0.2515630700396589\n",
      "Epoch: 100 - Loss: 0.25133871374186034\n",
      "Epoch: 101 - Loss: 0.25111721054206093\n",
      "Epoch: 102 - Loss: 0.25089850278242265\n",
      "Epoch: 103 - Loss: 0.2506825345251493\n",
      "Epoch: 104 - Loss: 0.25046925148436516\n",
      "Epoch: 105 - Loss: 0.2502586009612923\n",
      "Epoch: 106 - Loss: 0.2500505317825423\n",
      "Epoch: 107 - Loss: 0.24984499424134488\n",
      "Epoch: 108 - Loss: 0.2496419400415533\n",
      "Epoch: 109 - Loss: 0.24944132224427007\n",
      "Epoch: 110 - Loss: 0.2492430952169519\n",
      "Epoch: 111 - Loss: 0.24904721458485804\n",
      "Epoch: 112 - Loss: 0.2488536371847165\n",
      "Epoch: 113 - Loss: 0.24866232102048894\n",
      "Epoch: 114 - Loss: 0.24847322522112336\n",
      "Epoch: 115 - Loss: 0.24828631000018989\n",
      "Epoch: 116 - Loss: 0.24810153661730117\n",
      "Epoch: 117 - Loss: 0.24791886734122534\n",
      "Epoch: 118 - Loss: 0.24773826541460386\n",
      "Epoch: 119 - Loss: 0.24755969502019312\n",
      "Epoch: 120 - Loss: 0.2473831212485519\n",
      "Epoch: 121 - Loss: 0.24720851006710257\n",
      "Epoch: 122 - Loss: 0.2470358282904969\n",
      "Epoch: 123 - Loss: 0.2468650435522227\n",
      "Epoch: 124 - Loss: 0.24669612427738932\n",
      "Epoch: 125 - Loss: 0.2465290396566354\n",
      "Epoch: 126 - Loss: 0.24636375962110346\n",
      "Epoch: 127 - Loss: 0.24620025481843136\n",
      "Epoch: 128 - Loss: 0.24603849658971044\n",
      "Epoch: 129 - Loss: 0.24587845694736613\n",
      "Epoch: 130 - Loss: 0.24572010855391616\n",
      "Epoch: 131 - Loss: 0.24556342470156647\n",
      "Epoch: 132 - Loss: 0.2454083792926051\n",
      "Epoch: 133 - Loss: 0.24525494682055768\n",
      "Epoch: 134 - Loss: 0.24510310235206978\n",
      "Epoch: 135 - Loss: 0.24495282150948264\n",
      "Epoch: 136 - Loss: 0.24480408045407098\n",
      "Epoch: 137 - Loss: 0.2446568558699136\n",
      "Epoch: 138 - Loss: 0.24451112494836838\n",
      "Epoch: 139 - Loss: 0.24436686537312477\n",
      "Epoch: 140 - Loss: 0.24422405530580818\n",
      "Epoch: 141 - Loss: 0.2440826733721131\n",
      "Epoch: 142 - Loss: 0.24394269864844068\n",
      "Epoch: 143 - Loss: 0.24380411064902\n",
      "Epoch: 144 - Loss: 0.24366688931349234\n",
      "Epoch: 145 - Loss: 0.24353101499493737\n",
      "Epoch: 146 - Loss: 0.24339646844832497\n",
      "Epoch: 147 - Loss: 0.2432632308193723\n",
      "Epoch: 148 - Loss: 0.24313128363379088\n",
      "Epoch: 149 - Loss: 0.2430006087869068\n",
      "Epoch: 150 - Loss: 0.24287118853363907\n",
      "Epoch: 151 - Loss: 0.24274300547882066\n",
      "Epoch: 152 - Loss: 0.24261604256784938\n",
      "Epoch: 153 - Loss: 0.24249028307765422\n",
      "Epoch: 154 - Loss: 0.2423657106079655\n",
      "Epoch: 155 - Loss: 0.24224230907287514\n",
      "Epoch: 156 - Loss: 0.24212006269267797\n",
      "Epoch: 157 - Loss: 0.24199895598597984\n",
      "Epoch: 158 - Loss: 0.2418789737620651\n",
      "Epoch: 159 - Loss: 0.24176010111351118\n",
      "Epoch: 160 - Loss: 0.2416423234090418\n",
      "Epoch: 161 - Loss: 0.2415256262866093\n",
      "Epoch: 162 - Loss: 0.2414099956466971\n",
      "Epoch: 163 - Loss: 0.24129541764583418\n",
      "Epoch: 164 - Loss: 0.2411818786903138\n",
      "Epoch: 165 - Loss: 0.2410693654301078\n",
      "Epoch: 166 - Loss: 0.2409578647529705\n",
      "Epoch: 167 - Loss: 0.2408473637787242\n",
      "Epoch: 168 - Loss: 0.24073784985371946\n",
      "Epoch: 169 - Loss: 0.2406293105454648\n",
      "Epoch: 170 - Loss: 0.24052173363741836\n",
      "Epoch: 171 - Loss: 0.2404151071239364\n",
      "Epoch: 172 - Loss: 0.24030941920537283\n",
      "Epoch: 173 - Loss: 0.24020465828332419\n",
      "Epoch: 174 - Loss: 0.24010081295601507\n",
      "Epoch: 175 - Loss: 0.2399978720138188\n",
      "Epoch: 176 - Loss: 0.23989582443490853\n",
      "Epoch: 177 - Loss: 0.23979465938103495\n",
      "Epoch: 178 - Loss: 0.23969436619342477\n",
      "Epoch: 179 - Loss: 0.2395949343887971\n",
      "Epoch: 180 - Loss: 0.23949635365549296\n",
      "Epoch: 181 - Loss: 0.2393986138497143\n",
      "Epoch: 182 - Loss: 0.23930170499186865\n",
      "Epoch: 183 - Loss: 0.2392056172630157\n",
      "Epoch: 184 - Loss: 0.2391103410014127\n",
      "Epoch: 185 - Loss: 0.23901586669915523\n",
      "Epoch: 186 - Loss: 0.23892218499891005\n",
      "Epoch: 187 - Loss: 0.2388292866907367\n",
      "Epoch: 188 - Loss: 0.23873716270899595\n",
      "Epoch: 189 - Loss: 0.23864580412934058\n",
      "Epoch: 190 - Loss: 0.23855520216578788\n",
      "Epoch: 191 - Loss: 0.23846534816786893\n",
      "Epoch: 192 - Loss: 0.23837623361785423\n",
      "Epoch: 193 - Loss: 0.23828785012805168\n",
      "Epoch: 194 - Loss: 0.2382001894381758\n",
      "Epoch: 195 - Loss: 0.2381132434127848\n",
      "Epoch: 196 - Loss: 0.23802700403878432\n",
      "Epoch: 197 - Loss: 0.23794146342299508\n",
      "Epoch: 198 - Loss: 0.23785661378978284\n",
      "Epoch: 199 - Loss: 0.23777244747874846\n",
      "Epoch: 200 - Loss: 0.2376889569424764\n",
      "Epoch: 201 - Loss: 0.2376061347443396\n",
      "Epoch: 202 - Loss: 0.23752397355635935\n",
      "Epoch: 203 - Loss: 0.2374424661571181\n",
      "Epoch: 204 - Loss: 0.23736160542972418\n",
      "Epoch: 205 - Loss: 0.2372813843598259\n",
      "Epoch: 206 - Loss: 0.23720179603367456\n",
      "Epoch: 207 - Loss: 0.23712283363623432\n",
      "Epoch: 208 - Loss: 0.2370444904493378\n",
      "Epoch: 209 - Loss: 0.2369667598498857\n",
      "Epoch: 210 - Loss: 0.23688963530808937\n",
      "Epoch: 211 - Loss: 0.23681311038575573\n",
      "Epoch: 212 - Loss: 0.23673717873461167\n",
      "Epoch: 213 - Loss: 0.2366618340946686\n",
      "Epoch: 214 - Loss: 0.23658707029262463\n",
      "Epoch: 215 - Loss: 0.23651288124030423\n",
      "Epoch: 216 - Loss: 0.23643926093313358\n",
      "Epoch: 217 - Loss: 0.2363662034486512\n",
      "Epoch: 218 - Loss: 0.23629370294505228\n",
      "Epoch: 219 - Loss: 0.2362217536597666\n",
      "Epoch: 220 - Loss: 0.23615034990806785\n",
      "Epoch: 221 - Loss: 0.2360794860817147\n",
      "Epoch: 222 - Loss: 0.23600915664762215\n",
      "Epoch: 223 - Loss: 0.23593935614656217\n",
      "Epoch: 224 - Loss: 0.23587007919189354\n",
      "Epoch: 225 - Loss: 0.23580132046831914\n",
      "Epoch: 226 - Loss: 0.23573307473067048\n",
      "Epoch: 227 - Loss: 0.23566533680271906\n",
      "Epoch: 228 - Loss: 0.2355981015760132\n",
      "Epoch: 229 - Loss: 0.23553136400873956\n",
      "Epoch: 230 - Loss: 0.2354651191246098\n",
      "Epoch: 231 - Loss: 0.2353993620117704\n",
      "Epoch: 232 - Loss: 0.23533408782173562\n",
      "Epoch: 233 - Loss: 0.2352692917683433\n",
      "Epoch: 234 - Loss: 0.2352049691267322\n",
      "Epoch: 235 - Loss: 0.23514111523234094\n",
      "Epoch: 236 - Loss: 0.23507772547992747\n",
      "Epoch: 237 - Loss: 0.23501479532260916\n",
      "Epoch: 238 - Loss: 0.2349523202709222\n",
      "Epoch: 239 - Loss: 0.23489029589190058\n",
      "Epoch: 240 - Loss: 0.23482871780817363\n",
      "Epoch: 241 - Loss: 0.23476758169708198\n",
      "Epoch: 242 - Loss: 0.23470688328981093\n",
      "Epoch: 243 - Loss: 0.23464661837054201\n",
      "Epoch: 244 - Loss: 0.23458678277562042\n",
      "Epoch: 245 - Loss: 0.23452737239274002\n",
      "Epoch: 246 - Loss: 0.23446838316014343\n",
      "Epoch: 247 - Loss: 0.23440981106583877\n",
      "Epoch: 248 - Loss: 0.2343516521468311\n",
      "Epoch: 249 - Loss: 0.2342939024883692\n",
      "Epoch: 250 - Loss: 0.23423655822320652\n",
      "Epoch: 251 - Loss: 0.234179615530877\n",
      "Epoch: 252 - Loss: 0.23412307063698398\n",
      "Epoch: 253 - Loss: 0.23406691981250352\n",
      "Epoch: 254 - Loss: 0.2340111593731006\n",
      "Epoch: 255 - Loss: 0.2339557856784579\n",
      "Epoch: 256 - Loss: 0.23390079513161788\n",
      "Epoch: 257 - Loss: 0.23384618417833677\n",
      "Epoch: 258 - Loss: 0.23379194930645072\n",
      "Epoch: 259 - Loss: 0.23373808704525398\n",
      "Epoch: 260 - Loss: 0.2336845939648878\n",
      "Epoch: 261 - Loss: 0.23363146667574178\n",
      "Epoch: 262 - Loss: 0.233578701827865\n",
      "Epoch: 263 - Loss: 0.23352629611038866\n",
      "Epoch: 264 - Loss: 0.233474246250959\n",
      "Epoch: 265 - Loss: 0.23342254901518014\n",
      "Epoch: 266 - Loss: 0.23337120120606783\n",
      "Epoch: 267 - Loss: 0.23332019966351172\n",
      "Epoch: 268 - Loss: 0.23326954126374844\n",
      "Epoch: 269 - Loss: 0.23321922291884323\n",
      "Epoch: 270 - Loss: 0.23316924157618116\n",
      "Epoch: 271 - Loss: 0.23311959421796713\n",
      "Epoch: 272 - Loss: 0.23307027786073473\n",
      "Epoch: 273 - Loss: 0.23302128955486356\n",
      "Epoch: 274 - Loss: 0.23297262638410499\n",
      "Epoch: 275 - Loss: 0.23292428546511648\n",
      "Epoch: 276 - Loss: 0.23287626394700311\n",
      "Epoch: 277 - Loss: 0.23282855901086794\n",
      "Epoch: 278 - Loss: 0.2327811678693692\n",
      "Epoch: 279 - Loss: 0.23273408776628599\n",
      "Epoch: 280 - Loss: 0.23268731597609016\n",
      "Epoch: 281 - Loss: 0.2326408498035265\n",
      "Epoch: 282 - Loss: 0.2325946865831994\n",
      "Epoch: 283 - Loss: 0.23254882367916663\n",
      "Epoch: 284 - Loss: 0.23250325848453976\n",
      "Epoch: 285 - Loss: 0.23245798842109142\n",
      "Epoch: 286 - Loss: 0.2324130109388689\n",
      "Epoch: 287 - Loss: 0.23236832351581435\n",
      "Epoch: 288 - Loss: 0.23232392365739105\n",
      "Epoch: 289 - Loss: 0.23227980889621547\n",
      "Epoch: 290 - Loss: 0.23223597679169625\n",
      "Epoch: 291 - Loss: 0.23219242492967798\n",
      "Epoch: 292 - Loss: 0.23214915092209157\n",
      "Epoch: 293 - Loss: 0.2321061524066097\n",
      "Epoch: 294 - Loss: 0.23206342704630842\n",
      "Epoch: 295 - Loss: 0.23202097252933318\n",
      "Epoch: 296 - Loss: 0.2319787865685714\n",
      "Epoch: 297 - Loss: 0.2319368669013294\n",
      "Epoch: 298 - Loss: 0.23189521128901444\n",
      "Epoch: 299 - Loss: 0.23185381751682244\n",
      "Epoch: 300 - Loss: 0.23181268339342997\n",
      "Epoch: 301 - Loss: 0.23177180675069145\n",
      "Epoch: 302 - Loss: 0.2317311854433405\n",
      "Epoch: 303 - Loss: 0.23169081734869698\n",
      "Epoch: 304 - Loss: 0.23165070036637725\n",
      "Epoch: 305 - Loss: 0.23161083241801048\n",
      "Epoch: 306 - Loss: 0.23157121144695733\n",
      "Epoch: 307 - Loss: 0.23153183541803504\n",
      "Epoch: 308 - Loss: 0.23149270231724536\n",
      "Epoch: 309 - Loss: 0.23145381015150682\n",
      "Epoch: 310 - Loss: 0.23141515694839168\n",
      "Epoch: 311 - Loss: 0.23137674075586614\n",
      "Epoch: 312 - Loss: 0.23133855964203498\n",
      "Epoch: 313 - Loss: 0.23130061169488955\n",
      "Epoch: 314 - Loss: 0.23126289502206024\n",
      "Epoch: 315 - Loss: 0.2312254077505718\n",
      "Epoch: 316 - Loss: 0.23118814802660315\n",
      "Epoch: 317 - Loss: 0.23115111401524993\n",
      "Epoch: 318 - Loss: 0.23111430390029142\n",
      "Epoch: 319 - Loss: 0.23107771588396023\n",
      "Epoch: 320 - Loss: 0.23104134818671554\n",
      "Epoch: 321 - Loss: 0.23100519904701988\n",
      "Epoch: 322 - Loss: 0.2309692667211191\n",
      "Epoch: 323 - Loss: 0.23093354948282518\n",
      "Epoch: 324 - Loss: 0.23089804562330282\n",
      "Epoch: 325 - Loss: 0.2308627534508588\n",
      "Epoch: 326 - Loss: 0.23082767129073392\n",
      "Epoch: 327 - Loss: 0.23079279748489914\n",
      "Epoch: 328 - Loss: 0.23075813039185342\n",
      "Epoch: 329 - Loss: 0.2307236683864248\n",
      "Epoch: 330 - Loss: 0.23068940985957517\n",
      "Epoch: 331 - Loss: 0.23065535321820615\n",
      "Epoch: 332 - Loss: 0.23062149688496966\n",
      "Epoch: 333 - Loss: 0.23058783929807958\n",
      "Epoch: 334 - Loss: 0.23055437891112687\n",
      "Epoch: 335 - Loss: 0.2305211141928972\n",
      "Epoch: 336 - Loss: 0.23048804362719102\n",
      "Epoch: 337 - Loss: 0.2304551657126461\n",
      "Epoch: 338 - Loss: 0.23042247896256274\n",
      "Epoch: 339 - Loss: 0.23038998190473137\n",
      "Epoch: 340 - Loss: 0.2303576730812623\n",
      "Epoch: 341 - Loss: 0.23032555104841826\n",
      "Epoch: 342 - Loss: 0.2302936143764488\n",
      "Epoch: 343 - Loss: 0.23026186164942727\n",
      "Epoch: 344 - Loss: 0.23023029146509014\n",
      "Epoch: 345 - Loss: 0.230198902434678\n",
      "Epoch: 346 - Loss: 0.23016769318277933\n",
      "Epoch: 347 - Loss: 0.23013666234717603\n",
      "Epoch: 348 - Loss: 0.2301058085786912\n",
      "Epoch: 349 - Loss: 0.23007513054103904\n",
      "Epoch: 350 - Loss: 0.2300446269106766\n",
      "Epoch: 351 - Loss: 0.2300142963766576\n",
      "Epoch: 352 - Loss: 0.2299841376404884\n",
      "Epoch: 353 - Loss: 0.22995414941598544\n",
      "Epoch: 354 - Loss: 0.2299243304291355\n",
      "Epoch: 355 - Loss: 0.22989467941795652\n",
      "Epoch: 356 - Loss: 0.22986519513236156\n",
      "Epoch: 357 - Loss: 0.22983587633402372\n",
      "Epoch: 358 - Loss: 0.22980672179624317\n",
      "Epoch: 359 - Loss: 0.2297777303038159\n",
      "Epoch: 360 - Loss: 0.22974890065290446\n",
      "Epoch: 361 - Loss: 0.22972023165090985\n",
      "Epoch: 362 - Loss: 0.22969172211634534\n",
      "Epoch: 363 - Loss: 0.22966337087871244\n",
      "Epoch: 364 - Loss: 0.22963517677837766\n",
      "Epoch: 365 - Loss: 0.2296071386664517\n",
      "Epoch: 366 - Loss: 0.22957925540466909\n",
      "Epoch: 367 - Loss: 0.22955152586527106\n",
      "Epoch: 368 - Loss: 0.2295239489308881\n",
      "Epoch: 369 - Loss: 0.2294965234944255\n",
      "Epoch: 370 - Loss: 0.2294692484589496\n",
      "Epoch: 371 - Loss: 0.22944212273757558\n",
      "Epoch: 372 - Loss: 0.22941514525335707\n",
      "Epoch: 373 - Loss: 0.22938831493917658\n",
      "Epoch: 374 - Loss: 0.22936163073763804\n",
      "Epoch: 375 - Loss: 0.2293350916009602\n",
      "Epoch: 376 - Loss: 0.22930869649087118\n",
      "Epoch: 377 - Loss: 0.22928244437850553\n",
      "Epoch: 378 - Loss: 0.22925633424430095\n",
      "Epoch: 379 - Loss: 0.22923036507789765\n",
      "Epoch: 380 - Loss: 0.22920453587803843\n",
      "Epoch: 381 - Loss: 0.22917884565246985\n",
      "Epoch: 382 - Loss: 0.22915329341784516\n",
      "Epoch: 383 - Loss: 0.229127878199628\n",
      "Epoch: 384 - Loss: 0.22910259903199756\n",
      "Epoch: 385 - Loss: 0.22907745495775486\n",
      "Epoch: 386 - Loss: 0.22905244502823008\n",
      "Epoch: 387 - Loss: 0.22902756830319135\n",
      "Epoch: 388 - Loss: 0.22900282385075413\n",
      "Epoch: 389 - Loss: 0.22897821074729272\n",
      "Epoch: 390 - Loss: 0.2289537280773513\n",
      "Epoch: 391 - Loss: 0.22892937493355792\n",
      "Epoch: 392 - Loss: 0.2289051504165379\n",
      "Epoch: 393 - Loss: 0.22888105363482947\n",
      "Epoch: 394 - Loss: 0.22885708370479985\n",
      "Epoch: 395 - Loss: 0.2288332397505625\n",
      "Epoch: 396 - Loss: 0.2288095209038953\n",
      "Epoch: 397 - Loss: 0.22878592630416014\n",
      "Epoch: 398 - Loss: 0.22876245509822268\n",
      "Epoch: 399 - Loss: 0.22873910644037423\n",
      "Epoch: 400 - Loss: 0.22871587949225333\n",
      "Epoch: 401 - Loss: 0.22869277342276928\n",
      "Epoch: 402 - Loss: 0.22866978740802604\n",
      "Epoch: 403 - Loss: 0.22864692063124747\n",
      "Epoch: 404 - Loss: 0.2286241722827027\n",
      "Epoch: 405 - Loss: 0.22860154155963366\n",
      "Epoch: 406 - Loss: 0.22857902766618202\n",
      "Epoch: 407 - Loss: 0.22855662981331828\n",
      "Epoch: 408 - Loss: 0.22853434721877094\n",
      "Epoch: 409 - Loss: 0.2285121791069568\n",
      "Epoch: 410 - Loss: 0.22849012470891195\n",
      "Epoch: 411 - Loss: 0.22846818326222396\n",
      "Epoch: 412 - Loss: 0.228446354010964\n",
      "Epoch: 413 - Loss: 0.22842463620562134\n",
      "Epoch: 414 - Loss: 0.22840302910303656\n",
      "Epoch: 415 - Loss: 0.2283815319663375\n",
      "Epoch: 416 - Loss: 0.2283601440648746\n",
      "Epoch: 417 - Loss: 0.22833886467415784\n",
      "Epoch: 418 - Loss: 0.2283176930757939\n",
      "Epoch: 419 - Loss: 0.22829662855742425\n",
      "Epoch: 420 - Loss: 0.22827567041266414\n",
      "Epoch: 421 - Loss: 0.2282548179410422\n",
      "Epoch: 422 - Loss: 0.22823407044794028\n",
      "Epoch: 423 - Loss: 0.228213427244535\n",
      "Epoch: 424 - Loss: 0.22819288764773885\n",
      "Epoch: 425 - Loss: 0.2281724509801429\n",
      "Epoch: 426 - Loss: 0.22815211656995968\n",
      "Epoch: 427 - Loss: 0.22813188375096669\n",
      "Epoch: 428 - Loss: 0.228111751862451\n",
      "Epoch: 429 - Loss: 0.228091720249154\n",
      "Epoch: 430 - Loss: 0.22807178826121702\n",
      "Epoch: 431 - Loss: 0.22805195525412778\n",
      "Epoch: 432 - Loss: 0.2280322205886668\n",
      "Epoch: 433 - Loss: 0.22801258363085533\n",
      "Epoch: 434 - Loss: 0.22799304375190318\n",
      "Epoch: 435 - Loss: 0.22797360032815728\n",
      "Epoch: 436 - Loss: 0.2279542527410512\n",
      "Epoch: 437 - Loss: 0.2279350003770548\n",
      "Epoch: 438 - Loss: 0.22791584262762465\n",
      "Epoch: 439 - Loss: 0.22789677888915497\n",
      "Epoch: 440 - Loss: 0.22787780856292938\n",
      "Epoch: 441 - Loss: 0.2278589310550728\n",
      "Epoch: 442 - Loss: 0.22784014577650372\n",
      "Epoch: 443 - Loss: 0.2278214521428882\n",
      "Epoch: 444 - Loss: 0.22780284957459296\n",
      "Epoch: 445 - Loss: 0.2277843374966395\n",
      "Epoch: 446 - Loss: 0.2277659153386593\n",
      "Epoch: 447 - Loss: 0.2277475825348488\n",
      "Epoch: 448 - Loss: 0.2277293385239251\n",
      "Epoch: 449 - Loss: 0.22771118274908228\n",
      "Epoch: 450 - Loss: 0.2276931146579481\n",
      "Epoch: 451 - Loss: 0.22767513370254142\n",
      "Epoch: 452 - Loss: 0.22765723933922954\n",
      "Epoch: 453 - Loss: 0.22763943102868653\n",
      "Epoch: 454 - Loss: 0.22762170823585215\n",
      "Epoch: 455 - Loss: 0.22760407042989042\n",
      "Epoch: 456 - Loss: 0.22758651708414965\n",
      "Epoch: 457 - Loss: 0.2275690476761221\n",
      "Epoch: 458 - Loss: 0.2275516616874047\n",
      "Epoch: 459 - Loss: 0.22753435860365967\n",
      "Epoch: 460 - Loss: 0.22751713791457606\n",
      "Epoch: 461 - Loss: 0.22749999911383142\n",
      "Epoch: 462 - Loss: 0.22748294169905384\n",
      "Epoch: 463 - Loss: 0.22746596517178486\n",
      "Epoch: 464 - Loss: 0.22744906903744191\n",
      "Epoch: 465 - Loss: 0.22743225280528243\n",
      "Epoch: 466 - Loss: 0.22741551598836698\n",
      "Epoch: 467 - Loss: 0.22739885810352392\n",
      "Epoch: 468 - Loss: 0.22738227867131391\n",
      "Epoch: 469 - Loss: 0.2273657772159946\n",
      "Epoch: 470 - Loss: 0.22734935326548622\n",
      "Epoch: 471 - Loss: 0.22733300635133738\n",
      "Epoch: 472 - Loss: 0.2273167360086911\n",
      "Epoch: 473 - Loss: 0.22730054177625122\n",
      "Epoch: 474 - Loss: 0.22728442319624922\n",
      "Epoch: 475 - Loss: 0.22726837981441164\n",
      "Epoch: 476 - Loss: 0.22725241117992734\n",
      "Epoch: 477 - Loss: 0.22723651684541565\n",
      "Epoch: 478 - Loss: 0.22722069636689443\n",
      "Epoch: 479 - Loss: 0.2272049493037487\n",
      "Epoch: 480 - Loss: 0.22718927521869983\n",
      "Epoch: 481 - Loss: 0.2271736736777743\n",
      "Epoch: 482 - Loss: 0.22715814425027372\n",
      "Epoch: 483 - Loss: 0.2271426865087446\n",
      "Epoch: 484 - Loss: 0.22712730002894863\n",
      "Epoch: 485 - Loss: 0.22711198438983296\n",
      "Epoch: 486 - Loss: 0.22709673917350157\n",
      "Epoch: 487 - Loss: 0.227081563965186\n",
      "Epoch: 488 - Loss: 0.22706645835321707\n",
      "Epoch: 489 - Loss: 0.22705142192899666\n",
      "Epoch: 490 - Loss: 0.2270364542869697\n",
      "Epoch: 491 - Loss: 0.22702155502459667\n",
      "Epoch: 492 - Loss: 0.2270067237423261\n",
      "Epoch: 493 - Loss: 0.22699196004356773\n",
      "Epoch: 494 - Loss: 0.22697726353466563\n",
      "Epoch: 495 - Loss: 0.22696263382487178\n",
      "Epoch: 496 - Loss: 0.22694807052631985\n",
      "Epoch: 497 - Loss: 0.22693357325399943\n",
      "Epoch: 498 - Loss: 0.22691914162573001\n",
      "Epoch: 499 - Loss: 0.22690477526213618\n",
      "Epoch: 500 - Loss: 0.22689047378662186\n"
     ]
    }
   ],
   "source": [
    "X, Y, prob  = model_LR.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 300)\n"
     ]
    }
   ],
   "source": [
    "preds, prob, X = model_LR.classify(pred_file + \".txt\", model_LR.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(train_file, sep='\\t', header=None, names=['text', 'true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos.dec    94\n",
       "neg.tru    83\n",
       "pos.tru    70\n",
       "neg.dec    65\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 2)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 1)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 25.64%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
