{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = KeyedVectors.load(\"work/word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [text.split() for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_4dim = Features(\"work/datasets/4dim.train.txt\")\n",
    "feat_products = Features(\"work/datasets/products.train.txt\")\n",
    "feat_questions = Features(\"work/datasets/questions.train.txt\")\n",
    "feat_odiya = Features(\"work/datasets/odiya.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = []\n",
    "for sentence in feat_4dim.tokenized_text:\n",
    "    for word in sentence:\n",
    "        try: \n",
    "            embedding[word] = model_embeddings[word]\n",
    "            all_vocab.append(word)\n",
    "        except:\n",
    "            pass\n",
    "for sentence in feat_products.tokenized_text:\n",
    "    for word in sentence:\n",
    "        try: \n",
    "            embedding[word] = model_embeddings[word]\n",
    "            all_vocab.append(word)\n",
    "        except:\n",
    "            pass\n",
    "for sentence in feat_questions.tokenized_text:\n",
    "    for word in sentence:\n",
    "        try: \n",
    "            embedding[word] = model_embeddings[word]\n",
    "            all_vocab.append(word)\n",
    "        except:\n",
    "            pass\n",
    "for sentence in feat_odiya.tokenized_text:\n",
    "    for word in sentence:\n",
    "        try: \n",
    "            embedding[word] = model_embeddings[word]\n",
    "            all_vocab.append(word)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"word2vec_embedding.wordvectors\", \"wb\") as file:\n",
    "    pickle.dump(embedding, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2vec_embedding.wordvectors\", \"rb\") as file:\n",
    "    emb = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(emb.values())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    # 5) Remove Stop Words\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_word2vec(tokenized_sentence, embedding_matrix):\n",
    "    \"\"\"Convert sentence to Word2Vec Embeeding.\n",
    "    Each sentence is represented by the average of all of word embeddings \n",
    "    of that sentence.\n",
    "    \"\"\"\n",
    "    sentence_embedding = []\n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        # get embedding of word if exists\n",
    "        try:\n",
    "            word_emb = embedding_matrix[word]\n",
    "            sentence_embedding.append(word_emb)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Compute average of the sentence\n",
    "    if len(sentence_embedding) > 0:\n",
    "        stacked_arrays = np.vstack(sentence_embedding)\n",
    "        elementwise_average = np.mean(stacked_arrays, axis=0)\n",
    "    else:\n",
    "        elementwise_average = np.zeros(embedding_matrix.shape[1])\n",
    "    return elementwise_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Logistic Regression Features #\n",
    "################################\n",
    "\n",
    "class Features_LR_Word2Vec(Features):\n",
    "\n",
    "    def __init__(self, model_file, embedding_matrix):\n",
    "        super(Features_LR_Word2Vec, self).__init__(model_file)\n",
    "        self.embedding_matrix = embedding_matrix # Need to save IDF values for inference\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def get_features_word2vec(self, tokenized_sentence):\n",
    "        \"\"\"Convert sentence to Word2Vec Embeeding.\n",
    "        Each sentence is represented by the average of all of word embeddings \n",
    "        of that sentence.\n",
    "        \"\"\"\n",
    "        sentence_embedding = []\n",
    "        \n",
    "        for word in tokenized_sentence:\n",
    "            # get embedding of word if exists\n",
    "            try:\n",
    "                word_emb = self.embedding_matrix[word]\n",
    "                sentence_embedding.append(word_emb)\n",
    "            except: # remove Out-of-Vocabulary words\n",
    "                pass\n",
    "        \n",
    "        # Compute average of the sentence\n",
    "        if len(sentence_embedding) > 0:\n",
    "            stacked_arrays = np.vstack(sentence_embedding)\n",
    "            elementwise_average = np.mean(stacked_arrays, axis=0)\n",
    "        else:\n",
    "            elementwise_average = np.zeros(list(emb.values())[0].shape[0])\n",
    "        return elementwise_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Refer to Chapter 5 for more details on how to implement a LogisticRegression\n",
    "\"\"\"\n",
    "from work.Model import *\n",
    "\n",
    "class LogisticRegressionWord2Vec(Model):\n",
    "    def __init__(self, model_file, learning_rate=None, epochs=None, batch_size=None, embedding_matrix=None):\n",
    "        super(LogisticRegressionWord2Vec, self).__init__(model_file)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.Y_to_categorical = None # Map Y label to numerical\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "\n",
    "    def initialize_weights(self, num_features, num_labels):\n",
    "        self.weights = np.zeros((num_features, num_labels))\n",
    "        self.bias = np.zeros(num_labels)\n",
    "        # np.random.seed(0)\n",
    "        # self.weights = np.random.rand(num_features, num_labels)\n",
    "        # self.bias = np.random.rand(num_labels)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"Sigmoid function for binary classification\n",
    "\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return 1/(1+e^{-Z})\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "        \n",
    "    def predict_prob(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of shape [num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "        return S\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate Binary cross-entropy\n",
    "        \"\"\"\n",
    "        return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n",
    "\n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "\n",
    "            # Rows with highest probability\n",
    "            S_max = np.argmax(S, axis=1)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "            # Rows with highest probability\n",
    "            S_max = [1 if i > 0.5 else 0 for i in S]\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "\n",
    "    def train(self, input_file, verbose=False):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset and create vocabulary\n",
    "        features_lr_class = Features_LR_Word2Vec(input_file, self.embedding_matrix)\n",
    "        embedding_size = list(self.embedding_matrix.values())[0].shape[0]\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in enumerate(features_lr_class.tokenized_text):\n",
    "            tmp = features_lr_class.get_features_word2vec(sentence)\n",
    "            updated_text.append(tmp)\n",
    "        \n",
    "        # Transform dataset to Word2Vec space\n",
    "        # Return features with format (n_documents, embedding_size=300)\n",
    "        X = np.array(updated_text)\n",
    "        \n",
    "        # Y\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_lr_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_lr_class.labels]\n",
    "\n",
    "        # Initialize Weights\n",
    "        sample_size = len(features_lr_class.tokenized_text)\n",
    "        n_features = embedding_size\n",
    "        num_labels = len(features_lr_class.labelset)\n",
    "\n",
    "\n",
    "        # Check if it's multinomial or binary classification\n",
    "        if num_labels == 2:\n",
    "            multinomial = False\n",
    "            num_labels = 1 # Only one column to reference 0 or 1\n",
    "        else:\n",
    "            multinomial = True\n",
    "\n",
    "        self.initialize_weights(n_features, num_labels)\n",
    "\n",
    "        # One Hot encoded Y\n",
    "        if multinomial:\n",
    "            Y_onehot = self.OneHot(Y, num_labels)\n",
    "        else:\n",
    "            Y_onehot = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        X_permutation = X[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # Batch_size implementation\n",
    "            for j in range(0, sample_size, batch_size):\n",
    "                X_mini_batch = X_permutation[j:j+batch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+batch_size]\n",
    "\n",
    "                # Z = softmax(X*W + b)\n",
    "                prob = self.predict_prob(X_mini_batch, self.weights, self.bias, multinomial)\n",
    "\n",
    "                # dL/dW\n",
    "                grad_w = (1/batch_size)*np.dot(X_mini_batch.T, prob - y_mini_batch)\n",
    "                grad_b =  (1/batch_size)*np.sum(prob - y_mini_batch, axis=0)\n",
    "\n",
    "            # # break            \n",
    "            # dL/dW\n",
    "                # grad_w = (1/sample_size)*np.dot(X.T, prob - Y_onehot)\n",
    "                # grad_b =  (1/sample_size)*np.sum(prob - Y_onehot, axis=0)\n",
    "\n",
    "                self.weights = self.weights - (self.learning_rate*grad_w)\n",
    "                self.bias = self.bias - (self.learning_rate*grad_b)\n",
    "\n",
    "            # Computing cross-entropy loss\n",
    "            if multinomial:\n",
    "                loss = self.cross_entropy_loss(prob, y_mini_batch)\n",
    "            else:\n",
    "                loss = self.binary_cross_entropy_loss(prob, y_mini_batch)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "        model = {\n",
    "            \"feature_weights\": {\n",
    "                \"weights\": self.weights,\n",
    "                \"bias\": self.bias,\n",
    "                \"Y_to_categorical\": self.Y_to_categorical\n",
    "            },\n",
    "            \"Feature\": features_lr_class\n",
    "        }\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return X, Y_onehot, prob, \n",
    "\n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_fixle: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\"\n",
    "\n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_LR_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_LR_class.read_inference_file(input_file)\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in enumerate(tokenized_text):\n",
    "            tmp = Feature_LR_class.get_features_word2vec(sentence)\n",
    "            updated_text.append(tmp)\n",
    "\n",
    "        tokenized_text = updated_text\n",
    "        \n",
    "\n",
    "        X = np.vstack(tokenized_text)\n",
    "\n",
    "        print(X.shape)\n",
    "\n",
    "        # Prediction\n",
    "        multinomial = True if len(feature_weights['Y_to_categorical'].keys()) > 2 else False\n",
    "        preds_numerical = self.predict(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        # Map indexes to Categorical space\n",
    "        preds_label = []\n",
    "        probs = self.predict_prob(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        for y in preds_numerical:\n",
    "            tmp = feature_weights['Y_to_categorical'][y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.36270217289891027\n",
      "Epoch: 2 - Loss: 0.3601051232552232\n",
      "Epoch: 3 - Loss: 0.3582109124187851\n",
      "Epoch: 4 - Loss: 0.3568322016818795\n",
      "Epoch: 5 - Loss: 0.3558299613062518\n",
      "Epoch: 6 - Loss: 0.35510193245926847\n",
      "Epoch: 7 - Loss: 0.3545733635078525\n",
      "Epoch: 8 - Loss: 0.3541898269834113\n",
      "Epoch: 9 - Loss: 0.3539117830649953\n",
      "Epoch: 10 - Loss: 0.3537105340252476\n",
      "Epoch: 11 - Loss: 0.3535652491414153\n",
      "Epoch: 12 - Loss: 0.35346079456274326\n",
      "Epoch: 13 - Loss: 0.3533861589129823\n",
      "Epoch: 14 - Loss: 0.3533333149571248\n",
      "Epoch: 15 - Loss: 0.3532963980564893\n",
      "Epoch: 16 - Loss: 0.35327111360644964\n",
      "Epoch: 17 - Loss: 0.3532543094678373\n",
      "Epoch: 18 - Loss: 0.35324366708202476\n",
      "Epoch: 19 - Loss: 0.35323747790972493\n",
      "Epoch: 20 - Loss: 0.3532344812328114\n",
      "Epoch: 21 - Loss: 0.35323374613736064\n",
      "Epoch: 22 - Loss: 0.35323458536423646\n",
      "Epoch: 23 - Loss: 0.35323649219983333\n",
      "Epoch: 24 - Loss: 0.3532390940724155\n",
      "Epoch: 25 - Loss: 0.3532421183008438\n",
      "Epoch: 26 - Loss: 0.35324536671566775\n",
      "Epoch: 27 - Loss: 0.35324869678332044\n",
      "Epoch: 28 - Loss: 0.35325200751662\n",
      "Epoch: 29 - Loss: 0.35325522892312045\n",
      "Epoch: 30 - Loss: 0.3532583140798951\n",
      "Epoch: 31 - Loss: 0.3532612331665726\n",
      "Epoch: 32 - Loss: 0.353263968964603\n",
      "Epoch: 33 - Loss: 0.3532665134587706\n",
      "Epoch: 34 - Loss: 0.3532688652704263\n",
      "Epoch: 35 - Loss: 0.35327102772042157\n",
      "Epoch: 36 - Loss: 0.3532730073701811\n",
      "Epoch: 37 - Loss: 0.3532748129266926\n",
      "Epoch: 38 - Loss: 0.3532764544249603\n",
      "Epoch: 39 - Loss: 0.3532779426222251\n",
      "Epoch: 40 - Loss: 0.35327928855384433\n",
      "Epoch: 41 - Loss: 0.3532805032124917\n",
      "Epoch: 42 - Loss: 0.3532815973212626\n",
      "Epoch: 43 - Loss: 0.35328258117806943\n",
      "Epoch: 44 - Loss: 0.35328346455390663\n",
      "Epoch: 45 - Loss: 0.3532842566315657\n",
      "Epoch: 46 - Loss: 0.35328496597444903\n",
      "Epoch: 47 - Loss: 0.35328560051751323\n",
      "Epoch: 48 - Loss: 0.3532861675742115\n",
      "Epoch: 49 - Loss: 0.3532866738547347\n",
      "Epoch: 50 - Loss: 0.3532871254919584\n",
      "Epoch: 51 - Loss: 0.35328752807236896\n",
      "Epoch: 52 - Loss: 0.35328788666991257\n",
      "Epoch: 53 - Loss: 0.35328820588122867\n",
      "Epoch: 54 - Loss: 0.3532884898611403\n",
      "Epoch: 55 - Loss: 0.353288742357581\n",
      "Epoch: 56 - Loss: 0.35328896674538146\n",
      "Epoch: 57 - Loss: 0.35328916605852656\n",
      "Epoch: 58 - Loss: 0.35328934302062914\n",
      "Epoch: 59 - Loss: 0.3532895000734808\n",
      "Epoch: 60 - Loss: 0.3532896394036135\n",
      "Epoch: 61 - Loss: 0.35328976296686804\n",
      "Epoch: 62 - Loss: 0.3532898725110098\n",
      "Epoch: 63 - Loss: 0.3532899695964561\n",
      "Epoch: 64 - Loss: 0.3532900556152069\n",
      "Epoch: 65 - Loss: 0.3532901318080784\n",
      "Epoch: 66 - Loss: 0.35329019928034966\n",
      "Epoch: 67 - Loss: 0.3532902590159318\n",
      "Epoch: 68 - Loss: 0.3532903118901724\n",
      "Epoch: 69 - Loss: 0.3532903586814048\n",
      "Epoch: 70 - Loss: 0.3532904000813453\n",
      "Epoch: 71 - Loss: 0.353290436704441\n",
      "Epoch: 72 - Loss: 0.3532904690962599\n",
      "Epoch: 73 - Loss: 0.3532904977410131\n",
      "Epoch: 74 - Loss: 0.3532905230682911\n",
      "Epoch: 75 - Loss: 0.35329054545908883\n",
      "Epoch: 76 - Loss: 0.35329056525119035\n",
      "Epoch: 77 - Loss: 0.3532905827439766\n",
      "Epoch: 78 - Loss: 0.35329059820271413\n",
      "Epoch: 79 - Loss: 0.35329061186237837\n",
      "Epoch: 80 - Loss: 0.3532906239310597\n",
      "Epoch: 81 - Loss: 0.35329063459299537\n",
      "Epoch: 82 - Loss: 0.35329064401126753\n",
      "Epoch: 83 - Loss: 0.3532906523302011\n",
      "Epoch: 84 - Loss: 0.35329065967749723\n",
      "Epoch: 85 - Loss: 0.35329066616612637\n",
      "Epoch: 86 - Loss: 0.35329067189601\n",
      "Epoch: 87 - Loss: 0.3532906769555137\n",
      "Epoch: 88 - Loss: 0.3532906814227716\n",
      "Epoch: 89 - Loss: 0.35329068536686087\n",
      "Epoch: 90 - Loss: 0.3532906888488432\n",
      "Epoch: 91 - Loss: 0.35329069192268797\n",
      "Epoch: 92 - Loss: 0.3532906946360904\n",
      "Epoch: 93 - Loss: 0.35329069703119564\n",
      "Epoch: 94 - Loss: 0.3532906991452416\n",
      "Epoch: 95 - Loss: 0.35329070101112603\n",
      "Epoch: 96 - Loss: 0.3532907026579097\n",
      "Epoch: 97 - Loss: 0.3532907041112623\n",
      "Epoch: 98 - Loss: 0.35329070539385526\n",
      "Epoch: 99 - Loss: 0.3532907065257114\n",
      "Epoch: 100 - Loss: 0.3532907075245121\n",
      "Epoch: 101 - Loss: 0.3532907084058703\n",
      "Epoch: 102 - Loss: 0.3532907091835717\n",
      "Epoch: 103 - Loss: 0.3532907098697877\n",
      "Epoch: 104 - Loss: 0.35329071047526395\n",
      "Epoch: 105 - Loss: 0.3532907110094861\n",
      "Epoch: 106 - Loss: 0.35329071148082836\n",
      "Epoch: 107 - Loss: 0.3532907118966824\n",
      "Epoch: 108 - Loss: 0.3532907122635722\n",
      "Epoch: 109 - Loss: 0.35329071258725664\n",
      "Epoch: 110 - Loss: 0.35329071287281766\n",
      "Epoch: 111 - Loss: 0.3532907131247413\n",
      "Epoch: 112 - Loss: 0.3532907133469856\n",
      "Epoch: 113 - Loss: 0.35329071354304387\n",
      "Epoch: 114 - Loss: 0.3532907137159989\n",
      "Epoch: 115 - Loss: 0.3532907138685707\n",
      "Epoch: 116 - Loss: 0.3532907140031598\n",
      "Epoch: 117 - Loss: 0.3532907141218841\n",
      "Epoch: 118 - Loss: 0.35329071422661235\n",
      "Epoch: 119 - Loss: 0.35329071431899317\n",
      "Epoch: 120 - Loss: 0.3532907144004817\n",
      "Epoch: 121 - Loss: 0.3532907144723614\n",
      "Epoch: 122 - Loss: 0.35329071453576444\n",
      "Epoch: 123 - Loss: 0.3532907145916902\n",
      "Epoch: 124 - Loss: 0.35329071464101985\n",
      "Epoch: 125 - Loss: 0.35329071468453116\n",
      "Epoch: 126 - Loss: 0.35329071472290996\n",
      "Epoch: 127 - Loss: 0.3532907147567613\n",
      "Epoch: 128 - Loss: 0.3532907147866194\n",
      "Epoch: 129 - Loss: 0.35329071481295476\n",
      "Epoch: 130 - Loss: 0.3532907148361832\n",
      "Epoch: 131 - Loss: 0.3532907148566708\n",
      "Epoch: 132 - Loss: 0.35329071487474106\n",
      "Epoch: 133 - Loss: 0.35329071489067915\n",
      "Epoch: 134 - Loss: 0.35329071490473635\n",
      "Epoch: 135 - Loss: 0.3532907149171348\n",
      "Epoch: 136 - Loss: 0.35329071492807007\n",
      "Epoch: 137 - Loss: 0.3532907149377148\n",
      "Epoch: 138 - Loss: 0.3532907149462213\n",
      "Epoch: 139 - Loss: 0.3532907149537238\n",
      "Epoch: 140 - Loss: 0.3532907149603408\n",
      "Epoch: 141 - Loss: 0.35329071496617687\n",
      "Epoch: 142 - Loss: 0.353290714971324\n",
      "Epoch: 143 - Loss: 0.3532907149758638\n",
      "Epoch: 144 - Loss: 0.3532907149798675\n",
      "Epoch: 145 - Loss: 0.35329071498339876\n",
      "Epoch: 146 - Loss: 0.3532907149865131\n",
      "Epoch: 147 - Loss: 0.3532907149892598\n",
      "Epoch: 148 - Loss: 0.3532907149916823\n",
      "Epoch: 149 - Loss: 0.35329071499381887\n",
      "Epoch: 150 - Loss: 0.3532907149957031\n",
      "Epoch: 151 - Loss: 0.353290714997365\n",
      "Epoch: 152 - Loss: 0.35329071499883064\n",
      "Epoch: 153 - Loss: 0.3532907150001233\n",
      "Epoch: 154 - Loss: 0.3532907150012633\n",
      "Epoch: 155 - Loss: 0.3532907150022688\n",
      "Epoch: 156 - Loss: 0.35329071500315545\n",
      "Epoch: 157 - Loss: 0.3532907150039375\n",
      "Epoch: 158 - Loss: 0.3532907150046272\n",
      "Epoch: 159 - Loss: 0.3532907150052356\n",
      "Epoch: 160 - Loss: 0.35329071500577197\n",
      "Epoch: 161 - Loss: 0.35329071500624504\n",
      "Epoch: 162 - Loss: 0.35329071500666237\n",
      "Epoch: 163 - Loss: 0.35329071500703035\n",
      "Epoch: 164 - Loss: 0.35329071500735487\n",
      "Epoch: 165 - Loss: 0.3532907150076412\n",
      "Epoch: 166 - Loss: 0.3532907150078936\n",
      "Epoch: 167 - Loss: 0.3532907150081162\n",
      "Epoch: 168 - Loss: 0.35329071500831255\n",
      "Epoch: 169 - Loss: 0.3532907150084857\n",
      "Epoch: 170 - Loss: 0.35329071500863846\n",
      "Epoch: 171 - Loss: 0.35329071500877307\n",
      "Epoch: 172 - Loss: 0.35329071500889186\n",
      "Epoch: 173 - Loss: 0.35329071500899667\n",
      "Epoch: 174 - Loss: 0.353290715009089\n",
      "Epoch: 175 - Loss: 0.3532907150091706\n",
      "Epoch: 176 - Loss: 0.35329071500924236\n",
      "Epoch: 177 - Loss: 0.35329071500930564\n",
      "Epoch: 178 - Loss: 0.35329071500936166\n",
      "Epoch: 179 - Loss: 0.35329071500941095\n",
      "Epoch: 180 - Loss: 0.3532907150094544\n",
      "Epoch: 181 - Loss: 0.3532907150094927\n",
      "Epoch: 182 - Loss: 0.3532907150095265\n",
      "Epoch: 183 - Loss: 0.3532907150095564\n",
      "Epoch: 184 - Loss: 0.3532907150095826\n",
      "Epoch: 185 - Loss: 0.35329071500960585\n",
      "Epoch: 186 - Loss: 0.35329071500962633\n",
      "Epoch: 187 - Loss: 0.3532907150096444\n",
      "Epoch: 188 - Loss: 0.3532907150096602\n",
      "Epoch: 189 - Loss: 0.35329071500967424\n",
      "Epoch: 190 - Loss: 0.3532907150096867\n",
      "Epoch: 191 - Loss: 0.3532907150096975\n",
      "Epoch: 192 - Loss: 0.35329071500970727\n",
      "Epoch: 193 - Loss: 0.3532907150097156\n",
      "Epoch: 194 - Loss: 0.3532907150097232\n",
      "Epoch: 195 - Loss: 0.3532907150097298\n",
      "Epoch: 196 - Loss: 0.3532907150097355\n",
      "Epoch: 197 - Loss: 0.3532907150097407\n",
      "Epoch: 198 - Loss: 0.35329071500974524\n",
      "Epoch: 199 - Loss: 0.3532907150097493\n",
      "Epoch: 200 - Loss: 0.3532907150097527\n",
      "Epoch: 201 - Loss: 0.3532907150097558\n",
      "Epoch: 202 - Loss: 0.3532907150097586\n",
      "Epoch: 203 - Loss: 0.353290715009761\n",
      "Epoch: 204 - Loss: 0.3532907150097632\n",
      "Epoch: 205 - Loss: 0.35329071500976506\n",
      "Epoch: 206 - Loss: 0.3532907150097667\n",
      "Epoch: 207 - Loss: 0.35329071500976805\n",
      "Epoch: 208 - Loss: 0.35329071500976944\n",
      "Epoch: 209 - Loss: 0.3532907150097705\n",
      "Epoch: 210 - Loss: 0.35329071500977155\n",
      "Epoch: 211 - Loss: 0.35329071500977244\n",
      "Epoch: 212 - Loss: 0.3532907150097732\n",
      "Epoch: 213 - Loss: 0.3532907150097739\n",
      "Epoch: 214 - Loss: 0.3532907150097745\n",
      "Epoch: 215 - Loss: 0.3532907150097751\n",
      "Epoch: 216 - Loss: 0.35329071500977555\n",
      "Epoch: 217 - Loss: 0.35329071500977594\n",
      "Epoch: 218 - Loss: 0.3532907150097763\n",
      "Epoch: 219 - Loss: 0.3532907150097766\n",
      "Epoch: 220 - Loss: 0.353290715009777\n",
      "Epoch: 221 - Loss: 0.35329071500977716\n",
      "Epoch: 222 - Loss: 0.3532907150097774\n",
      "Epoch: 223 - Loss: 0.35329071500977755\n",
      "Epoch: 224 - Loss: 0.35329071500977777\n",
      "Epoch: 225 - Loss: 0.3532907150097778\n",
      "Epoch: 226 - Loss: 0.353290715009778\n",
      "Epoch: 227 - Loss: 0.3532907150097782\n",
      "Epoch: 228 - Loss: 0.3532907150097783\n",
      "Epoch: 229 - Loss: 0.35329071500977843\n",
      "Epoch: 230 - Loss: 0.35329071500977843\n",
      "Epoch: 231 - Loss: 0.35329071500977843\n",
      "Epoch: 232 - Loss: 0.3532907150097786\n",
      "Epoch: 233 - Loss: 0.3532907150097786\n",
      "Epoch: 234 - Loss: 0.35329071500977866\n",
      "Epoch: 235 - Loss: 0.35329071500977877\n",
      "Epoch: 236 - Loss: 0.35329071500977877\n",
      "Epoch: 237 - Loss: 0.3532907150097788\n",
      "Epoch: 238 - Loss: 0.3532907150097788\n",
      "Epoch: 239 - Loss: 0.3532907150097788\n",
      "Epoch: 240 - Loss: 0.353290715009779\n",
      "Epoch: 241 - Loss: 0.3532907150097788\n",
      "Epoch: 242 - Loss: 0.3532907150097789\n",
      "Epoch: 243 - Loss: 0.3532907150097788\n",
      "Epoch: 244 - Loss: 0.353290715009779\n",
      "Epoch: 245 - Loss: 0.353290715009779\n",
      "Epoch: 246 - Loss: 0.353290715009779\n",
      "Epoch: 247 - Loss: 0.353290715009779\n",
      "Epoch: 248 - Loss: 0.353290715009779\n",
      "Epoch: 249 - Loss: 0.353290715009779\n",
      "Epoch: 250 - Loss: 0.353290715009779\n",
      "Epoch: 251 - Loss: 0.353290715009779\n",
      "Epoch: 252 - Loss: 0.35329071500977904\n",
      "Epoch: 253 - Loss: 0.35329071500977904\n",
      "Epoch: 254 - Loss: 0.353290715009779\n",
      "Epoch: 255 - Loss: 0.353290715009779\n",
      "Epoch: 256 - Loss: 0.353290715009779\n",
      "Epoch: 257 - Loss: 0.353290715009779\n",
      "Epoch: 258 - Loss: 0.35329071500977904\n",
      "Epoch: 259 - Loss: 0.35329071500977904\n",
      "Epoch: 260 - Loss: 0.35329071500977904\n",
      "Epoch: 261 - Loss: 0.35329071500977904\n",
      "Epoch: 262 - Loss: 0.353290715009779\n",
      "Epoch: 263 - Loss: 0.35329071500977904\n",
      "Epoch: 264 - Loss: 0.353290715009779\n",
      "Epoch: 265 - Loss: 0.353290715009779\n",
      "Epoch: 266 - Loss: 0.353290715009779\n",
      "Epoch: 267 - Loss: 0.353290715009779\n",
      "Epoch: 268 - Loss: 0.35329071500977904\n",
      "Epoch: 269 - Loss: 0.35329071500977904\n",
      "Epoch: 270 - Loss: 0.35329071500977904\n",
      "Epoch: 271 - Loss: 0.35329071500977904\n",
      "Epoch: 272 - Loss: 0.35329071500977904\n",
      "Epoch: 273 - Loss: 0.35329071500977904\n",
      "Epoch: 274 - Loss: 0.35329071500977904\n",
      "Epoch: 275 - Loss: 0.35329071500977904\n",
      "Epoch: 276 - Loss: 0.35329071500977904\n",
      "Epoch: 277 - Loss: 0.35329071500977904\n",
      "Epoch: 278 - Loss: 0.35329071500977904\n",
      "Epoch: 279 - Loss: 0.35329071500977904\n",
      "Epoch: 280 - Loss: 0.35329071500977904\n",
      "Epoch: 281 - Loss: 0.35329071500977904\n",
      "Epoch: 282 - Loss: 0.35329071500977904\n",
      "Epoch: 283 - Loss: 0.35329071500977904\n",
      "Epoch: 284 - Loss: 0.35329071500977904\n",
      "Epoch: 285 - Loss: 0.35329071500977904\n",
      "Epoch: 286 - Loss: 0.35329071500977904\n",
      "Epoch: 287 - Loss: 0.35329071500977904\n",
      "Epoch: 288 - Loss: 0.35329071500977904\n",
      "Epoch: 289 - Loss: 0.35329071500977904\n",
      "Epoch: 290 - Loss: 0.35329071500977904\n",
      "Epoch: 291 - Loss: 0.35329071500977904\n",
      "Epoch: 292 - Loss: 0.35329071500977904\n",
      "Epoch: 293 - Loss: 0.35329071500977904\n",
      "Epoch: 294 - Loss: 0.35329071500977904\n",
      "Epoch: 295 - Loss: 0.35329071500977904\n",
      "Epoch: 296 - Loss: 0.35329071500977904\n",
      "Epoch: 297 - Loss: 0.35329071500977904\n",
      "Epoch: 298 - Loss: 0.35329071500977904\n",
      "Epoch: 299 - Loss: 0.35329071500977904\n",
      "Epoch: 300 - Loss: 0.35329071500977904\n",
      "Epoch: 301 - Loss: 0.35329071500977904\n",
      "Epoch: 302 - Loss: 0.35329071500977904\n",
      "Epoch: 303 - Loss: 0.35329071500977904\n",
      "Epoch: 304 - Loss: 0.35329071500977904\n",
      "Epoch: 305 - Loss: 0.35329071500977904\n",
      "Epoch: 306 - Loss: 0.35329071500977904\n",
      "Epoch: 307 - Loss: 0.35329071500977904\n",
      "Epoch: 308 - Loss: 0.35329071500977904\n",
      "Epoch: 309 - Loss: 0.35329071500977904\n",
      "Epoch: 310 - Loss: 0.35329071500977904\n",
      "Epoch: 311 - Loss: 0.35329071500977904\n",
      "Epoch: 312 - Loss: 0.35329071500977904\n",
      "Epoch: 313 - Loss: 0.35329071500977904\n",
      "Epoch: 314 - Loss: 0.35329071500977904\n",
      "Epoch: 315 - Loss: 0.35329071500977904\n",
      "Epoch: 316 - Loss: 0.35329071500977904\n",
      "Epoch: 317 - Loss: 0.35329071500977904\n",
      "Epoch: 318 - Loss: 0.35329071500977904\n",
      "Epoch: 319 - Loss: 0.35329071500977904\n",
      "Epoch: 320 - Loss: 0.35329071500977904\n",
      "Epoch: 321 - Loss: 0.35329071500977904\n",
      "Epoch: 322 - Loss: 0.35329071500977904\n",
      "Epoch: 323 - Loss: 0.35329071500977904\n",
      "Epoch: 324 - Loss: 0.35329071500977904\n",
      "Epoch: 325 - Loss: 0.35329071500977904\n",
      "Epoch: 326 - Loss: 0.35329071500977904\n",
      "Epoch: 327 - Loss: 0.35329071500977904\n",
      "Epoch: 328 - Loss: 0.35329071500977904\n",
      "Epoch: 329 - Loss: 0.35329071500977904\n",
      "Epoch: 330 - Loss: 0.35329071500977904\n",
      "Epoch: 331 - Loss: 0.35329071500977904\n",
      "Epoch: 332 - Loss: 0.35329071500977904\n",
      "Epoch: 333 - Loss: 0.35329071500977904\n",
      "Epoch: 334 - Loss: 0.35329071500977904\n",
      "Epoch: 335 - Loss: 0.35329071500977904\n",
      "Epoch: 336 - Loss: 0.35329071500977904\n",
      "Epoch: 337 - Loss: 0.35329071500977904\n",
      "Epoch: 338 - Loss: 0.35329071500977904\n",
      "Epoch: 339 - Loss: 0.35329071500977904\n",
      "Epoch: 340 - Loss: 0.35329071500977904\n",
      "Epoch: 341 - Loss: 0.35329071500977904\n",
      "Epoch: 342 - Loss: 0.35329071500977904\n",
      "Epoch: 343 - Loss: 0.35329071500977904\n",
      "Epoch: 344 - Loss: 0.35329071500977904\n",
      "Epoch: 345 - Loss: 0.35329071500977904\n",
      "Epoch: 346 - Loss: 0.35329071500977904\n",
      "Epoch: 347 - Loss: 0.35329071500977904\n",
      "Epoch: 348 - Loss: 0.35329071500977904\n",
      "Epoch: 349 - Loss: 0.35329071500977904\n",
      "Epoch: 350 - Loss: 0.35329071500977904\n",
      "Epoch: 351 - Loss: 0.35329071500977904\n",
      "Epoch: 352 - Loss: 0.35329071500977904\n",
      "Epoch: 353 - Loss: 0.35329071500977904\n",
      "Epoch: 354 - Loss: 0.35329071500977904\n",
      "Epoch: 355 - Loss: 0.35329071500977904\n",
      "Epoch: 356 - Loss: 0.35329071500977904\n",
      "Epoch: 357 - Loss: 0.35329071500977904\n",
      "Epoch: 358 - Loss: 0.35329071500977904\n",
      "Epoch: 359 - Loss: 0.35329071500977904\n",
      "Epoch: 360 - Loss: 0.35329071500977904\n",
      "Epoch: 361 - Loss: 0.35329071500977904\n",
      "Epoch: 362 - Loss: 0.35329071500977904\n",
      "Epoch: 363 - Loss: 0.35329071500977904\n",
      "Epoch: 364 - Loss: 0.35329071500977904\n",
      "Epoch: 365 - Loss: 0.35329071500977904\n",
      "Epoch: 366 - Loss: 0.35329071500977904\n",
      "Epoch: 367 - Loss: 0.35329071500977904\n",
      "Epoch: 368 - Loss: 0.35329071500977904\n",
      "Epoch: 369 - Loss: 0.35329071500977904\n",
      "Epoch: 370 - Loss: 0.35329071500977904\n",
      "Epoch: 371 - Loss: 0.35329071500977904\n",
      "Epoch: 372 - Loss: 0.35329071500977904\n",
      "Epoch: 373 - Loss: 0.35329071500977904\n",
      "Epoch: 374 - Loss: 0.35329071500977904\n",
      "Epoch: 375 - Loss: 0.35329071500977904\n",
      "Epoch: 376 - Loss: 0.35329071500977904\n",
      "Epoch: 377 - Loss: 0.35329071500977904\n",
      "Epoch: 378 - Loss: 0.35329071500977904\n",
      "Epoch: 379 - Loss: 0.35329071500977904\n",
      "Epoch: 380 - Loss: 0.35329071500977904\n",
      "Epoch: 381 - Loss: 0.35329071500977904\n",
      "Epoch: 382 - Loss: 0.35329071500977904\n",
      "Epoch: 383 - Loss: 0.35329071500977904\n",
      "Epoch: 384 - Loss: 0.35329071500977904\n",
      "Epoch: 385 - Loss: 0.35329071500977904\n",
      "Epoch: 386 - Loss: 0.35329071500977904\n",
      "Epoch: 387 - Loss: 0.35329071500977904\n",
      "Epoch: 388 - Loss: 0.35329071500977904\n",
      "Epoch: 389 - Loss: 0.35329071500977904\n",
      "Epoch: 390 - Loss: 0.35329071500977904\n",
      "Epoch: 391 - Loss: 0.35329071500977904\n",
      "Epoch: 392 - Loss: 0.35329071500977904\n",
      "Epoch: 393 - Loss: 0.35329071500977904\n",
      "Epoch: 394 - Loss: 0.35329071500977904\n",
      "Epoch: 395 - Loss: 0.35329071500977904\n",
      "Epoch: 396 - Loss: 0.35329071500977904\n",
      "Epoch: 397 - Loss: 0.35329071500977904\n",
      "Epoch: 398 - Loss: 0.35329071500977904\n",
      "Epoch: 399 - Loss: 0.35329071500977904\n",
      "Epoch: 400 - Loss: 0.35329071500977904\n",
      "Epoch: 401 - Loss: 0.35329071500977904\n",
      "Epoch: 402 - Loss: 0.35329071500977904\n",
      "Epoch: 403 - Loss: 0.35329071500977904\n",
      "Epoch: 404 - Loss: 0.35329071500977904\n",
      "Epoch: 405 - Loss: 0.35329071500977904\n",
      "Epoch: 406 - Loss: 0.35329071500977904\n",
      "Epoch: 407 - Loss: 0.35329071500977904\n",
      "Epoch: 408 - Loss: 0.35329071500977904\n",
      "Epoch: 409 - Loss: 0.35329071500977904\n",
      "Epoch: 410 - Loss: 0.35329071500977904\n",
      "Epoch: 411 - Loss: 0.35329071500977904\n",
      "Epoch: 412 - Loss: 0.35329071500977904\n",
      "Epoch: 413 - Loss: 0.35329071500977904\n",
      "Epoch: 414 - Loss: 0.35329071500977904\n",
      "Epoch: 415 - Loss: 0.35329071500977904\n",
      "Epoch: 416 - Loss: 0.35329071500977904\n",
      "Epoch: 417 - Loss: 0.35329071500977904\n",
      "Epoch: 418 - Loss: 0.35329071500977904\n",
      "Epoch: 419 - Loss: 0.35329071500977904\n",
      "Epoch: 420 - Loss: 0.35329071500977904\n",
      "Epoch: 421 - Loss: 0.35329071500977904\n",
      "Epoch: 422 - Loss: 0.35329071500977904\n",
      "Epoch: 423 - Loss: 0.35329071500977904\n",
      "Epoch: 424 - Loss: 0.35329071500977904\n",
      "Epoch: 425 - Loss: 0.35329071500977904\n",
      "Epoch: 426 - Loss: 0.35329071500977904\n",
      "Epoch: 427 - Loss: 0.35329071500977904\n",
      "Epoch: 428 - Loss: 0.35329071500977904\n",
      "Epoch: 429 - Loss: 0.35329071500977904\n",
      "Epoch: 430 - Loss: 0.35329071500977904\n",
      "Epoch: 431 - Loss: 0.35329071500977904\n",
      "Epoch: 432 - Loss: 0.35329071500977904\n",
      "Epoch: 433 - Loss: 0.35329071500977904\n",
      "Epoch: 434 - Loss: 0.35329071500977904\n",
      "Epoch: 435 - Loss: 0.35329071500977904\n",
      "Epoch: 436 - Loss: 0.35329071500977904\n",
      "Epoch: 437 - Loss: 0.35329071500977904\n",
      "Epoch: 438 - Loss: 0.35329071500977904\n",
      "Epoch: 439 - Loss: 0.35329071500977904\n",
      "Epoch: 440 - Loss: 0.35329071500977904\n",
      "Epoch: 441 - Loss: 0.35329071500977904\n",
      "Epoch: 442 - Loss: 0.35329071500977904\n",
      "Epoch: 443 - Loss: 0.35329071500977904\n",
      "Epoch: 444 - Loss: 0.35329071500977904\n",
      "Epoch: 445 - Loss: 0.35329071500977904\n",
      "Epoch: 446 - Loss: 0.35329071500977904\n",
      "Epoch: 447 - Loss: 0.35329071500977904\n",
      "Epoch: 448 - Loss: 0.35329071500977904\n",
      "Epoch: 449 - Loss: 0.35329071500977904\n",
      "Epoch: 450 - Loss: 0.35329071500977904\n",
      "Epoch: 451 - Loss: 0.35329071500977904\n",
      "Epoch: 452 - Loss: 0.35329071500977904\n",
      "Epoch: 453 - Loss: 0.35329071500977904\n",
      "Epoch: 454 - Loss: 0.35329071500977904\n",
      "Epoch: 455 - Loss: 0.35329071500977904\n",
      "Epoch: 456 - Loss: 0.35329071500977904\n",
      "Epoch: 457 - Loss: 0.35329071500977904\n",
      "Epoch: 458 - Loss: 0.35329071500977904\n",
      "Epoch: 459 - Loss: 0.35329071500977904\n",
      "Epoch: 460 - Loss: 0.35329071500977904\n",
      "Epoch: 461 - Loss: 0.35329071500977904\n",
      "Epoch: 462 - Loss: 0.35329071500977904\n",
      "Epoch: 463 - Loss: 0.35329071500977904\n",
      "Epoch: 464 - Loss: 0.35329071500977904\n",
      "Epoch: 465 - Loss: 0.35329071500977904\n",
      "Epoch: 466 - Loss: 0.35329071500977904\n",
      "Epoch: 467 - Loss: 0.35329071500977904\n",
      "Epoch: 468 - Loss: 0.35329071500977904\n",
      "Epoch: 469 - Loss: 0.35329071500977904\n",
      "Epoch: 470 - Loss: 0.35329071500977904\n",
      "Epoch: 471 - Loss: 0.35329071500977904\n",
      "Epoch: 472 - Loss: 0.35329071500977904\n",
      "Epoch: 473 - Loss: 0.35329071500977904\n",
      "Epoch: 474 - Loss: 0.35329071500977904\n",
      "Epoch: 475 - Loss: 0.35329071500977904\n",
      "Epoch: 476 - Loss: 0.35329071500977904\n",
      "Epoch: 477 - Loss: 0.35329071500977904\n",
      "Epoch: 478 - Loss: 0.35329071500977904\n",
      "Epoch: 479 - Loss: 0.35329071500977904\n",
      "Epoch: 480 - Loss: 0.35329071500977904\n",
      "Epoch: 481 - Loss: 0.35329071500977904\n",
      "Epoch: 482 - Loss: 0.35329071500977904\n",
      "Epoch: 483 - Loss: 0.35329071500977904\n",
      "Epoch: 484 - Loss: 0.35329071500977904\n",
      "Epoch: 485 - Loss: 0.35329071500977904\n",
      "Epoch: 486 - Loss: 0.35329071500977904\n",
      "Epoch: 487 - Loss: 0.35329071500977904\n",
      "Epoch: 488 - Loss: 0.35329071500977904\n",
      "Epoch: 489 - Loss: 0.35329071500977904\n",
      "Epoch: 490 - Loss: 0.35329071500977904\n",
      "Epoch: 491 - Loss: 0.35329071500977904\n",
      "Epoch: 492 - Loss: 0.35329071500977904\n",
      "Epoch: 493 - Loss: 0.35329071500977904\n",
      "Epoch: 494 - Loss: 0.35329071500977904\n",
      "Epoch: 495 - Loss: 0.35329071500977904\n",
      "Epoch: 496 - Loss: 0.35329071500977904\n",
      "Epoch: 497 - Loss: 0.35329071500977904\n",
      "Epoch: 498 - Loss: 0.35329071500977904\n",
      "Epoch: 499 - Loss: 0.35329071500977904\n",
      "Epoch: 500 - Loss: 0.35329071500977904\n",
      "Epoch: 501 - Loss: 0.35329071500977904\n",
      "Epoch: 502 - Loss: 0.35329071500977904\n",
      "Epoch: 503 - Loss: 0.35329071500977904\n",
      "Epoch: 504 - Loss: 0.35329071500977904\n",
      "Epoch: 505 - Loss: 0.35329071500977904\n",
      "Epoch: 506 - Loss: 0.35329071500977904\n",
      "Epoch: 507 - Loss: 0.35329071500977904\n",
      "Epoch: 508 - Loss: 0.35329071500977904\n",
      "Epoch: 509 - Loss: 0.35329071500977904\n",
      "Epoch: 510 - Loss: 0.35329071500977904\n",
      "Epoch: 511 - Loss: 0.35329071500977904\n",
      "Epoch: 512 - Loss: 0.35329071500977904\n",
      "Epoch: 513 - Loss: 0.35329071500977904\n",
      "Epoch: 514 - Loss: 0.35329071500977904\n",
      "Epoch: 515 - Loss: 0.35329071500977904\n",
      "Epoch: 516 - Loss: 0.35329071500977904\n",
      "Epoch: 517 - Loss: 0.35329071500977904\n",
      "Epoch: 518 - Loss: 0.35329071500977904\n",
      "Epoch: 519 - Loss: 0.35329071500977904\n",
      "Epoch: 520 - Loss: 0.35329071500977904\n",
      "Epoch: 521 - Loss: 0.35329071500977904\n",
      "Epoch: 522 - Loss: 0.35329071500977904\n",
      "Epoch: 523 - Loss: 0.35329071500977904\n",
      "Epoch: 524 - Loss: 0.35329071500977904\n",
      "Epoch: 525 - Loss: 0.35329071500977904\n",
      "Epoch: 526 - Loss: 0.35329071500977904\n",
      "Epoch: 527 - Loss: 0.35329071500977904\n",
      "Epoch: 528 - Loss: 0.35329071500977904\n",
      "Epoch: 529 - Loss: 0.35329071500977904\n",
      "Epoch: 530 - Loss: 0.35329071500977904\n",
      "Epoch: 531 - Loss: 0.35329071500977904\n",
      "Epoch: 532 - Loss: 0.35329071500977904\n",
      "Epoch: 533 - Loss: 0.35329071500977904\n",
      "Epoch: 534 - Loss: 0.35329071500977904\n",
      "Epoch: 535 - Loss: 0.35329071500977904\n",
      "Epoch: 536 - Loss: 0.35329071500977904\n",
      "Epoch: 537 - Loss: 0.35329071500977904\n",
      "Epoch: 538 - Loss: 0.35329071500977904\n",
      "Epoch: 539 - Loss: 0.35329071500977904\n",
      "Epoch: 540 - Loss: 0.35329071500977904\n",
      "Epoch: 541 - Loss: 0.35329071500977904\n",
      "Epoch: 542 - Loss: 0.35329071500977904\n",
      "Epoch: 543 - Loss: 0.35329071500977904\n",
      "Epoch: 544 - Loss: 0.35329071500977904\n",
      "Epoch: 545 - Loss: 0.35329071500977904\n",
      "Epoch: 546 - Loss: 0.35329071500977904\n",
      "Epoch: 547 - Loss: 0.35329071500977904\n",
      "Epoch: 548 - Loss: 0.35329071500977904\n",
      "Epoch: 549 - Loss: 0.35329071500977904\n",
      "Epoch: 550 - Loss: 0.35329071500977904\n",
      "Epoch: 551 - Loss: 0.35329071500977904\n",
      "Epoch: 552 - Loss: 0.35329071500977904\n",
      "Epoch: 553 - Loss: 0.35329071500977904\n",
      "Epoch: 554 - Loss: 0.35329071500977904\n",
      "Epoch: 555 - Loss: 0.35329071500977904\n",
      "Epoch: 556 - Loss: 0.35329071500977904\n",
      "Epoch: 557 - Loss: 0.35329071500977904\n",
      "Epoch: 558 - Loss: 0.35329071500977904\n",
      "Epoch: 559 - Loss: 0.35329071500977904\n",
      "Epoch: 560 - Loss: 0.35329071500977904\n",
      "Epoch: 561 - Loss: 0.35329071500977904\n",
      "Epoch: 562 - Loss: 0.35329071500977904\n",
      "Epoch: 563 - Loss: 0.35329071500977904\n",
      "Epoch: 564 - Loss: 0.35329071500977904\n",
      "Epoch: 565 - Loss: 0.35329071500977904\n",
      "Epoch: 566 - Loss: 0.35329071500977904\n",
      "Epoch: 567 - Loss: 0.35329071500977904\n",
      "Epoch: 568 - Loss: 0.35329071500977904\n",
      "Epoch: 569 - Loss: 0.35329071500977904\n",
      "Epoch: 570 - Loss: 0.35329071500977904\n",
      "Epoch: 571 - Loss: 0.35329071500977904\n",
      "Epoch: 572 - Loss: 0.35329071500977904\n",
      "Epoch: 573 - Loss: 0.35329071500977904\n",
      "Epoch: 574 - Loss: 0.35329071500977904\n",
      "Epoch: 575 - Loss: 0.35329071500977904\n",
      "Epoch: 576 - Loss: 0.35329071500977904\n",
      "Epoch: 577 - Loss: 0.35329071500977904\n",
      "Epoch: 578 - Loss: 0.35329071500977904\n",
      "Epoch: 579 - Loss: 0.35329071500977904\n",
      "Epoch: 580 - Loss: 0.35329071500977904\n",
      "Epoch: 581 - Loss: 0.35329071500977904\n",
      "Epoch: 582 - Loss: 0.35329071500977904\n",
      "Epoch: 583 - Loss: 0.35329071500977904\n",
      "Epoch: 584 - Loss: 0.35329071500977904\n",
      "Epoch: 585 - Loss: 0.35329071500977904\n",
      "Epoch: 586 - Loss: 0.35329071500977904\n",
      "Epoch: 587 - Loss: 0.35329071500977904\n",
      "Epoch: 588 - Loss: 0.35329071500977904\n",
      "Epoch: 589 - Loss: 0.35329071500977904\n",
      "Epoch: 590 - Loss: 0.35329071500977904\n",
      "Epoch: 591 - Loss: 0.35329071500977904\n",
      "Epoch: 592 - Loss: 0.35329071500977904\n",
      "Epoch: 593 - Loss: 0.35329071500977904\n",
      "Epoch: 594 - Loss: 0.35329071500977904\n",
      "Epoch: 595 - Loss: 0.35329071500977904\n",
      "Epoch: 596 - Loss: 0.35329071500977904\n",
      "Epoch: 597 - Loss: 0.35329071500977904\n",
      "Epoch: 598 - Loss: 0.35329071500977904\n",
      "Epoch: 599 - Loss: 0.35329071500977904\n",
      "Epoch: 600 - Loss: 0.35329071500977904\n",
      "Epoch: 601 - Loss: 0.35329071500977904\n",
      "Epoch: 602 - Loss: 0.35329071500977904\n",
      "Epoch: 603 - Loss: 0.35329071500977904\n",
      "Epoch: 604 - Loss: 0.35329071500977904\n",
      "Epoch: 605 - Loss: 0.35329071500977904\n",
      "Epoch: 606 - Loss: 0.35329071500977904\n",
      "Epoch: 607 - Loss: 0.35329071500977904\n",
      "Epoch: 608 - Loss: 0.35329071500977904\n",
      "Epoch: 609 - Loss: 0.35329071500977904\n",
      "Epoch: 610 - Loss: 0.35329071500977904\n",
      "Epoch: 611 - Loss: 0.35329071500977904\n",
      "Epoch: 612 - Loss: 0.35329071500977904\n",
      "Epoch: 613 - Loss: 0.35329071500977904\n",
      "Epoch: 614 - Loss: 0.35329071500977904\n",
      "Epoch: 615 - Loss: 0.35329071500977904\n",
      "Epoch: 616 - Loss: 0.35329071500977904\n",
      "Epoch: 617 - Loss: 0.35329071500977904\n",
      "Epoch: 618 - Loss: 0.35329071500977904\n",
      "Epoch: 619 - Loss: 0.35329071500977904\n",
      "Epoch: 620 - Loss: 0.35329071500977904\n",
      "Epoch: 621 - Loss: 0.35329071500977904\n",
      "Epoch: 622 - Loss: 0.35329071500977904\n",
      "Epoch: 623 - Loss: 0.35329071500977904\n",
      "Epoch: 624 - Loss: 0.35329071500977904\n",
      "Epoch: 625 - Loss: 0.35329071500977904\n",
      "Epoch: 626 - Loss: 0.35329071500977904\n",
      "Epoch: 627 - Loss: 0.35329071500977904\n",
      "Epoch: 628 - Loss: 0.35329071500977904\n",
      "Epoch: 629 - Loss: 0.35329071500977904\n",
      "Epoch: 630 - Loss: 0.35329071500977904\n",
      "Epoch: 631 - Loss: 0.35329071500977904\n",
      "Epoch: 632 - Loss: 0.35329071500977904\n",
      "Epoch: 633 - Loss: 0.35329071500977904\n",
      "Epoch: 634 - Loss: 0.35329071500977904\n",
      "Epoch: 635 - Loss: 0.35329071500977904\n",
      "Epoch: 636 - Loss: 0.35329071500977904\n",
      "Epoch: 637 - Loss: 0.35329071500977904\n",
      "Epoch: 638 - Loss: 0.35329071500977904\n",
      "Epoch: 639 - Loss: 0.35329071500977904\n",
      "Epoch: 640 - Loss: 0.35329071500977904\n",
      "Epoch: 641 - Loss: 0.35329071500977904\n",
      "Epoch: 642 - Loss: 0.35329071500977904\n",
      "Epoch: 643 - Loss: 0.35329071500977904\n",
      "Epoch: 644 - Loss: 0.35329071500977904\n",
      "Epoch: 645 - Loss: 0.35329071500977904\n",
      "Epoch: 646 - Loss: 0.35329071500977904\n",
      "Epoch: 647 - Loss: 0.35329071500977904\n",
      "Epoch: 648 - Loss: 0.35329071500977904\n",
      "Epoch: 649 - Loss: 0.35329071500977904\n",
      "Epoch: 650 - Loss: 0.35329071500977904\n",
      "Epoch: 651 - Loss: 0.35329071500977904\n",
      "Epoch: 652 - Loss: 0.35329071500977904\n",
      "Epoch: 653 - Loss: 0.35329071500977904\n",
      "Epoch: 654 - Loss: 0.35329071500977904\n",
      "Epoch: 655 - Loss: 0.35329071500977904\n",
      "Epoch: 656 - Loss: 0.35329071500977904\n",
      "Epoch: 657 - Loss: 0.35329071500977904\n",
      "Epoch: 658 - Loss: 0.35329071500977904\n",
      "Epoch: 659 - Loss: 0.35329071500977904\n",
      "Epoch: 660 - Loss: 0.35329071500977904\n",
      "Epoch: 661 - Loss: 0.35329071500977904\n",
      "Epoch: 662 - Loss: 0.35329071500977904\n",
      "Epoch: 663 - Loss: 0.35329071500977904\n",
      "Epoch: 664 - Loss: 0.35329071500977904\n",
      "Epoch: 665 - Loss: 0.35329071500977904\n",
      "Epoch: 666 - Loss: 0.35329071500977904\n",
      "Epoch: 667 - Loss: 0.35329071500977904\n",
      "Epoch: 668 - Loss: 0.35329071500977904\n",
      "Epoch: 669 - Loss: 0.35329071500977904\n",
      "Epoch: 670 - Loss: 0.35329071500977904\n",
      "Epoch: 671 - Loss: 0.35329071500977904\n",
      "Epoch: 672 - Loss: 0.35329071500977904\n",
      "Epoch: 673 - Loss: 0.35329071500977904\n",
      "Epoch: 674 - Loss: 0.35329071500977904\n",
      "Epoch: 675 - Loss: 0.35329071500977904\n",
      "Epoch: 676 - Loss: 0.35329071500977904\n",
      "Epoch: 677 - Loss: 0.35329071500977904\n",
      "Epoch: 678 - Loss: 0.35329071500977904\n",
      "Epoch: 679 - Loss: 0.35329071500977904\n",
      "Epoch: 680 - Loss: 0.35329071500977904\n",
      "Epoch: 681 - Loss: 0.35329071500977904\n",
      "Epoch: 682 - Loss: 0.35329071500977904\n",
      "Epoch: 683 - Loss: 0.35329071500977904\n",
      "Epoch: 684 - Loss: 0.35329071500977904\n",
      "Epoch: 685 - Loss: 0.35329071500977904\n",
      "Epoch: 686 - Loss: 0.35329071500977904\n",
      "Epoch: 687 - Loss: 0.35329071500977904\n",
      "Epoch: 688 - Loss: 0.35329071500977904\n",
      "Epoch: 689 - Loss: 0.35329071500977904\n",
      "Epoch: 690 - Loss: 0.35329071500977904\n",
      "Epoch: 691 - Loss: 0.35329071500977904\n",
      "Epoch: 692 - Loss: 0.35329071500977904\n",
      "Epoch: 693 - Loss: 0.35329071500977904\n",
      "Epoch: 694 - Loss: 0.35329071500977904\n",
      "Epoch: 695 - Loss: 0.35329071500977904\n",
      "Epoch: 696 - Loss: 0.35329071500977904\n",
      "Epoch: 697 - Loss: 0.35329071500977904\n",
      "Epoch: 698 - Loss: 0.35329071500977904\n",
      "Epoch: 699 - Loss: 0.35329071500977904\n",
      "Epoch: 700 - Loss: 0.35329071500977904\n",
      "Epoch: 701 - Loss: 0.35329071500977904\n",
      "Epoch: 702 - Loss: 0.35329071500977904\n",
      "Epoch: 703 - Loss: 0.35329071500977904\n",
      "Epoch: 704 - Loss: 0.35329071500977904\n",
      "Epoch: 705 - Loss: 0.35329071500977904\n",
      "Epoch: 706 - Loss: 0.35329071500977904\n",
      "Epoch: 707 - Loss: 0.35329071500977904\n",
      "Epoch: 708 - Loss: 0.35329071500977904\n",
      "Epoch: 709 - Loss: 0.35329071500977904\n",
      "Epoch: 710 - Loss: 0.35329071500977904\n",
      "Epoch: 711 - Loss: 0.35329071500977904\n",
      "Epoch: 712 - Loss: 0.35329071500977904\n",
      "Epoch: 713 - Loss: 0.35329071500977904\n",
      "Epoch: 714 - Loss: 0.35329071500977904\n",
      "Epoch: 715 - Loss: 0.35329071500977904\n",
      "Epoch: 716 - Loss: 0.35329071500977904\n",
      "Epoch: 717 - Loss: 0.35329071500977904\n",
      "Epoch: 718 - Loss: 0.35329071500977904\n",
      "Epoch: 719 - Loss: 0.35329071500977904\n",
      "Epoch: 720 - Loss: 0.35329071500977904\n",
      "Epoch: 721 - Loss: 0.35329071500977904\n",
      "Epoch: 722 - Loss: 0.35329071500977904\n",
      "Epoch: 723 - Loss: 0.35329071500977904\n",
      "Epoch: 724 - Loss: 0.35329071500977904\n",
      "Epoch: 725 - Loss: 0.35329071500977904\n",
      "Epoch: 726 - Loss: 0.35329071500977904\n",
      "Epoch: 727 - Loss: 0.35329071500977904\n",
      "Epoch: 728 - Loss: 0.35329071500977904\n",
      "Epoch: 729 - Loss: 0.35329071500977904\n",
      "Epoch: 730 - Loss: 0.35329071500977904\n",
      "Epoch: 731 - Loss: 0.35329071500977904\n",
      "Epoch: 732 - Loss: 0.35329071500977904\n",
      "Epoch: 733 - Loss: 0.35329071500977904\n",
      "Epoch: 734 - Loss: 0.35329071500977904\n",
      "Epoch: 735 - Loss: 0.35329071500977904\n",
      "Epoch: 736 - Loss: 0.35329071500977904\n",
      "Epoch: 737 - Loss: 0.35329071500977904\n",
      "Epoch: 738 - Loss: 0.35329071500977904\n",
      "Epoch: 739 - Loss: 0.35329071500977904\n",
      "Epoch: 740 - Loss: 0.35329071500977904\n",
      "Epoch: 741 - Loss: 0.35329071500977904\n",
      "Epoch: 742 - Loss: 0.35329071500977904\n",
      "Epoch: 743 - Loss: 0.35329071500977904\n",
      "Epoch: 744 - Loss: 0.35329071500977904\n",
      "Epoch: 745 - Loss: 0.35329071500977904\n",
      "Epoch: 746 - Loss: 0.35329071500977904\n",
      "Epoch: 747 - Loss: 0.35329071500977904\n",
      "Epoch: 748 - Loss: 0.35329071500977904\n",
      "Epoch: 749 - Loss: 0.35329071500977904\n",
      "Epoch: 750 - Loss: 0.35329071500977904\n",
      "Epoch: 751 - Loss: 0.35329071500977904\n",
      "Epoch: 752 - Loss: 0.35329071500977904\n",
      "Epoch: 753 - Loss: 0.35329071500977904\n",
      "Epoch: 754 - Loss: 0.35329071500977904\n",
      "Epoch: 755 - Loss: 0.35329071500977904\n",
      "Epoch: 756 - Loss: 0.35329071500977904\n",
      "Epoch: 757 - Loss: 0.35329071500977904\n",
      "Epoch: 758 - Loss: 0.35329071500977904\n",
      "Epoch: 759 - Loss: 0.35329071500977904\n",
      "Epoch: 760 - Loss: 0.35329071500977904\n",
      "Epoch: 761 - Loss: 0.35329071500977904\n",
      "Epoch: 762 - Loss: 0.35329071500977904\n",
      "Epoch: 763 - Loss: 0.35329071500977904\n",
      "Epoch: 764 - Loss: 0.35329071500977904\n",
      "Epoch: 765 - Loss: 0.35329071500977904\n",
      "Epoch: 766 - Loss: 0.35329071500977904\n",
      "Epoch: 767 - Loss: 0.35329071500977904\n",
      "Epoch: 768 - Loss: 0.35329071500977904\n",
      "Epoch: 769 - Loss: 0.35329071500977904\n",
      "Epoch: 770 - Loss: 0.35329071500977904\n",
      "Epoch: 771 - Loss: 0.35329071500977904\n",
      "Epoch: 772 - Loss: 0.35329071500977904\n",
      "Epoch: 773 - Loss: 0.35329071500977904\n",
      "Epoch: 774 - Loss: 0.35329071500977904\n",
      "Epoch: 775 - Loss: 0.35329071500977904\n",
      "Epoch: 776 - Loss: 0.35329071500977904\n",
      "Epoch: 777 - Loss: 0.35329071500977904\n",
      "Epoch: 778 - Loss: 0.35329071500977904\n",
      "Epoch: 779 - Loss: 0.35329071500977904\n",
      "Epoch: 780 - Loss: 0.35329071500977904\n",
      "Epoch: 781 - Loss: 0.35329071500977904\n",
      "Epoch: 782 - Loss: 0.35329071500977904\n",
      "Epoch: 783 - Loss: 0.35329071500977904\n",
      "Epoch: 784 - Loss: 0.35329071500977904\n",
      "Epoch: 785 - Loss: 0.35329071500977904\n",
      "Epoch: 786 - Loss: 0.35329071500977904\n",
      "Epoch: 787 - Loss: 0.35329071500977904\n",
      "Epoch: 788 - Loss: 0.35329071500977904\n",
      "Epoch: 789 - Loss: 0.35329071500977904\n",
      "Epoch: 790 - Loss: 0.35329071500977904\n",
      "Epoch: 791 - Loss: 0.35329071500977904\n",
      "Epoch: 792 - Loss: 0.35329071500977904\n",
      "Epoch: 793 - Loss: 0.35329071500977904\n",
      "Epoch: 794 - Loss: 0.35329071500977904\n",
      "Epoch: 795 - Loss: 0.35329071500977904\n",
      "Epoch: 796 - Loss: 0.35329071500977904\n",
      "Epoch: 797 - Loss: 0.35329071500977904\n",
      "Epoch: 798 - Loss: 0.35329071500977904\n",
      "Epoch: 799 - Loss: 0.35329071500977904\n",
      "Epoch: 800 - Loss: 0.35329071500977904\n",
      "Epoch: 801 - Loss: 0.35329071500977904\n",
      "Epoch: 802 - Loss: 0.35329071500977904\n",
      "Epoch: 803 - Loss: 0.35329071500977904\n",
      "Epoch: 804 - Loss: 0.35329071500977904\n",
      "Epoch: 805 - Loss: 0.35329071500977904\n",
      "Epoch: 806 - Loss: 0.35329071500977904\n",
      "Epoch: 807 - Loss: 0.35329071500977904\n",
      "Epoch: 808 - Loss: 0.35329071500977904\n",
      "Epoch: 809 - Loss: 0.35329071500977904\n",
      "Epoch: 810 - Loss: 0.35329071500977904\n",
      "Epoch: 811 - Loss: 0.35329071500977904\n",
      "Epoch: 812 - Loss: 0.35329071500977904\n",
      "Epoch: 813 - Loss: 0.35329071500977904\n",
      "Epoch: 814 - Loss: 0.35329071500977904\n",
      "Epoch: 815 - Loss: 0.35329071500977904\n",
      "Epoch: 816 - Loss: 0.35329071500977904\n",
      "Epoch: 817 - Loss: 0.35329071500977904\n",
      "Epoch: 818 - Loss: 0.35329071500977904\n",
      "Epoch: 819 - Loss: 0.35329071500977904\n",
      "Epoch: 820 - Loss: 0.35329071500977904\n",
      "Epoch: 821 - Loss: 0.35329071500977904\n",
      "Epoch: 822 - Loss: 0.35329071500977904\n",
      "Epoch: 823 - Loss: 0.35329071500977904\n",
      "Epoch: 824 - Loss: 0.35329071500977904\n",
      "Epoch: 825 - Loss: 0.35329071500977904\n",
      "Epoch: 826 - Loss: 0.35329071500977904\n",
      "Epoch: 827 - Loss: 0.35329071500977904\n",
      "Epoch: 828 - Loss: 0.35329071500977904\n",
      "Epoch: 829 - Loss: 0.35329071500977904\n",
      "Epoch: 830 - Loss: 0.35329071500977904\n",
      "Epoch: 831 - Loss: 0.35329071500977904\n",
      "Epoch: 832 - Loss: 0.35329071500977904\n",
      "Epoch: 833 - Loss: 0.35329071500977904\n",
      "Epoch: 834 - Loss: 0.35329071500977904\n",
      "Epoch: 835 - Loss: 0.35329071500977904\n",
      "Epoch: 836 - Loss: 0.35329071500977904\n",
      "Epoch: 837 - Loss: 0.35329071500977904\n",
      "Epoch: 838 - Loss: 0.35329071500977904\n",
      "Epoch: 839 - Loss: 0.35329071500977904\n",
      "Epoch: 840 - Loss: 0.35329071500977904\n",
      "Epoch: 841 - Loss: 0.35329071500977904\n",
      "Epoch: 842 - Loss: 0.35329071500977904\n",
      "Epoch: 843 - Loss: 0.35329071500977904\n",
      "Epoch: 844 - Loss: 0.35329071500977904\n",
      "Epoch: 845 - Loss: 0.35329071500977904\n",
      "Epoch: 846 - Loss: 0.35329071500977904\n",
      "Epoch: 847 - Loss: 0.35329071500977904\n",
      "Epoch: 848 - Loss: 0.35329071500977904\n",
      "Epoch: 849 - Loss: 0.35329071500977904\n",
      "Epoch: 850 - Loss: 0.35329071500977904\n",
      "Epoch: 851 - Loss: 0.35329071500977904\n",
      "Epoch: 852 - Loss: 0.35329071500977904\n",
      "Epoch: 853 - Loss: 0.35329071500977904\n",
      "Epoch: 854 - Loss: 0.35329071500977904\n",
      "Epoch: 855 - Loss: 0.35329071500977904\n",
      "Epoch: 856 - Loss: 0.35329071500977904\n",
      "Epoch: 857 - Loss: 0.35329071500977904\n",
      "Epoch: 858 - Loss: 0.35329071500977904\n",
      "Epoch: 859 - Loss: 0.35329071500977904\n",
      "Epoch: 860 - Loss: 0.35329071500977904\n",
      "Epoch: 861 - Loss: 0.35329071500977904\n",
      "Epoch: 862 - Loss: 0.35329071500977904\n",
      "Epoch: 863 - Loss: 0.35329071500977904\n",
      "Epoch: 864 - Loss: 0.35329071500977904\n",
      "Epoch: 865 - Loss: 0.35329071500977904\n",
      "Epoch: 866 - Loss: 0.35329071500977904\n",
      "Epoch: 867 - Loss: 0.35329071500977904\n",
      "Epoch: 868 - Loss: 0.35329071500977904\n",
      "Epoch: 869 - Loss: 0.35329071500977904\n",
      "Epoch: 870 - Loss: 0.35329071500977904\n",
      "Epoch: 871 - Loss: 0.35329071500977904\n",
      "Epoch: 872 - Loss: 0.35329071500977904\n",
      "Epoch: 873 - Loss: 0.35329071500977904\n",
      "Epoch: 874 - Loss: 0.35329071500977904\n",
      "Epoch: 875 - Loss: 0.35329071500977904\n",
      "Epoch: 876 - Loss: 0.35329071500977904\n",
      "Epoch: 877 - Loss: 0.35329071500977904\n",
      "Epoch: 878 - Loss: 0.35329071500977904\n",
      "Epoch: 879 - Loss: 0.35329071500977904\n",
      "Epoch: 880 - Loss: 0.35329071500977904\n",
      "Epoch: 881 - Loss: 0.35329071500977904\n",
      "Epoch: 882 - Loss: 0.35329071500977904\n",
      "Epoch: 883 - Loss: 0.35329071500977904\n",
      "Epoch: 884 - Loss: 0.35329071500977904\n",
      "Epoch: 885 - Loss: 0.35329071500977904\n",
      "Epoch: 886 - Loss: 0.35329071500977904\n",
      "Epoch: 887 - Loss: 0.35329071500977904\n",
      "Epoch: 888 - Loss: 0.35329071500977904\n",
      "Epoch: 889 - Loss: 0.35329071500977904\n",
      "Epoch: 890 - Loss: 0.35329071500977904\n",
      "Epoch: 891 - Loss: 0.35329071500977904\n",
      "Epoch: 892 - Loss: 0.35329071500977904\n",
      "Epoch: 893 - Loss: 0.35329071500977904\n",
      "Epoch: 894 - Loss: 0.35329071500977904\n",
      "Epoch: 895 - Loss: 0.35329071500977904\n",
      "Epoch: 896 - Loss: 0.35329071500977904\n",
      "Epoch: 897 - Loss: 0.35329071500977904\n",
      "Epoch: 898 - Loss: 0.35329071500977904\n",
      "Epoch: 899 - Loss: 0.35329071500977904\n",
      "Epoch: 900 - Loss: 0.35329071500977904\n",
      "Epoch: 901 - Loss: 0.35329071500977904\n",
      "Epoch: 902 - Loss: 0.35329071500977904\n",
      "Epoch: 903 - Loss: 0.35329071500977904\n",
      "Epoch: 904 - Loss: 0.35329071500977904\n",
      "Epoch: 905 - Loss: 0.35329071500977904\n",
      "Epoch: 906 - Loss: 0.35329071500977904\n",
      "Epoch: 907 - Loss: 0.35329071500977904\n",
      "Epoch: 908 - Loss: 0.35329071500977904\n",
      "Epoch: 909 - Loss: 0.35329071500977904\n",
      "Epoch: 910 - Loss: 0.35329071500977904\n",
      "Epoch: 911 - Loss: 0.35329071500977904\n",
      "Epoch: 912 - Loss: 0.35329071500977904\n",
      "Epoch: 913 - Loss: 0.35329071500977904\n",
      "Epoch: 914 - Loss: 0.35329071500977904\n",
      "Epoch: 915 - Loss: 0.35329071500977904\n",
      "Epoch: 916 - Loss: 0.35329071500977904\n",
      "Epoch: 917 - Loss: 0.35329071500977904\n",
      "Epoch: 918 - Loss: 0.35329071500977904\n",
      "Epoch: 919 - Loss: 0.35329071500977904\n",
      "Epoch: 920 - Loss: 0.35329071500977904\n",
      "Epoch: 921 - Loss: 0.35329071500977904\n",
      "Epoch: 922 - Loss: 0.35329071500977904\n",
      "Epoch: 923 - Loss: 0.35329071500977904\n",
      "Epoch: 924 - Loss: 0.35329071500977904\n",
      "Epoch: 925 - Loss: 0.35329071500977904\n",
      "Epoch: 926 - Loss: 0.35329071500977904\n",
      "Epoch: 927 - Loss: 0.35329071500977904\n",
      "Epoch: 928 - Loss: 0.35329071500977904\n",
      "Epoch: 929 - Loss: 0.35329071500977904\n",
      "Epoch: 930 - Loss: 0.35329071500977904\n",
      "Epoch: 931 - Loss: 0.35329071500977904\n",
      "Epoch: 932 - Loss: 0.35329071500977904\n",
      "Epoch: 933 - Loss: 0.35329071500977904\n",
      "Epoch: 934 - Loss: 0.35329071500977904\n",
      "Epoch: 935 - Loss: 0.35329071500977904\n",
      "Epoch: 936 - Loss: 0.35329071500977904\n",
      "Epoch: 937 - Loss: 0.35329071500977904\n",
      "Epoch: 938 - Loss: 0.35329071500977904\n",
      "Epoch: 939 - Loss: 0.35329071500977904\n",
      "Epoch: 940 - Loss: 0.35329071500977904\n",
      "Epoch: 941 - Loss: 0.35329071500977904\n",
      "Epoch: 942 - Loss: 0.35329071500977904\n",
      "Epoch: 943 - Loss: 0.35329071500977904\n",
      "Epoch: 944 - Loss: 0.35329071500977904\n",
      "Epoch: 945 - Loss: 0.35329071500977904\n",
      "Epoch: 946 - Loss: 0.35329071500977904\n",
      "Epoch: 947 - Loss: 0.35329071500977904\n",
      "Epoch: 948 - Loss: 0.35329071500977904\n",
      "Epoch: 949 - Loss: 0.35329071500977904\n",
      "Epoch: 950 - Loss: 0.35329071500977904\n",
      "Epoch: 951 - Loss: 0.35329071500977904\n",
      "Epoch: 952 - Loss: 0.35329071500977904\n",
      "Epoch: 953 - Loss: 0.35329071500977904\n",
      "Epoch: 954 - Loss: 0.35329071500977904\n",
      "Epoch: 955 - Loss: 0.35329071500977904\n",
      "Epoch: 956 - Loss: 0.35329071500977904\n",
      "Epoch: 957 - Loss: 0.35329071500977904\n",
      "Epoch: 958 - Loss: 0.35329071500977904\n",
      "Epoch: 959 - Loss: 0.35329071500977904\n",
      "Epoch: 960 - Loss: 0.35329071500977904\n",
      "Epoch: 961 - Loss: 0.35329071500977904\n",
      "Epoch: 962 - Loss: 0.35329071500977904\n",
      "Epoch: 963 - Loss: 0.35329071500977904\n",
      "Epoch: 964 - Loss: 0.35329071500977904\n",
      "Epoch: 965 - Loss: 0.35329071500977904\n",
      "Epoch: 966 - Loss: 0.35329071500977904\n",
      "Epoch: 967 - Loss: 0.35329071500977904\n",
      "Epoch: 968 - Loss: 0.35329071500977904\n",
      "Epoch: 969 - Loss: 0.35329071500977904\n",
      "Epoch: 970 - Loss: 0.35329071500977904\n",
      "Epoch: 971 - Loss: 0.35329071500977904\n",
      "Epoch: 972 - Loss: 0.35329071500977904\n",
      "Epoch: 973 - Loss: 0.35329071500977904\n",
      "Epoch: 974 - Loss: 0.35329071500977904\n",
      "Epoch: 975 - Loss: 0.35329071500977904\n",
      "Epoch: 976 - Loss: 0.35329071500977904\n",
      "Epoch: 977 - Loss: 0.35329071500977904\n",
      "Epoch: 978 - Loss: 0.35329071500977904\n",
      "Epoch: 979 - Loss: 0.35329071500977904\n",
      "Epoch: 980 - Loss: 0.35329071500977904\n",
      "Epoch: 981 - Loss: 0.35329071500977904\n",
      "Epoch: 982 - Loss: 0.35329071500977904\n",
      "Epoch: 983 - Loss: 0.35329071500977904\n",
      "Epoch: 984 - Loss: 0.35329071500977904\n",
      "Epoch: 985 - Loss: 0.35329071500977904\n",
      "Epoch: 986 - Loss: 0.35329071500977904\n",
      "Epoch: 987 - Loss: 0.35329071500977904\n",
      "Epoch: 988 - Loss: 0.35329071500977904\n",
      "Epoch: 989 - Loss: 0.35329071500977904\n",
      "Epoch: 990 - Loss: 0.35329071500977904\n",
      "Epoch: 991 - Loss: 0.35329071500977904\n",
      "Epoch: 992 - Loss: 0.35329071500977904\n",
      "Epoch: 993 - Loss: 0.35329071500977904\n",
      "Epoch: 994 - Loss: 0.35329071500977904\n",
      "Epoch: 995 - Loss: 0.35329071500977904\n",
      "Epoch: 996 - Loss: 0.35329071500977904\n",
      "Epoch: 997 - Loss: 0.35329071500977904\n",
      "Epoch: 998 - Loss: 0.35329071500977904\n",
      "Epoch: 999 - Loss: 0.35329071500977904\n",
      "Epoch: 1000 - Loss: 0.35329071500977904\n"
     ]
    }
   ],
   "source": [
    "# questions\n",
    "# train_file = \"work/datasets/questions/train.txt\"\n",
    "# pred_file = \"work/datasets/questions/val.test\"\n",
    "# pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "# model_file_name = \"logred_word2vec.questions.model\"\n",
    "# # model_LR = LogisticRegression(model_file_name, learning_rate=0.1, epochs=1000, threshold=0, max_features=10)\n",
    "# model_LR = LogisticRegressionWord2Vec(model_file_name, learning_rate=0.15, epochs=500,  batch_size=32, embedding_matrix=emb)\n",
    "# X, Y, prob  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "# odiya\n",
    "train_file = \"work/datasets/odiya/train.txt\"\n",
    "pred_file = \"work/datasets/odiya/val.test\"\n",
    "pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "model_file_name = \"logred_word2vec.odiya.model\"\n",
    "model_LR = LogisticRegressionWord2Vec(model_file_name, learning_rate=0.01, epochs=1000, batch_size=256, embedding_matrix=emb)\n",
    "X, Y, prob  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "\n",
    "# # 4dim\n",
    "# train_file = \"work/datasets/4dim/train.txt\"\n",
    "# pred_file = \"work/datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "# model_file_name = \"logred_word2vec.4dim.model\"\n",
    "# model_LR = LogisticRegressionWord2Vec(model_file_name, learning_rate=0.9, epochs=500, batch_size=128, embedding_matrix=emb)\n",
    "# X, Y, prob  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "\n",
    "# # #Products\n",
    "# train_file = \"work/datasets/products/train.txt\"\n",
    "# pred_file = \"work/datasets/products/val.test\"\n",
    "# pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "# model_file_name = \"logred_word2vec.products.model\"\n",
    "# # model_LR = LogisticRegression(model_file_name, learning_rate=0.9, epochs=1000, threshold=2, max_features=500)\n",
    "# #80% of the dataset\n",
    "# model_LR = LogisticRegressionWord2Vec(model_file_name, learning_rate=0.9, epochs=100, batch_size=16, embedding_matrix=emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 300)\n"
     ]
    }
   ],
   "source": [
    "preds  = model_LR.classify(pred_file + \".txt\", model_LR.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(train_file, sep='\\t', header=None, names=['text', 'true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos.dec    94\n",
       "neg.tru    83\n",
       "pos.tru    70\n",
       "neg.dec    65\n",
       "Name: true_label, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 2)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 1)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 20.83%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
