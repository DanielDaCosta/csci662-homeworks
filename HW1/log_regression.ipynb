{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    return text.split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Logistic Regression Features #\n",
    "################################\n",
    "\n",
    "class Features_LR(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold=1):\n",
    "        super(Features_LR, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text, threshold)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = [word for word, count in flattened_list_count.items() if count > threshold]\n",
    "\n",
    "        return flattened_list_count_filter\n",
    "    \n",
    "    def get_features(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = 1\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = tf_array/words_per_document\n",
    "        return tf*idf_array\n",
    "        \n",
    "    \n",
    "    def tf_idf(self, tokenized_text): #max_features=1000):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = tf_array/words_per_document.reshape(-1, 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Refer to Chapter 5 for more details on how to implement a LogisticRegression\n",
    "\"\"\"\n",
    "from work.Model import *\n",
    "\n",
    "class LogisticRegression(Model):\n",
    "    def __init__(self, model_file, learning_rate=0.01, epochs=100):\n",
    "        super(LogisticRegression, self).__init__(model_file)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.Y_to_categorical = None # Map Y label to numerical\n",
    "\n",
    "    def initialize_weights(self, num_features, num_labels):\n",
    "        self.weights = np.zeros((num_features, num_labels))\n",
    "        self.bias = np.zeros(num_labels)\n",
    "        # np.random.seed(0)\n",
    "        # self.weights = np.random.rand(num_features, num_labels)\n",
    "        # self.bias = np.random.rand(num_labels)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        : return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        \n",
    "    def predict_prob(self, X, weights, bias):\n",
    "        \"\"\"Return prediction of shape [num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        # Apply Softmax\n",
    "        S = self.softmax(Z)\n",
    "        return S\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X, weights, bias):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        # Apply Softmax\n",
    "        S = self.softmax(Z)\n",
    "\n",
    "        # Rows with highest probability\n",
    "        S_max = np.argmax(S, axis=1)\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "\n",
    "    def train(self, input_file, verbose=False):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset and create vocabulary\n",
    "        features_lr_class = Features_LR(input_file)\n",
    "\n",
    "        # Transform dataset to TF-IDF space\n",
    "        # Return features with format (n_documents, size_vocabulary)\n",
    "        X = features_lr_class.tf_idf(features_lr_class.tokenized_text)\n",
    "        \n",
    "        # Y\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_lr_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_lr_class.labels]\n",
    "\n",
    "        # Initialize Weights\n",
    "        sample_size = len(features_lr_class.tokenized_text)\n",
    "        n_features = len(features_lr_class.vocabulary)\n",
    "        num_labels = len(features_lr_class.labelset)\n",
    "        self.initialize_weights(n_features, num_labels)\n",
    "\n",
    "        # One Hot encoded Y\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Z = softmax(X*W + b)\n",
    "            prob = self.predict_prob(X, self.weights, self.bias)\n",
    "            \n",
    "            # dL/dW\n",
    "            grad_w = (1/sample_size)*np.dot(X.T, prob - Y_onehot)\n",
    "            grad_b =  (1/sample_size)*np.sum(prob - Y_onehot, axis=0)\n",
    "\n",
    "            # Updating weights and bias\n",
    "            self.weights = self.weights - (self.learning_rate*grad_w)\n",
    "            self.bias = self.bias - (self.learning_rate*grad_b)\n",
    "\n",
    "            # Computing cross-entropy loss\n",
    "            loss = self.cross_entropy_loss(prob, Y_onehot)\n",
    "            self.loss.append(loss)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "        model = {\n",
    "            \"feature_weights\": {\n",
    "                \"weights\": self.weights,\n",
    "                \"bias\": self.bias,\n",
    "                \"Y_to_categorical\": self.Y_to_categorical\n",
    "            },\n",
    "            \"Feature\": features_lr_class\n",
    "        }\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return X, Y_onehot, prob\n",
    "\n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_fixle: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\"\n",
    "        ## TODO write your code here (and change return)\n",
    "\n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_LR_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_LR_class.read_inference_file(input_file)\n",
    "        X = []\n",
    "\n",
    "        # Get features from inference file\n",
    "        for sentence in tokenized_text:\n",
    "            # Transform dataset to TF-IDF space\n",
    "            # Return features with format (1, size_vocabulary)\n",
    "            X_sentence = Feature_LR_class.get_features(sentence, Feature_LR_class.idf)\n",
    "\n",
    "            # Concatenate A and B vertically\n",
    "            X.append(X_sentence)\n",
    "\n",
    "        X = np.vstack(X)\n",
    "\n",
    "        # Prediction\n",
    "        preds_numerical = self.predict(X, feature_weights['weights'], feature_weights['bias'])\n",
    "        # Map indexes to Categorical space\n",
    "        preds_label = []\n",
    "        for y in preds_numerical:\n",
    "            tmp = feature_weights['Y_to_categorical'][y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"work/datasets/4dim/train.txt\"\n",
    "pred_file = \"work/datasets/4dim/val.test\"\n",
    "pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "model_file_name = \"logreg.4dim.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LogisticRegression(\"logreg.4dim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.3465735902799726\n",
      "Epoch: 2 - Loss: 0.3466025681359213\n",
      "Epoch: 3 - Loss: 0.34663154967254756\n",
      "Epoch: 4 - Loss: 0.3466605350724734\n",
      "Epoch: 5 - Loss: 0.34668952451836266\n",
      "Epoch: 6 - Loss: 0.3467185181929219\n",
      "Epoch: 7 - Loss: 0.34674751627890105\n",
      "Epoch: 8 - Loss: 0.34677651895909456\n",
      "Epoch: 9 - Loss: 0.3468055264163416\n",
      "Epoch: 10 - Loss: 0.34683453883352744\n",
      "Epoch: 11 - Loss: 0.3468635563935838\n",
      "Epoch: 12 - Loss: 0.34689257927948985\n",
      "Epoch: 13 - Loss: 0.34692160767427294\n",
      "Epoch: 14 - Loss: 0.34695064176100954\n",
      "Epoch: 15 - Loss: 0.3469796817228256\n",
      "Epoch: 16 - Loss: 0.3470087277428979\n",
      "Epoch: 17 - Loss: 0.3470377800044545\n",
      "Epoch: 18 - Loss: 0.3470668386907755\n",
      "Epoch: 19 - Loss: 0.3470959039851938\n",
      "Epoch: 20 - Loss: 0.34712497607109644\n",
      "Epoch: 21 - Loss: 0.34715405513192477\n",
      "Epoch: 22 - Loss: 0.34718314135117534\n",
      "Epoch: 23 - Loss: 0.3472122349124008\n",
      "Epoch: 24 - Loss: 0.34724133599921103\n",
      "Epoch: 25 - Loss: 0.34727044479527325\n",
      "Epoch: 26 - Loss: 0.34729956148431346\n",
      "Epoch: 27 - Loss: 0.34732868625011687\n",
      "Epoch: 28 - Loss: 0.3473578192765287\n",
      "Epoch: 29 - Loss: 0.3473869607474552\n",
      "Epoch: 30 - Loss: 0.34741611084686436\n",
      "Epoch: 31 - Loss: 0.3474452697587865\n",
      "Epoch: 32 - Loss: 0.34747443766731545\n",
      "Epoch: 33 - Loss: 0.34750361475660924\n",
      "Epoch: 34 - Loss: 0.3475328012108906\n",
      "Epoch: 35 - Loss: 0.34756199721444814\n",
      "Epoch: 36 - Loss: 0.347591202951637\n",
      "Epoch: 37 - Loss: 0.34762041860687953\n",
      "Epoch: 38 - Loss: 0.3476496443646666\n",
      "Epoch: 39 - Loss: 0.3476788804095578\n",
      "Epoch: 40 - Loss: 0.34770812692618247\n",
      "Epoch: 41 - Loss: 0.34773738409924065\n",
      "Epoch: 42 - Loss: 0.3477666521135039\n",
      "Epoch: 43 - Loss: 0.34779593115381574\n",
      "Epoch: 44 - Loss: 0.34782522140509303\n",
      "Epoch: 45 - Loss: 0.3478545230523262\n",
      "Epoch: 46 - Loss: 0.34788383628058056\n",
      "Epoch: 47 - Loss: 0.34791316127499694\n",
      "Epoch: 48 - Loss: 0.3479424982207921\n",
      "Epoch: 49 - Loss: 0.3479718473032605\n",
      "Epoch: 50 - Loss: 0.348001208707774\n",
      "Epoch: 51 - Loss: 0.34803058261978365\n",
      "Epoch: 52 - Loss: 0.34805996922481974\n",
      "Epoch: 53 - Loss: 0.3480893687084932\n",
      "Epoch: 54 - Loss: 0.34811878125649603\n",
      "Epoch: 55 - Loss: 0.3481482070546025\n",
      "Epoch: 56 - Loss: 0.34817764628866943\n",
      "Epoch: 57 - Loss: 0.3482070991446374\n",
      "Epoch: 58 - Loss: 0.3482365658085318\n",
      "Epoch: 59 - Loss: 0.34826604646646303\n",
      "Epoch: 60 - Loss: 0.3482955413046278\n",
      "Epoch: 61 - Loss: 0.34832505050930973\n",
      "Epoch: 62 - Loss: 0.34835457426688043\n",
      "Epoch: 63 - Loss: 0.3483841127637999\n",
      "Epoch: 64 - Loss: 0.3484136661866178\n",
      "Epoch: 65 - Loss: 0.3484432347219739\n",
      "Epoch: 66 - Loss: 0.3484728185565994\n",
      "Epoch: 67 - Loss: 0.3485024178773172\n",
      "Epoch: 68 - Loss: 0.34853203287104306\n",
      "Epoch: 69 - Loss: 0.34856166372478636\n",
      "Epoch: 70 - Loss: 0.3485913106256509\n",
      "Epoch: 71 - Loss: 0.34862097376083584\n",
      "Epoch: 72 - Loss: 0.34865065331763634\n",
      "Epoch: 73 - Loss: 0.3486803494834446\n",
      "Epoch: 74 - Loss: 0.34871006244575053\n",
      "Epoch: 75 - Loss: 0.34873979239214276\n",
      "Epoch: 76 - Loss: 0.34876953951030915\n",
      "Epoch: 77 - Loss: 0.34879930398803805\n",
      "Epoch: 78 - Loss: 0.34882908601321877\n",
      "Epoch: 79 - Loss: 0.3488588857738429\n",
      "Epoch: 80 - Loss: 0.3488887034580042\n",
      "Epoch: 81 - Loss: 0.34891853925390065\n",
      "Epoch: 82 - Loss: 0.34894839334983446\n",
      "Epoch: 83 - Loss: 0.3489782659342131\n",
      "Epoch: 84 - Loss: 0.3490081571955501\n",
      "Epoch: 85 - Loss: 0.34903806732246634\n",
      "Epoch: 86 - Loss: 0.34906799650368997\n",
      "Epoch: 87 - Loss: 0.3490979449280583\n",
      "Epoch: 88 - Loss: 0.3491279127845179\n",
      "Epoch: 89 - Loss: 0.3491579002621255\n",
      "Epoch: 90 - Loss: 0.3491879075500494\n",
      "Epoch: 91 - Loss: 0.3492179348375697\n",
      "Epoch: 92 - Loss: 0.3492479823140792\n",
      "Epoch: 93 - Loss: 0.3492780501690848\n",
      "Epoch: 94 - Loss: 0.3493081385922076\n",
      "Epoch: 95 - Loss: 0.3493382477731842\n",
      "Epoch: 96 - Loss: 0.34936837790186753\n",
      "Epoch: 97 - Loss: 0.34939852916822733\n",
      "Epoch: 98 - Loss: 0.3494287017623517\n",
      "Epoch: 99 - Loss: 0.349458895874447\n",
      "Epoch: 100 - Loss: 0.3494891116948394\n"
     ]
    }
   ],
   "source": [
    "X, Y, prob  = model_LR.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1248, 5253)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR_loaded = model_LR.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15005661e-03,  5.14989670e-04, -1.30262516e-03,\n",
       "         2.28707418e-03],\n",
       "       [ 1.06360747e-03,  4.99647357e-05, -7.92836111e-04,\n",
       "        -3.01790746e-05],\n",
       "       [ 5.18194221e-04,  6.28149116e-05, -6.25831051e-04,\n",
       "         2.53237055e-04],\n",
       "       ...,\n",
       "       [ 3.25096888e-05, -1.07517348e-05, -1.07111154e-05,\n",
       "        -1.07776991e-05],\n",
       "       [-3.73634824e-05, -7.44952571e-06,  8.28999616e-05,\n",
       "        -3.72105150e-05],\n",
       "       [-6.70726298e-06,  2.01832519e-05, -6.63441166e-06,\n",
       "        -6.67601232e-06]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_loaded['feature_weights']['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_LR.classify(pred_file + \".txt\", model_LR_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.23%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
