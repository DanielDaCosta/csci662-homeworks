{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    return text.split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2111,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Logistic Regression Features #\n",
    "################################\n",
    "\n",
    "class Features_LR(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold, max_features):\n",
    "        super(Features_LR, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text, threshold, max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features-1))\n",
    "\n",
    "        # Add to vocabulary the Out-of-Vocabulary token\n",
    "        return list(flattened_list_count_filter.keys()) + ['OOV']\n",
    "    \n",
    "    def replace_unknown_word_with_oov(self, tokenized_sentence):\n",
    "        \"\"\"Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        token\n",
    "        \"\"\"\n",
    "        updated_sentence = []\n",
    "        for word in tokenized_sentence:\n",
    "            if word not in self.vocabulary:\n",
    "                updated_sentence.append('OOV')\n",
    "            else:\n",
    "                updated_sentence.append(word)\n",
    "        return updated_sentence\n",
    "    \n",
    "    def get_features(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothinf\n",
    "        return tf*idf_array\n",
    "        \n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Refer to Chapter 5 for more details on how to implement a LogisticRegression\n",
    "\"\"\"\n",
    "from work.Model import *\n",
    "\n",
    "class LogisticRegression(Model):\n",
    "    def __init__(self, model_file, learning_rate=None, epochs=None, threshold=None, max_features=None):\n",
    "        super(LogisticRegression, self).__init__(model_file)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.Y_to_categorical = None # Map Y label to numerical\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def initialize_weights(self, num_features, num_labels):\n",
    "        self.weights = np.zeros((num_features, num_labels))\n",
    "        self.bias = np.zeros(num_labels)\n",
    "        # np.random.seed(0)\n",
    "        # self.weights = np.random.rand(num_features, num_labels)\n",
    "        # self.bias = np.random.rand(num_labels)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"Sigmoid function for binary classification\n",
    "\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return 1/(1+e^{-Z})\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "        \n",
    "    def predict_prob(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of shape [num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "        return S\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate Binary cross-entropy\n",
    "        \"\"\"\n",
    "        return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n",
    "\n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "\n",
    "            # Rows with highest probability\n",
    "            S_max = np.argmax(S, axis=1)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "            # Rows with highest probability\n",
    "            S_max = [1 if i > 0.5 else 0 for i in S]\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "\n",
    "    def train(self, input_file, verbose=False):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset and create vocabulary\n",
    "        features_lr_class = Features_LR(input_file, self.threshold, self.max_features)\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in features_lr_class.tokenized_text:\n",
    "            tmp = features_lr_class.replace_unknown_word_with_oov(sentence)\n",
    "            updated_text.append(tmp)\n",
    "\n",
    "        # Transform dataset to TF-IDF space\n",
    "        # Return features with format (n_documents, size_vocabulary)\n",
    "        X = features_lr_class.tf_idf(updated_text)\n",
    "        \n",
    "        # Y\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_lr_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_lr_class.labels]\n",
    "\n",
    "        # Initialize Weights\n",
    "        sample_size = len(features_lr_class.tokenized_text)\n",
    "        n_features = len(features_lr_class.vocabulary)\n",
    "        num_labels = len(features_lr_class.labelset)\n",
    "\n",
    "\n",
    "        # Check if it's multinomial or binary classification\n",
    "        if num_labels == 2:\n",
    "            multinomial = False\n",
    "            num_labels = 1 # Only one column to reference 0 or 1\n",
    "        else:\n",
    "            multinomial = True\n",
    "\n",
    "        self.initialize_weights(n_features, num_labels)\n",
    "\n",
    "        # One Hot encoded Y\n",
    "        if multinomial:\n",
    "            Y_onehot = self.OneHot(Y, num_labels)\n",
    "        else:\n",
    "            Y_onehot = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # # Z = softmax(X*W + b)\n",
    "            # prob = self.predict_prob(X, self.weights, self.bias, multinomial)\n",
    "\n",
    "            # # break            \n",
    "            # # dL/dW\n",
    "            # grad_w = (1/sample_size)*np.dot(X.T, prob - Y_onehot)\n",
    "            # grad_b =  (1/sample_size)*np.sum(prob - Y_onehot, axis=0)\n",
    "\n",
    "            # self.weights = self.weights - (self.learning_rate*grad_w)\n",
    "            # self.bias = self.bias - (self.learning_rate*grad_b)\n",
    "\n",
    "            # Computing cross-entropy loss\n",
    "            if multinomial:\n",
    "                loss = self.cross_entropy_loss(prob, Y_onehot)\n",
    "            else:\n",
    "                loss = self.binary_cross_entropy_loss(prob, Y_onehot)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "        model = {\n",
    "            \"feature_weights\": {\n",
    "                \"weights\": self.weights,\n",
    "                \"bias\": self.bias,\n",
    "                \"Y_to_categorical\": self.Y_to_categorical\n",
    "            },\n",
    "            \"Feature\": features_lr_class\n",
    "        }\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return X, Y_onehot, prob, \n",
    "\n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_fixle: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\"\n",
    "\n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_LR_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_LR_class.read_inference_file(input_file)\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            tmp = Feature_LR_class.replace_unknown_word_with_oov(sentence)\n",
    "            updated_text.append(tmp)      \n",
    "        tokenized_text = updated_text\n",
    "        \n",
    "        X = []\n",
    "\n",
    "        # Get features from inference file\n",
    "        for sentence in tokenized_text:\n",
    "            # Transform dataset to TF-IDF space\n",
    "            # Return features with format (1, size_vocabulary)\n",
    "            X_sentence = Feature_LR_class.get_features(sentence, Feature_LR_class.idf)\n",
    "\n",
    "            # Concatenate A and B vertically\n",
    "            X.append(X_sentence)\n",
    "\n",
    "        X = np.vstack(X)\n",
    "\n",
    "        # Prediction\n",
    "        multinomial = True if len(feature_weights['Y_to_categorical'].keys()) > 2 else False\n",
    "        preds_numerical = self.predict(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        # Map indexes to Categorical space\n",
    "        preds_label = []\n",
    "        probs = self.predict_prob(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        for y in preds_numerical:\n",
    "            tmp = feature_weights['Y_to_categorical'][y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label, probs, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "# train_file = \"work/datasets/questions/train.txt\"\n",
    "# pred_file = \"work/datasets/questions/val.test\"\n",
    "# pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "# model_file_name = \"logreg.questions.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.1, epochs=1000, threshold=0, max_features=10)\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.005, epochs=400, threshold=0, max_features=100)\n",
    "\n",
    "# # odiya\n",
    "# train_file = \"work/datasets/odiya/train.txt\"\n",
    "# pred_file = \"work/datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "# model_file_name = \"logreg.odiya.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.000001, epochs=1000, threshold=10, max_features=1000)\n",
    "\n",
    "\n",
    "# 4dim\n",
    "# train_file = \"work/datasets/4dim/train.txt\"\n",
    "# pred_file = \"work/datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "# model_file_name = \"logreg.4dim.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.2, epochs=200, threshold=1, max_features=100)\n",
    "\n",
    "\n",
    "# # Products\n",
    "# train_file = \"work/datasets/products/train.txt\"\n",
    "train_file = \"work/datasets/products/train.txt\"\n",
    "pred_file = \"work/datasets/products/val.test\"\n",
    "pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "model_file_name = \"logreg.products.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.9, epochs=1000, threshold=2, max_features=500)\n",
    "model_LR = LogisticRegression(model_file_name, learning_rate=0.95, epochs=4, threshold=2, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.6931471805599453\n",
      "Epoch: 2 - Loss: 0.7511250881802476\n",
      "Epoch: 3 - Loss: 0.8107820773546406\n",
      "Epoch: 4 - Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/6vxh_8k15_n5m7tsxrjkzsvm0000gn/T/ipykernel_1036/1169811041.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n",
      "/var/folders/68/6vxh_8k15_n5m7tsxrjkzsvm0000gn/T/ipykernel_1036/1169811041.py:68: RuntimeWarning: invalid value encountered in multiply\n",
      "  return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n"
     ]
    }
   ],
   "source": [
    "X, Y, prob  = model_LR.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/6vxh_8k15_n5m7tsxrjkzsvm0000gn/T/ipykernel_1036/1169811041.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n",
      "/var/folders/68/6vxh_8k15_n5m7tsxrjkzsvm0000gn/T/ipykernel_1036/1169811041.py:68: RuntimeWarning: invalid value encountered in multiply\n",
      "  return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 2179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_LR.binary_cross_entropy_loss(prob, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 2188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prob[np.log(prob) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10367 [1.]\n",
      "19036 [1.]\n",
      "22117 [1.]\n"
     ]
    }
   ],
   "source": [
    "# for i, p in enumerate(prob):\n",
    "#     if p == 1:\n",
    "#         print(i,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds, prob, X = model_LR.classify(pred_file + \".txt\", model_LR.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos'}"
      ]
     },
     "execution_count": 2158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2161,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(train_file, sep='\\t', header=None, names=['text', 'true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true_label\n",
       "pos    15427\n",
       "neg    10651\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.01%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
