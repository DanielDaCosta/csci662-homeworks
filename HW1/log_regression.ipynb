{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    return text.split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Logistic Regression Features #\n",
    "################################\n",
    "\n",
    "class Features_LR(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold=2):\n",
    "        super(Features_LR, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text, threshold)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = [word for word, count in flattened_list_count.items() if count > threshold]\n",
    "\n",
    "        return flattened_list_count_filter\n",
    "    \n",
    "    def get_features(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = 1\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = tf_array/words_per_document\n",
    "        return tf*idf_array\n",
    "        \n",
    "    \n",
    "    def tf_idf(self, tokenized_text): #max_features=1000):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = tf_array/words_per_document.reshape(-1, 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF sc\n",
    "        # ore\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf, tf, idf # Shape (n_documents, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Refer to Chapter 5 for more details on how to implement a LogisticRegression\n",
    "\"\"\"\n",
    "from work.Model import *\n",
    "\n",
    "class LogisticRegression(Model):\n",
    "    def __init__(self, model_file, learning_rate=0.01, epochs=1000):\n",
    "        super(LogisticRegression, self).__init__(model_file)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.Y_mapping = None # Map Y label to numerical\n",
    "\n",
    "    def initialize_weights(self, num_features, num_labels):\n",
    "        self.weights = np.zeros((num_features, num_labels))\n",
    "        self.bias = np.zeros(num_labels)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        : return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z)/np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        \n",
    "\n",
    "    def predict(self, X, weights, bias):\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        # Apply Softmax\n",
    "        S = self.softmax(Z)\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "\n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y = np.zeros((len(targets), num_labels))\n",
    "        Y[np.arange(len(targets)), targets] = 1\n",
    "        return Y\n",
    "    \n",
    "\n",
    "    def train(self, input_file):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset and create vocabulary\n",
    "        features_lr_class = Features_LR(input_file)\n",
    "\n",
    "        # Transform dataset to TF-IDF space\n",
    "        # Return features with format (n_documents, size_vocabulary)\n",
    "        X = features_lr_class.tf_idf(features_lr_class.tokenized_text)\n",
    "        \n",
    "        # Y\n",
    "        self.Y_mapping = {label: index for index, label in enumerate(np.unique(features_lr_class.labels))}\n",
    "        Y = [self.Y_mapping[y] for y in features_lr_class.labels]\n",
    "\n",
    "\n",
    "        # Initialize Weights\n",
    "        sample_size = len(features_lr_class.tokenized_text)\n",
    "        n_features = len(features_lr_class.vocabulary)\n",
    "        num_labels = len(features_lr_class.labelset)\n",
    "        self.initialize_weights(sample_size, n_features)\n",
    "        # One Hot encoded Y\n",
    "        Y_onehot = self.OneHot(Y, num_labels)\n",
    "\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Z = softmax(X*W + b)\n",
    "            prob = self.predict(X, self.weights, self.bias)\n",
    "            \n",
    "            # dL/dW\n",
    "            grad_w = (1/sample_size)*np.dot(X.T, prob - Y_onehot)\n",
    "            grad_b =  (1/sample_size)*np.sum(prob - Y_onehot, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        ## TODO write your code here\n",
    "        model = None\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_file: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\"\n",
    "        ## TODO write your code here (and change return)\n",
    "        preds = None\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot(targets, num_labels):\n",
    "    Y = np.zeros((len(targets), num_labels))\n",
    "    Y[np.arange(len(targets)), targets] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [1, 0, 3, 2, 1, 1]\n",
    "num_labels = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHot(targets, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.random.rand(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9468947 , 0.68072595, 0.02457844, 0.71077654, 0.12075853],\n",
       "       [0.33585364, 0.36489347, 0.42197878, 0.26787945, 0.7907006 ],\n",
       "       [0.58635958, 0.12694744, 0.87740103, 0.17388748, 0.79550203],\n",
       "       [0.67859676, 0.51188968, 0.80106312, 0.24684786, 0.45458267],\n",
       "       [0.50807223, 0.9378627 , 0.43132378, 0.66662698, 0.15393962],\n",
       "       [0.81975571, 0.40176412, 0.9147828 , 0.64112081, 0.81244211],\n",
       "       [0.26190353, 0.56067029, 0.04882806, 0.62511781, 0.72490852],\n",
       "       [0.35422805, 0.83007158, 0.8746002 , 0.78267619, 0.71202362],\n",
       "       [0.64031492, 0.61272653, 0.69183811, 0.79761212, 0.8256599 ],\n",
       "       [0.05098652, 0.8515289 , 0.99808452, 0.05089972, 0.79544978]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -np.mean(np.log(Softmax(z)[np.arange(len(y)), y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9417422323201528"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29486943, 0.22596133, 0.11723921, 0.23285466, 0.12907537],\n",
       "       [0.17763172, 0.18286575, 0.1936084 , 0.16595858, 0.27993555],\n",
       "       [0.20558758, 0.12986039, 0.27503918, 0.13610137, 0.25341148],\n",
       "       [0.22596802, 0.19127007, 0.25540739, 0.1467374 , 0.18061712],\n",
       "       [0.18738325, 0.28799597, 0.1735399 , 0.21957871, 0.13150217],\n",
       "       [0.21799882, 0.1435234 , 0.23973083, 0.18233666, 0.21641028],\n",
       "       [0.16173181, 0.21804604, 0.13069469, 0.23256128, 0.25696618],\n",
       "       [0.13779662, 0.22176591, 0.231864  , 0.21150042, 0.19707304],\n",
       "       [0.18519756, 0.18015809, 0.19498962, 0.21674474, 0.22291   ],\n",
       "       [0.11223106, 0.24991032, 0.289356  , 0.11222132, 0.2362813 ]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(Z)/np.sum(np.exp(Z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.74181064171361"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.9468947) + np.exp(0.68072595) + np.exp(0.02457844) + np.exp(0.71077654) +  np.exp(0.12075853)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"work/datasets/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = Features_LR(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf, tf, idf_arra = feat.tf_idf(feat.tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16666667, 0.16666667, 0.2821912 , 0.        , 0.34976871,\n",
       "       0.34976871, 0.34976871, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.get_features(feat.tokenized_text[0], idf_arra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.69314718, 1.69314718, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_arra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a/b.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][4] =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0., 25.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 1., 1., 1., 5., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[4] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
