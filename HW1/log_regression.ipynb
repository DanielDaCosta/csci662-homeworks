{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    # 5) Remove Stop Words\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Logistic Regression Features #\n",
    "################################\n",
    "\n",
    "class Features_LR(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold, max_features):\n",
    "        super(Features_LR, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text, threshold, max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features-1))\n",
    "\n",
    "        # Add to vocabulary the Out-of-Vocabulary token\n",
    "        return list(flattened_list_count_filter.keys()) + ['OOV']\n",
    "    \n",
    "    def replace_unknown_word_with_oov(self, tokenized_sentence):\n",
    "        \"\"\"Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        token\n",
    "        \"\"\"\n",
    "        updated_sentence = []\n",
    "        for word in tokenized_sentence:\n",
    "            if word not in self.vocabulary:\n",
    "                updated_sentence.append('OOV')\n",
    "            else:\n",
    "                updated_sentence.append(word)\n",
    "        return updated_sentence\n",
    "    \n",
    "    def get_features(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothinf\n",
    "        return tf*idf_array\n",
    "        \n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Refer to Chapter 5 for more details on how to implement a LogisticRegression\n",
    "\"\"\"\n",
    "from work.Model import *\n",
    "\n",
    "class LogisticRegression(Model):\n",
    "    def __init__(self, model_file, learning_rate=None, epochs=None, threshold=None, max_features=None, batch_size=None):\n",
    "        super(LogisticRegression, self).__init__(model_file)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.Y_to_categorical = None # Map Y label to numerical\n",
    "        self.batch_size = batch_size\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def initialize_weights(self, num_features, num_labels):\n",
    "        self.weights = np.zeros((num_features, num_labels))\n",
    "        self.bias = np.zeros(num_labels)\n",
    "        # np.random.seed(0)\n",
    "        # self.weights = np.random.rand(num_features, num_labels)\n",
    "        # self.bias = np.random.rand(num_labels)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"Sigmoid function for binary classification\n",
    "\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return 1/(1+e^{-Z})\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "        \n",
    "    def predict_prob(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of shape [num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "        return S\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate Binary cross-entropy\n",
    "        \"\"\"\n",
    "        return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n",
    "\n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "\n",
    "            # Rows with highest probability\n",
    "            S_max = np.argmax(S, axis=1)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "            # Rows with highest probability\n",
    "            S_max = [1 if i > 0.5 else 0 for i in S]\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "\n",
    "    def train(self, input_file, verbose=False):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset and create vocabulary\n",
    "        features_lr_class = Features_LR(input_file, self.threshold, self.max_features)\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in features_lr_class.tokenized_text:\n",
    "            tmp = features_lr_class.replace_unknown_word_with_oov(sentence)\n",
    "            updated_text.append(tmp)\n",
    "\n",
    "        # Transform dataset to TF-IDF space\n",
    "        # Return features with format (n_documents, size_vocabulary)\n",
    "        X = features_lr_class.tf_idf(updated_text)\n",
    "        \n",
    "        # Y\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_lr_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_lr_class.labels]\n",
    "\n",
    "        # Initialize Weights\n",
    "        sample_size = len(features_lr_class.tokenized_text)\n",
    "        n_features = len(features_lr_class.vocabulary)\n",
    "        num_labels = len(features_lr_class.labelset)\n",
    "\n",
    "\n",
    "        # Check if it's multinomial or binary classification\n",
    "        if num_labels == 2:\n",
    "            multinomial = False\n",
    "            num_labels = 1 # Only one column to reference 0 or 1\n",
    "        else:\n",
    "            multinomial = True\n",
    "\n",
    "        self.initialize_weights(n_features, num_labels)\n",
    "\n",
    "        # One Hot encoded Y\n",
    "        if multinomial:\n",
    "            Y_onehot = self.OneHot(Y, num_labels)\n",
    "        else:\n",
    "            Y_onehot = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation(sample_size)\n",
    "        X_permutation = X[permutation]\n",
    "        Y_permutation_onehot = Y_onehot[permutation]\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # Batch_size implementation\n",
    "            for j in range(0, sample_size, batch_size):\n",
    "                X_mini_batch = X_permutation[j:j+batch_size]\n",
    "                y_mini_batch = Y_permutation_onehot[j:j+batch_size]\n",
    "\n",
    "                # Z = softmax(X*W + b)\n",
    "                prob = self.predict_prob(X_mini_batch, self.weights, self.bias, multinomial)\n",
    "\n",
    "                # dL/dW\n",
    "                grad_w = (1/batch_size)*np.dot(X_mini_batch.T, prob - y_mini_batch)\n",
    "                grad_b =  (1/batch_size)*np.sum(prob - y_mini_batch, axis=0)\n",
    "\n",
    "            # # break            \n",
    "            # dL/dW\n",
    "                # grad_w = (1/sample_size)*np.dot(X.T, prob - Y_onehot)\n",
    "                # grad_b =  (1/sample_size)*np.sum(prob - Y_onehot, axis=0)\n",
    "\n",
    "                self.weights = self.weights - (self.learning_rate*grad_w)\n",
    "                self.bias = self.bias - (self.learning_rate*grad_b)\n",
    "\n",
    "            # Computing cross-entropy loss\n",
    "            if multinomial:\n",
    "                loss = self.cross_entropy_loss(prob, y_mini_batch)\n",
    "            else:\n",
    "                loss = self.binary_cross_entropy_loss(prob, y_mini_batch)\n",
    "\n",
    "            self.loss.append(loss)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "        model = {\n",
    "            \"feature_weights\": {\n",
    "                \"weights\": self.weights,\n",
    "                \"bias\": self.bias,\n",
    "                \"Y_to_categorical\": self.Y_to_categorical\n",
    "            },\n",
    "            \"Feature\": features_lr_class\n",
    "        }\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return X, Y_onehot, self.loss \n",
    "\n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_fixle: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\"\n",
    "\n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_LR_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_LR_class.read_inference_file(input_file)\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        for sentence in tokenized_text:\n",
    "            tmp = Feature_LR_class.replace_unknown_word_with_oov(sentence)\n",
    "            updated_text.append(tmp)      \n",
    "        tokenized_text = updated_text\n",
    "        \n",
    "        X = []\n",
    "\n",
    "        # Get features from inference file\n",
    "        for sentence in tokenized_text:\n",
    "            # Transform dataset to TF-IDF space\n",
    "            # Return features with format (1, size_vocabulary)\n",
    "            X_sentence = Feature_LR_class.get_features(sentence, Feature_LR_class.idf)\n",
    "\n",
    "            # Concatenate A and B vertically\n",
    "            X.append(X_sentence)\n",
    "\n",
    "        X = np.vstack(X)\n",
    "\n",
    "        # Prediction\n",
    "        multinomial = True if len(feature_weights['Y_to_categorical'].keys()) > 2 else False\n",
    "        preds_numerical = self.predict(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        # Map indexes to Categorical space\n",
    "        preds_label = []\n",
    "        probs = self.predict_prob(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        for y in preds_numerical:\n",
    "            tmp = feature_weights['Y_to_categorical'][y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.6987004163624384\n",
      "Epoch: 2 - Loss: 0.6931347816950392\n",
      "Epoch: 3 - Loss: 0.688011134152698\n",
      "Epoch: 4 - Loss: 0.6835161405052735\n",
      "Epoch: 5 - Loss: 0.6819526394730139\n",
      "Epoch: 6 - Loss: 0.6634917280271193\n",
      "Epoch: 7 - Loss: 0.6694038163817597\n",
      "Epoch: 8 - Loss: 0.6639377429389184\n",
      "Epoch: 9 - Loss: 0.6594660893104549\n",
      "Epoch: 10 - Loss: 0.6548106718477419\n",
      "Epoch: 11 - Loss: 0.6501666257173091\n",
      "Epoch: 12 - Loss: 0.645572370456536\n",
      "Epoch: 13 - Loss: 0.6410304899175009\n",
      "Epoch: 14 - Loss: 0.636556348220451\n",
      "Epoch: 15 - Loss: 0.6321770703412404\n",
      "Epoch: 16 - Loss: 0.6279190174909957\n",
      "Epoch: 17 - Loss: 0.6237987426694847\n",
      "Epoch: 18 - Loss: 0.6198241261238877\n",
      "Epoch: 19 - Loss: 0.615999449265277\n",
      "Epoch: 20 - Loss: 0.6123245938679667\n",
      "Epoch: 21 - Loss: 0.608799787235726\n",
      "Epoch: 22 - Loss: 0.6054238824253217\n",
      "Epoch: 23 - Loss: 0.6021786936240254\n",
      "Epoch: 24 - Loss: 0.5990416562384016\n",
      "Epoch: 25 - Loss: 0.5962144473142339\n",
      "Epoch: 26 - Loss: 0.5934148992047888\n",
      "Epoch: 27 - Loss: 0.5907236404075594\n",
      "Epoch: 28 - Loss: 0.5881396284483049\n",
      "Epoch: 29 - Loss: 0.5856638166536379\n",
      "Epoch: 30 - Loss: 0.5832933786525799\n",
      "Epoch: 31 - Loss: 0.5810221636208721\n",
      "Epoch: 32 - Loss: 0.578839405161893\n",
      "Epoch: 33 - Loss: 0.5767296332401621\n",
      "Epoch: 34 - Loss: 0.5746780379557076\n",
      "Epoch: 35 - Loss: 0.5726773197290373\n",
      "Epoch: 36 - Loss: 0.5707268389727366\n",
      "Epoch: 37 - Loss: 0.5688252807846915\n",
      "Epoch: 38 - Loss: 0.5669658265190448\n",
      "Epoch: 39 - Loss: 0.5651389250241399\n",
      "Epoch: 40 - Loss: 0.5633421133095472\n",
      "Epoch: 41 - Loss: 0.5615970475528762\n",
      "Epoch: 42 - Loss: 0.560151183127162\n",
      "Epoch: 43 - Loss: 0.5641631124825481\n",
      "Epoch: 44 - Loss: 0.5381132166236208\n",
      "Epoch: 45 - Loss: 0.5448627241519286\n",
      "Epoch: 46 - Loss: 0.5428931245210419\n",
      "Epoch: 47 - Loss: 0.5413887329077385\n",
      "Epoch: 48 - Loss: 0.5397836410947596\n",
      "Epoch: 49 - Loss: 0.5381254110418533\n",
      "Epoch: 50 - Loss: 0.5364820659004298\n",
      "Epoch: 51 - Loss: 0.5348755172675915\n",
      "Epoch: 52 - Loss: 0.5333085501329652\n",
      "Epoch: 53 - Loss: 0.5317790718874068\n",
      "Epoch: 54 - Loss: 0.5302844880018909\n",
      "Epoch: 55 - Loss: 0.5288226702023058\n",
      "Epoch: 56 - Loss: 0.5273919971361997\n",
      "Epoch: 57 - Loss: 0.5259912160794995\n",
      "Epoch: 58 - Loss: 0.52461935898606\n",
      "Epoch: 59 - Loss: 0.5232760877813076\n",
      "Epoch: 60 - Loss: 0.5219642300891425\n",
      "Epoch: 61 - Loss: 0.5206868370260811\n",
      "Epoch: 62 - Loss: 0.5194390632211112\n",
      "Epoch: 63 - Loss: 0.5182189491721653\n",
      "Epoch: 64 - Loss: 0.5170260267509438\n",
      "Epoch: 65 - Loss: 0.5158799197535764\n",
      "Epoch: 66 - Loss: 0.5147513190543668\n",
      "Epoch: 67 - Loss: 0.5136416194047261\n",
      "Epoch: 68 - Loss: 0.512551280763072\n",
      "Epoch: 69 - Loss: 0.5114803455718983\n",
      "Epoch: 70 - Loss: 0.5104287137837372\n",
      "Epoch: 71 - Loss: 0.509396195506785\n",
      "Epoch: 72 - Loss: 0.5083825409395579\n",
      "Epoch: 73 - Loss: 0.5073874631879637\n",
      "Epoch: 74 - Loss: 0.5064106556976994\n",
      "Epoch: 75 - Loss: 0.5054518054057547\n",
      "Epoch: 76 - Loss: 0.5045106025305724\n",
      "Epoch: 77 - Loss: 0.5035867476586829\n",
      "Epoch: 78 - Loss: 0.5026799565764287\n",
      "Epoch: 79 - Loss: 0.5017899631701687\n",
      "Epoch: 80 - Loss: 0.500916520660185\n",
      "Epoch: 81 - Loss: 0.5000594014173324\n",
      "Epoch: 82 - Loss: 0.4992183956159769\n",
      "Epoch: 83 - Loss: 0.4983933089864218\n",
      "Epoch: 84 - Loss: 0.4975839599345963\n",
      "Epoch: 85 - Loss: 0.4967901762905953\n",
      "Epoch: 86 - Loss: 0.4960117919287886\n",
      "Epoch: 87 - Loss: 0.4952486434716851\n",
      "Epoch: 88 - Loss: 0.494500567250571\n",
      "Epoch: 89 - Loss: 0.49376739665221897\n",
      "Epoch: 90 - Loss: 0.49304895993687947\n",
      "Epoch: 91 - Loss: 0.49234507857188087\n",
      "Epoch: 92 - Loss: 0.4916555660898517\n",
      "Epoch: 93 - Loss: 0.49098022745177117\n",
      "Epoch: 94 - Loss: 0.49031885887218823\n",
      "Epoch: 95 - Loss: 0.4896712480451667\n",
      "Epoch: 96 - Loss: 0.4890371746919355\n",
      "Epoch: 97 - Loss: 0.4884164113316375\n",
      "Epoch: 98 - Loss: 0.4878087241526961\n",
      "Epoch: 99 - Loss: 0.48721387383559456\n",
      "Epoch: 100 - Loss: 0.4866316161582999\n"
     ]
    }
   ],
   "source": [
    "# # questions\n",
    "# train_file = \"work/datasets/questions/train.txt\"\n",
    "# pred_file = \"work/datasets/questions/val.test\"\n",
    "# pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "# loss_file = \"work/datasets/questions/loss.txt\"\n",
    "# model_file_name = \"logreg.questions.model\"\n",
    "# # model_LR = LogisticRegression(model_file_name, learning_rate=0.1, epochs=1000, threshold=0, max_features=10)\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.15, epochs=500, threshold=0, max_features=150, batch_size=32)\n",
    "# X, Y, loss  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "# with open(loss_file, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for item in loss:\n",
    "#         writer.writerow([item])\n",
    "\n",
    "# odiya\n",
    "# train_file = \"work/datasets/odiya/train.txt\"\n",
    "# pred_file = \"work/datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "# loss_file = \"work/datasets/odiya/loss.txt\"\n",
    "# model_file_name = \"logreg.odiya.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.01, epochs=1000, threshold=10, max_features=1000, batch_size=256)\n",
    "# X, Y, loss  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "# with open(loss_file, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for item in loss:\n",
    "        # writer.writerow([item])\n",
    "\n",
    "\n",
    "# 4dim\n",
    "# train_file = \"work/datasets/4dim/train.txt\"\n",
    "# pred_file = \"work/datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "# model_file_name = \"logreg.4dim.model\"\n",
    "# loss_file = \"work/datasets/4dim/loss.txt\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.35, epochs=500, threshold=5, max_features=2000, batch_size=64)\n",
    "# X, Y, loss  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "# with open(loss_file, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for item in loss:\n",
    "#         writer.writerow([item])\n",
    "\n",
    "\n",
    "#Products\n",
    "train_file = \"work/datasets/products/train.txt\"\n",
    "pred_file = \"work/datasets/products/val.test\"\n",
    "pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "model_file_name = \"logreg.products.model\"\n",
    "loss_file = \"work/datasets/products/loss.txt\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.9, epochs=1000, threshold=2, max_features=500)\n",
    "#80% of the dataset\n",
    "model_LR = LogisticRegression(model_file_name, learning_rate=0.95, epochs=100, threshold=2, max_features=1000, batch_size=256)\n",
    "X, Y, loss  = model_LR.train(train_file, verbose=True)\n",
    "\n",
    "\n",
    "with open(loss_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for item in loss:\n",
    "        writer.writerow([item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_LR.classify(pred_file + \".txt\", model_LR.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(train_file, sep='\\t', header=None, names=['text', 'true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true_label\n",
       "pos    3844\n",
       "neg    2670\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       "pos    4152\n",
       "neg    2362\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dataset['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.20%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
