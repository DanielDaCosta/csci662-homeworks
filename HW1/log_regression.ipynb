{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "# def tokenize(text):\n",
    "#     # TODO customize to your needs\n",
    "#     text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "#     return text.split()\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    return text.split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Logistic Regression Features #\n",
    "################################\n",
    "\n",
    "class Features_LR(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold, max_features):\n",
    "        super(Features_LR, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text, threshold, max_features)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.vocabulary, start=0)}\n",
    "        self.idf = None # Need to save IDF values for inference\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold, max_features=None):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words\n",
    "        that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Sort the dictionary by values in descending order\n",
    "        flattened_list_count = dict(sorted(flattened_list_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = {word:count for word, count in flattened_list_count.items() if count > threshold}\n",
    "\n",
    "        # Limit the size of the vocabulary based on max_features\n",
    "        if max_features:\n",
    "            flattened_list_count_filter = dict(islice(flattened_list_count_filter.items(), max_features))\n",
    "\n",
    "        return list(flattened_list_count_filter.keys())\n",
    "    \n",
    "    def get_features(self, tokenized_sentence, idf_array):\n",
    "        \"\"\"Convert sentence to TF-IDF space\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = 1\n",
    "        tf_array = np.zeros(size_vocabulary)\n",
    "        words_per_document = 0\n",
    "        # Compute Term-Frequency\n",
    "        words_in_document = []\n",
    "        for word in tokenized_sentence:\n",
    "            index_word = self.word2index.get(word)\n",
    "            if word in self.word2index.keys():\n",
    "                tf_array[index_word] += 1\n",
    "                words_per_document += 1\n",
    "        tf = (tf_array + 1)/(words_per_document+1) # with smoothin\n",
    "        return tf*idf_array\n",
    "        \n",
    "    \n",
    "    def tf_idf(self, tokenized_text):\n",
    "        \"\"\"Term frequency-inverse document frequency\n",
    "        \"\"\"\n",
    "        size_vocabulary = len(self.vocabulary)\n",
    "        n_documents = len(tokenized_text)\n",
    "        tf_array = np.zeros((n_documents, size_vocabulary))\n",
    "        idf_array = np.zeros(size_vocabulary) # Inverse Document Frequency\n",
    "        words_per_document = np.zeros(n_documents)\n",
    "        # Compute Term-Frequency\n",
    "        for d_i, sentence in enumerate(tokenized_text, start=0):\n",
    "            words_in_document = []\n",
    "            for word in sentence:\n",
    "\n",
    "                index_word = self.word2index.get(word)\n",
    "                \n",
    "                if word in self.word2index.keys():\n",
    "                    tf_array[d_i][index_word] += 1\n",
    "                    words_per_document[d_i] += 1\n",
    "                    # Inverse Document Frequency\n",
    "                    if word not in words_in_document: # does not count repeated words in the same document\n",
    "                        words_in_document.append(word) \n",
    "                        idf_array[index_word] += 1 # number of documents containing the term\n",
    "        tf = (tf_array + 1)/(words_per_document.reshape(-1, 1) + 1)\n",
    "        # Smoothing: to avoid division by zero errors and to ensure that terms with zero document\n",
    "        # frequency still get a non-zero IDF score\n",
    "        idf = np.log((n_documents + 1)/(idf_array + 1)) + 1 # Smoothing\n",
    "\n",
    "        self.idf = idf\n",
    "        tf_idf = tf*idf\n",
    "        return tf_idf # Shape (n_documents, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Refer to Chapter 5 for more details on how to implement a LogisticRegression\n",
    "\"\"\"\n",
    "from work.Model import *\n",
    "\n",
    "class LogisticRegression(Model):\n",
    "    def __init__(self, model_file, learning_rate, epochs, threshold, max_features):\n",
    "        super(LogisticRegression, self).__init__(model_file)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.Y_to_categorical = None # Map Y label to numerical\n",
    "        self.threshold = threshold\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def initialize_weights(self, num_features, num_labels):\n",
    "        self.weights = np.zeros((num_features, num_labels))\n",
    "        self.bias = np.zeros(num_labels)\n",
    "        # np.random.seed(0)\n",
    "        # self.weights = np.random.rand(num_features, num_labels)\n",
    "        # self.bias = np.random.rand(num_labels)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax function: normalizing logit scores\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return e^Z/sum_{i=0}^{k}{e^{Z}}\n",
    "        \"\"\"\n",
    "        return np.exp(Z - np.max(Z, axis=1, keepdims=True))/np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"Sigmoid function for binary classification\n",
    "\n",
    "        :param Z([num_documents, num_labels])\n",
    "        :return 1/(1+e^{-Z})\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "        \n",
    "    def predict_prob(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of shape [num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "        return S\n",
    "\n",
    "    def cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate the cross-entropy\n",
    "        L = -1/n*_sum_{i=0}^{n}{y_i*log(s_i)} \n",
    "        y label is a vector containing K classes where yc = 1 if c is the correct class and the remaining elements will be 0.\n",
    "\n",
    "        :param S[num_documents, num_labels]: probabilities of features after softmax\n",
    "        :target [num_documents, num_labels]: target one hot encoded\n",
    "        \"\"\"\n",
    "        return -np.mean(np.log(S)*target)\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, S, target):\n",
    "        \"\"\"Calculate Binary cross-entropy\n",
    "        \"\"\"\n",
    "        return  -np.mean(target*(np.log(S)) + (1-target)*np.log(1-S))\n",
    "\n",
    "    def OneHot(self, targets, num_labels):\n",
    "        \"\"\"Convert arrary of targets to One Hot \n",
    "        :param targets([num_documents,])\n",
    "        :param num_labels(int)\n",
    "        :return Y[num_documents, num_labels]\n",
    "        \"\"\"\n",
    "        Y_onehot = np.zeros((len(targets), num_labels))\n",
    "        Y_onehot[np.arange(len(targets)), targets] = 1\n",
    "        return Y_onehot\n",
    "    \n",
    "    def predict(self, X, weights, bias, multinomial):\n",
    "        \"\"\"Return prediction of X with the categorical values]\n",
    "        \"\"\"\n",
    "        # z[num_documents, num_labels] = X[num_documents, num_features]*W[num_features, num_labels] + bias[num_labels]\n",
    "        Z = np.dot(X, weights) + bias\n",
    "\n",
    "        if multinomial:\n",
    "            # Apply Softmax\n",
    "            S = self.softmax(Z)\n",
    "\n",
    "            # Rows with highest probability\n",
    "            S_max = np.argmax(S, axis=1)\n",
    "        else:\n",
    "            # Apply Sigmoid\n",
    "            S = self.sigmoid(Z)\n",
    "            print(S)\n",
    "            # Rows with highest probability\n",
    "            S_max = [1 if i > 0.5 else 0 for i in S]\n",
    "\n",
    "        return S_max\n",
    "    \n",
    "\n",
    "    def train(self, input_file, verbose=False):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset and create vocabulary\n",
    "        features_lr_class = Features_LR(input_file, self.threshold, self.max_features)\n",
    "\n",
    "        # Transform dataset to TF-IDF space\n",
    "        # Return features with format (n_documents, size_vocabulary)\n",
    "        X = features_lr_class.tf_idf(features_lr_class.tokenized_text)\n",
    "        \n",
    "        # Y\n",
    "        Y_mapping = {label: index for index, label in enumerate(np.unique(features_lr_class.labels))}\n",
    "        self.Y_to_categorical = {index: label for label, index in Y_mapping.items()} # dictionary to convert back y's to categorical\n",
    "        Y = [Y_mapping[y] for y in features_lr_class.labels]\n",
    "\n",
    "        # Initialize Weights\n",
    "        sample_size = len(features_lr_class.tokenized_text)\n",
    "        n_features = len(features_lr_class.vocabulary)\n",
    "        num_labels = len(features_lr_class.labelset)\n",
    "\n",
    "\n",
    "        # Check if it's multinomial or binary classification\n",
    "        if num_labels == 2:\n",
    "            multinomial = False\n",
    "            num_labels = 1 # Only one column to reference 0 or 1\n",
    "        else:\n",
    "            multinomial = True\n",
    "\n",
    "        self.initialize_weights(n_features, num_labels)\n",
    "\n",
    "        # One Hot encoded Y\n",
    "        if multinomial:\n",
    "            Y_onehot = self.OneHot(Y, num_labels)\n",
    "        else:\n",
    "            Y_onehot = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Z = softmax(X*W + b)\n",
    "            prob = self.predict_prob(X, self.weights, self.bias, multinomial)\n",
    "\n",
    "            # break            \n",
    "            # dL/dW\n",
    "            grad_w = (1/sample_size)*np.dot(X.T, prob - Y_onehot)\n",
    "            grad_b =  (1/sample_size)*np.sum(prob - Y_onehot, axis=0)\n",
    "\n",
    "            # Updating weights and bias\n",
    "            # if balanced:\n",
    "            #     grad_w = grad_w * class_weights\n",
    "\n",
    "            self.weights = self.weights - (self.learning_rate*grad_w)\n",
    "            self.bias = self.bias - (self.learning_rate*grad_b)\n",
    "\n",
    "            # Computing cross-entropy loss\n",
    "            if multinomial:\n",
    "                loss = self.cross_entropy_loss(prob, Y_onehot)\n",
    "            else:\n",
    "                loss = self.binary_cross_entropy_loss(prob, Y_onehot)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i+1} - Loss: {loss}\")\n",
    "\n",
    "        model = {\n",
    "            \"feature_weights\": {\n",
    "                \"weights\": self.weights,\n",
    "                \"bias\": self.bias,\n",
    "                \"Y_to_categorical\": self.Y_to_categorical\n",
    "            },\n",
    "            \"Feature\": features_lr_class\n",
    "        }\n",
    "        ## Save the model\n",
    "        self.save_model(model)\n",
    "        return X, Y_onehot, prob\n",
    "\n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_fixle: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\"\n",
    "        ## TODO write your code here (and change return)\n",
    "\n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_LR_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_LR_class.read_inference_file(input_file)\n",
    "        X = []\n",
    "\n",
    "        # Get features from inference file\n",
    "        for sentence in tokenized_text:\n",
    "            # Transform dataset to TF-IDF space\n",
    "            # Return features with format (1, size_vocabulary)\n",
    "            X_sentence = Feature_LR_class.get_features(sentence, Feature_LR_class.idf)\n",
    "\n",
    "            # Concatenate A and B vertically\n",
    "            X.append(X_sentence)\n",
    "\n",
    "        X = np.vstack(X)\n",
    "\n",
    "        # Prediction\n",
    "        multinomial = True if len(feature_weights['Y_to_categorical'].keys()) > 2 else False\n",
    "        preds_numerical = self.predict(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        # Map indexes to Categorical space\n",
    "        preds_label = []\n",
    "        probs = self.predict_prob(X, feature_weights['weights'], feature_weights['bias'], multinomial)\n",
    "        for y in preds_numerical:\n",
    "            tmp = feature_weights['Y_to_categorical'][y]\n",
    "            preds_label.append(tmp)\n",
    "        \n",
    "        return preds_label, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "train_file = \"work/datasets/questions/train.txt\"\n",
    "pred_file = \"work/datasets/questions/val.test\"\n",
    "pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "model_file_name = \"logreg.questions.model\"\n",
    "model_LR = LogisticRegression(model_file_name, learning_rate=0.00005, epochs=1000, threshold=1, max_features=500)\n",
    "\n",
    "# # odiya\n",
    "# train_file = \"work/datasets/odiya/train.txt\"\n",
    "# pred_file = \"work/datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "# model_file_name = \"logreg.odiya.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.000001, epochs=1000, threshold=10, max_features=1000)\n",
    "\n",
    "\n",
    "# 4dim\n",
    "# train_file = \"work/datasets/4dim/train.txt\"\n",
    "# pred_file = \"work/datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "# model_file_name = \"logreg.4dim.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.2, epochs=200, threshold=1, max_features=100)\n",
    "\n",
    "\n",
    "# # Products\n",
    "# train_file = \"work/datasets/products/train.txt\"\n",
    "# pred_file = \"work/datasets/products/val.test\"\n",
    "# pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "# model_file_name = \"logreg.products.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.9, epochs=1000, threshold=1, max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.29862657820467586\n",
      "Epoch: 2 - Loss: 0.2988450038709276\n",
      "Epoch: 3 - Loss: 0.2990650998372262\n",
      "Epoch: 4 - Loss: 0.29928685496032864\n",
      "Epoch: 5 - Loss: 0.29951025852295926\n",
      "Epoch: 6 - Loss: 0.29973530022087197\n",
      "Epoch: 7 - Loss: 0.29996197015032944\n",
      "Epoch: 8 - Loss: 0.3001902587959862\n",
      "Epoch: 9 - Loss: 0.3004201570191589\n",
      "Epoch: 10 - Loss: 0.30065165604647276\n",
      "Epoch: 11 - Loss: 0.3008847474588693\n",
      "Epoch: 12 - Loss: 0.30111942318096463\n",
      "Epoch: 13 - Loss: 0.3013556754707443\n",
      "Epoch: 14 - Loss: 0.3015934969095842\n",
      "Epoch: 15 - Loss: 0.3018328803925857\n",
      "Epoch: 16 - Loss: 0.30207381911921477\n",
      "Epoch: 17 - Loss: 0.3023163065842338\n",
      "Epoch: 18 - Loss: 0.3025603365689159\n",
      "Epoch: 19 - Loss: 0.3028059031325331\n",
      "Epoch: 20 - Loss: 0.3030530006041075\n",
      "Epoch: 21 - Loss: 0.30330162357441737\n",
      "Epoch: 22 - Loss: 0.30355176688824864\n",
      "Epoch: 23 - Loss: 0.3038034256368844\n",
      "Epoch: 24 - Loss: 0.30405659515082223\n",
      "Epoch: 25 - Loss: 0.30431127099271416\n",
      "Epoch: 26 - Loss: 0.3045674489505195\n",
      "Epoch: 27 - Loss: 0.30482512503086406\n",
      "Epoch: 28 - Loss: 0.3050842954525989\n",
      "Epoch: 29 - Loss: 0.3053449566405512\n",
      "Epoch: 30 - Loss: 0.3056071052194614\n",
      "Epoch: 31 - Loss: 0.3058707380080993\n",
      "Epoch: 32 - Loss: 0.3061358520135544\n",
      "Epoch: 33 - Loss: 0.3064024444256934\n",
      "Epoch: 34 - Loss: 0.30667051261177924\n",
      "Epoch: 35 - Loss: 0.3069400541112473\n",
      "Epoch: 36 - Loss: 0.3072110666306323\n",
      "Epoch: 37 - Loss: 0.30748354803864136\n",
      "Epoch: 38 - Loss: 0.30775749636136857\n",
      "Epoch: 39 - Loss: 0.3080329097776453\n",
      "Epoch: 40 - Loss: 0.30830978661452324\n",
      "Epoch: 41 - Loss: 0.3085881253428846\n",
      "Epoch: 42 - Loss: 0.3088679245731754\n",
      "Epoch: 43 - Loss: 0.30914918305125844\n",
      "Epoch: 44 - Loss: 0.3094318996543811\n",
      "Epoch: 45 - Loss: 0.30971607338725526\n",
      "Epoch: 46 - Loss: 0.31000170337824423\n",
      "Epoch: 47 - Loss: 0.3102887888756549\n",
      "Epoch: 48 - Loss: 0.3105773292441302\n",
      "Epoch: 49 - Loss: 0.31086732396113936\n",
      "Epoch: 50 - Loss: 0.31115877261356306\n",
      "Epoch: 51 - Loss: 0.3114516748943692\n",
      "Epoch: 52 - Loss: 0.3117460305993785\n",
      "Epoch: 53 - Loss: 0.31204183962411425\n",
      "Epoch: 54 - Loss: 0.31233910196073655\n",
      "Epoch: 55 - Loss: 0.3126378176950556\n",
      "Epoch: 56 - Loss: 0.31293798700362374\n",
      "Epoch: 57 - Loss: 0.3132396101509021\n",
      "Epoch: 58 - Loss: 0.31354268748650127\n",
      "Epoch: 59 - Loss: 0.31384721944249133\n",
      "Epoch: 60 - Loss: 0.3141532065307815\n",
      "Epoch: 61 - Loss: 0.3144606493405658\n",
      "Epoch: 62 - Loss: 0.31476954853583283\n",
      "Epoch: 63 - Loss: 0.31507990485293885\n",
      "Epoch: 64 - Loss: 0.3153917190982398\n",
      "Epoch: 65 - Loss: 0.31570499214578335\n",
      "Epoch: 66 - Loss: 0.31601972493505737\n",
      "Epoch: 67 - Loss: 0.31633591846879333\n",
      "Epoch: 68 - Loss: 0.3166535738108234\n",
      "Epoch: 69 - Loss: 0.31697269208398965\n",
      "Epoch: 70 - Loss: 0.3172932744681033\n",
      "Epoch: 71 - Loss: 0.31761532219795313\n",
      "Epoch: 72 - Loss: 0.31793883656136085\n",
      "Epoch: 73 - Loss: 0.3182638188972835\n",
      "Epoch: 74 - Loss: 0.3185902705939597\n",
      "Epoch: 75 - Loss: 0.31891819308709957\n",
      "Epoch: 76 - Loss: 0.319247587858117\n",
      "Epoch: 77 - Loss: 0.3195784564324032\n",
      "Epoch: 78 - Loss: 0.319910800377639\n",
      "Epoch: 79 - Loss: 0.32024462130214737\n",
      "Epoch: 80 - Loss: 0.3205799208532822\n",
      "Epoch: 81 - Loss: 0.3209167007158546\n",
      "Epoch: 82 - Loss: 0.3212549626105939\n",
      "Epoch: 83 - Loss: 0.32159470829264414\n",
      "Epoch: 84 - Loss: 0.3219359395500937\n",
      "Epoch: 85 - Loss: 0.32227865820253754\n",
      "Epoch: 86 - Loss: 0.3226228660996713\n",
      "Epoch: 87 - Loss: 0.3229685651199166\n",
      "Epoch: 88 - Loss: 0.32331575716907657\n",
      "Epoch: 89 - Loss: 0.32366444417902035\n",
      "Epoch: 90 - Loss: 0.32401462810639586\n",
      "Epoch: 91 - Loss: 0.32436631093137175\n",
      "Epoch: 92 - Loss: 0.3247194946564049\n",
      "Epoch: 93 - Loss: 0.32507418130503507\n",
      "Epoch: 94 - Loss: 0.32543037292070515\n",
      "Epoch: 95 - Loss: 0.32578807156560674\n",
      "Epoch: 96 - Loss: 0.3261472793195496\n",
      "Epoch: 97 - Loss: 0.32650799827885546\n",
      "Epoch: 98 - Loss: 0.32687023055527437\n",
      "Epoch: 99 - Loss: 0.3272339782749248\n",
      "Epoch: 100 - Loss: 0.32759924357725456\n",
      "Epoch: 101 - Loss: 0.32796602861402435\n",
      "Epoch: 102 - Loss: 0.3283343355483115\n",
      "Epoch: 103 - Loss: 0.3287041665535349\n",
      "Epoch: 104 - Loss: 0.32907552381249955\n",
      "Epoch: 105 - Loss: 0.32944840951646076\n",
      "Epoch: 106 - Loss: 0.3298228258642077\n",
      "Epoch: 107 - Loss: 0.33019877506116513\n",
      "Epoch: 108 - Loss: 0.3305762593185142\n",
      "Epoch: 109 - Loss: 0.3309552808523298\n",
      "Epoch: 110 - Loss: 0.3313358418827367\n",
      "Epoch: 111 - Loss: 0.3317179446330816\n",
      "Epoch: 112 - Loss: 0.33210159132912265\n",
      "Epoch: 113 - Loss: 0.3324867841982347\n",
      "Epoch: 114 - Loss: 0.33287352546863086\n",
      "Epoch: 115 - Loss: 0.33326181736859894\n",
      "Epoch: 116 - Loss: 0.3336516621257545\n",
      "Epoch: 117 - Loss: 0.33404306196630723\n",
      "Epoch: 118 - Loss: 0.33443601911434295\n",
      "Epoch: 119 - Loss: 0.3348305357911196\n",
      "Epoch: 120 - Loss: 0.3352266142143774\n",
      "Epoch: 121 - Loss: 0.33562425659766243\n",
      "Epoch: 122 - Loss: 0.33602346514966386\n",
      "Epoch: 123 - Loss: 0.33642424207356453\n",
      "Epoch: 124 - Loss: 0.33682658956640377\n",
      "Epoch: 125 - Loss: 0.33723050981845415\n",
      "Epoch: 126 - Loss: 0.3376360050126086\n",
      "Epoch: 127 - Loss: 0.3380430773237823\n",
      "Epoch: 128 - Loss: 0.33845172891832415\n",
      "Epoch: 129 - Loss: 0.33886196195344137\n",
      "Epoch: 130 - Loss: 0.33927377857663527\n",
      "Epoch: 131 - Loss: 0.3396871809251479\n",
      "Epoch: 132 - Loss: 0.34010217112542024\n",
      "Epoch: 133 - Loss: 0.3405187512925614\n",
      "Epoch: 134 - Loss: 0.34093692352982746\n",
      "Epoch: 135 - Loss: 0.3413566899281129\n",
      "Epoch: 136 - Loss: 0.3417780525654494\n",
      "Epoch: 137 - Loss: 0.34220101350651766\n",
      "Epoch: 138 - Loss: 0.3426255748021674\n",
      "Epoch: 139 - Loss: 0.34305173848894754\n",
      "Epoch: 140 - Loss: 0.34347950658864645\n",
      "Epoch: 141 - Loss: 0.3439088811078412\n",
      "Epoch: 142 - Loss: 0.3443398640374562\n",
      "Epoch: 143 - Loss: 0.34477245735233136\n",
      "Epoch: 144 - Loss: 0.3452066630107985\n",
      "Epoch: 145 - Loss: 0.34564248295426747\n",
      "Epoch: 146 - Loss: 0.34607991910682084\n",
      "Epoch: 147 - Loss: 0.34651897337481635\n",
      "Epoch: 148 - Loss: 0.34695964764649934\n",
      "Epoch: 149 - Loss: 0.34740194379162165\n",
      "Epoch: 150 - Loss: 0.3478458636610707\n",
      "Epoch: 151 - Loss: 0.3482914090865052\n",
      "Epoch: 152 - Loss: 0.3487385818799993\n",
      "Epoch: 153 - Loss: 0.3491873838336951\n",
      "Epoch: 154 - Loss: 0.3496378167194619\n",
      "Epoch: 155 - Loss: 0.350089882288564\n",
      "Epoch: 156 - Loss: 0.3505435822713356\n",
      "Epoch: 157 - Loss: 0.350998918376863\n",
      "Epoch: 158 - Loss: 0.3514558922926747\n",
      "Epoch: 159 - Loss: 0.3519145056844381\n",
      "Epoch: 160 - Loss: 0.35237476019566355\n",
      "Epoch: 161 - Loss: 0.3528366574474156\n",
      "Epoch: 162 - Loss: 0.35330019903803117\n",
      "Epoch: 163 - Loss: 0.35376538654284395\n",
      "Epoch: 164 - Loss: 0.35423222151391665\n",
      "Epoch: 165 - Loss: 0.354700705479779\n",
      "Epoch: 166 - Loss: 0.3551708399451729\n",
      "Epoch: 167 - Loss: 0.35564262639080374\n",
      "Epoch: 168 - Loss: 0.35611606627309816\n",
      "Epoch: 169 - Loss: 0.35659116102396865\n",
      "Epoch: 170 - Loss: 0.35706791205058375\n",
      "Epoch: 171 - Loss: 0.3575463207351447\n",
      "Epoch: 172 - Loss: 0.3580263884346686\n",
      "Epoch: 173 - Loss: 0.35850811648077685\n",
      "Epoch: 174 - Loss: 0.3589915061794903\n",
      "Epoch: 175 - Loss: 0.35947655881103\n",
      "Epoch: 176 - Loss: 0.3599632756296237\n",
      "Epoch: 177 - Loss: 0.36045165786331806\n",
      "Epoch: 178 - Loss: 0.36094170671379716\n",
      "Epoch: 179 - Loss: 0.3614334233562059\n",
      "Epoch: 180 - Loss: 0.3619268089389792\n",
      "Epoch: 181 - Loss: 0.36242186458367714\n",
      "Epoch: 182 - Loss: 0.3629185913848251\n",
      "Epoch: 183 - Loss: 0.36341699040975917\n",
      "Epoch: 184 - Loss: 0.3639170626984778\n",
      "Epoch: 185 - Loss: 0.36441880926349707\n",
      "Epoch: 186 - Loss: 0.3649222310897132\n",
      "Epoch: 187 - Loss: 0.3654273291342686\n",
      "Epoch: 188 - Loss: 0.3659341043264237\n",
      "Epoch: 189 - Loss: 0.3664425575674338\n",
      "Epoch: 190 - Loss: 0.3669526897304312\n",
      "Epoch: 191 - Loss: 0.3674645016603113\n",
      "Epoch: 192 - Loss: 0.3679779941736246\n",
      "Epoch: 193 - Loss: 0.36849316805847326\n",
      "Epoch: 194 - Loss: 0.3690100240744124\n",
      "Epoch: 195 - Loss: 0.36952856295235553\n",
      "Epoch: 196 - Loss: 0.37004878539448594\n",
      "Epoch: 197 - Loss: 0.37057069207417154\n",
      "Epoch: 198 - Loss: 0.37109428363588487\n",
      "Epoch: 199 - Loss: 0.37161956069512764\n",
      "Epoch: 200 - Loss: 0.3721465238383596\n",
      "Epoch: 201 - Loss: 0.3726751736229318\n",
      "Epoch: 202 - Loss: 0.3732055105770244\n",
      "Epoch: 203 - Loss: 0.37373753519958863\n",
      "Epoch: 204 - Loss: 0.37427124796029376\n",
      "Epoch: 205 - Loss: 0.37480664929947693\n",
      "Epoch: 206 - Loss: 0.3753437396280985\n",
      "Epoch: 207 - Loss: 0.37588251932770145\n",
      "Epoch: 208 - Loss: 0.3764229887503733\n",
      "Epoch: 209 - Loss: 0.37696514821871474\n",
      "Epoch: 210 - Loss: 0.3775089980258101\n",
      "Epoch: 211 - Loss: 0.37805453843520265\n",
      "Epoch: 212 - Loss: 0.3786017696808742\n",
      "Epoch: 213 - Loss: 0.3791506919672275\n",
      "Epoch: 214 - Loss: 0.37970130546907394\n",
      "Epoch: 215 - Loss: 0.3802536103316239\n",
      "Epoch: 216 - Loss: 0.3808076066704813\n",
      "Epoch: 217 - Loss: 0.3813632945716419\n",
      "Epoch: 218 - Loss: 0.3819206740914954\n",
      "Epoch: 219 - Loss: 0.382479745256831\n",
      "Epoch: 220 - Loss: 0.3830405080648465\n",
      "Epoch: 221 - Loss: 0.38360296248316106\n",
      "Epoch: 222 - Loss: 0.384167108449832\n",
      "Epoch: 223 - Loss: 0.3847329458733737\n",
      "Epoch: 224 - Loss: 0.3853004746327817\n",
      "Epoch: 225 - Loss: 0.385869694577559\n",
      "Epoch: 226 - Loss: 0.3864406055277459\n",
      "Epoch: 227 - Loss: 0.3870132072739536\n",
      "Epoch: 228 - Loss: 0.3875874995774006\n",
      "Epoch: 229 - Loss: 0.38816348216995283\n",
      "Epoch: 230 - Loss: 0.3887411547541664\n",
      "Epoch: 231 - Loss: 0.3893205170033339\n",
      "Epoch: 232 - Loss: 0.3899015685615341\n",
      "Epoch: 233 - Loss: 0.39048430904368364\n",
      "Epoch: 234 - Loss: 0.39106873803559344\n",
      "Epoch: 235 - Loss: 0.3916548550940263\n",
      "Epoch: 236 - Loss: 0.3922426597467591\n",
      "Epoch: 237 - Loss: 0.3928321514926465\n",
      "Epoch: 238 - Loss: 0.39342332980168876\n",
      "Epoch: 239 - Loss: 0.39401619411510164\n",
      "Epoch: 240 - Loss: 0.3946107438453894\n",
      "Epoch: 241 - Loss: 0.3952069783764205\n",
      "Epoch: 242 - Loss: 0.39580489706350597\n",
      "Epoch: 243 - Loss: 0.39640449923348037\n",
      "Epoch: 244 - Loss: 0.39700578418478644\n",
      "Epoch: 245 - Loss: 0.39760875118756045\n",
      "Epoch: 246 - Loss: 0.39821339948372203\n",
      "Epoch: 247 - Loss: 0.39881972828706524\n",
      "Epoch: 248 - Loss: 0.399427736783353\n",
      "Epoch: 249 - Loss: 0.4000374241304135\n",
      "Epoch: 250 - Loss: 0.400648789458239\n",
      "Epoch: 251 - Loss: 0.40126183186908754\n",
      "Epoch: 252 - Loss: 0.40187655043758647\n",
      "Epoch: 253 - Loss: 0.4024929442108387\n",
      "Epoch: 254 - Loss: 0.40311101220853074\n",
      "Epoch: 255 - Loss: 0.4037307534230436\n",
      "Epoch: 256 - Loss: 0.4043521668195656\n",
      "Epoch: 257 - Loss: 0.404975251336207\n",
      "Epoch: 258 - Loss: 0.4056000058841177\n",
      "Epoch: 259 - Loss: 0.40622642934760594\n",
      "Epoch: 260 - Loss: 0.40685452058425975\n",
      "Epoch: 261 - Loss: 0.40748427842507057\n",
      "Epoch: 262 - Loss: 0.4081157016745581\n",
      "Epoch: 263 - Loss: 0.40874878911089796\n",
      "Epoch: 264 - Loss: 0.4093835394860506\n",
      "Epoch: 265 - Loss: 0.4100199515258926\n",
      "Epoch: 266 - Loss: 0.41065802393034945\n",
      "Epoch: 267 - Loss: 0.4112977553735304\n",
      "Epoch: 268 - Loss: 0.41193914450386476\n",
      "Epoch: 269 - Loss: 0.41258218994424045\n",
      "Epoch: 270 - Loss: 0.4132268902921439\n",
      "Epoch: 271 - Loss: 0.41387324411980125\n",
      "Epoch: 272 - Loss: 0.41452124997432266\n",
      "Epoch: 273 - Loss: 0.41517090637784587\n",
      "Epoch: 274 - Loss: 0.4158222118276836\n",
      "Epoch: 275 - Loss: 0.4164751647964713\n",
      "Epoch: 276 - Loss: 0.41712976373231636\n",
      "Epoch: 277 - Loss: 0.4177860070589494\n",
      "Epoch: 278 - Loss: 0.4184438931758762\n",
      "Epoch: 279 - Loss: 0.4191034204585322\n",
      "Epoch: 280 - Loss: 0.419764587258437\n",
      "Epoch: 281 - Loss: 0.4204273919033506\n",
      "Epoch: 282 - Loss: 0.4210918326974319\n",
      "Epoch: 283 - Loss: 0.4217579079213973\n",
      "Epoch: 284 - Loss: 0.4224256158326809\n",
      "Epoch: 285 - Loss: 0.42309495466559605\n",
      "Epoch: 286 - Loss: 0.42376592263149776\n",
      "Epoch: 287 - Loss: 0.4244385179189467\n",
      "Epoch: 288 - Loss: 0.42511273869387367\n",
      "Epoch: 289 - Loss: 0.42578858309974593\n",
      "Epoch: 290 - Loss: 0.4264660492577336\n",
      "Epoch: 291 - Loss: 0.4271451352668779\n",
      "Epoch: 292 - Loss: 0.4278258392042601\n",
      "Epoch: 293 - Loss: 0.4285081591251709\n",
      "Epoch: 294 - Loss: 0.42919209306328165\n",
      "Epoch: 295 - Loss: 0.42987763903081566\n",
      "Epoch: 296 - Loss: 0.4305647950187207\n",
      "Epoch: 297 - Loss: 0.43125355899684226\n",
      "Epoch: 298 - Loss: 0.43194392891409744\n",
      "Epoch: 299 - Loss: 0.4326359026986502\n",
      "Epoch: 300 - Loss: 0.43332947825808615\n",
      "Epoch: 301 - Loss: 0.43402465347958935\n",
      "Epoch: 302 - Loss: 0.4347214262301189\n",
      "Epoch: 303 - Loss: 0.43541979435658634\n",
      "Epoch: 304 - Loss: 0.4361197556860339\n",
      "Epoch: 305 - Loss: 0.4368213080258132\n",
      "Epoch: 306 - Loss: 0.4375244491637642\n",
      "Epoch: 307 - Loss: 0.43822917686839535\n",
      "Epoch: 308 - Loss: 0.43893548888906325\n",
      "Epoch: 309 - Loss: 0.4396433829561538\n",
      "Epoch: 310 - Loss: 0.440352856781263\n",
      "Epoch: 311 - Loss: 0.44106390805737866\n",
      "Epoch: 312 - Loss: 0.44177653445906223\n",
      "Epoch: 313 - Loss: 0.4424907336426307\n",
      "Epoch: 314 - Loss: 0.44320650324633987\n",
      "Epoch: 315 - Loss: 0.4439238408905664\n",
      "Epoch: 316 - Loss: 0.4446427441779913\n",
      "Epoch: 317 - Loss: 0.4453632106937837\n",
      "Epoch: 318 - Loss: 0.4460852380057837\n",
      "Epoch: 319 - Loss: 0.44680882366468627\n",
      "Epoch: 320 - Loss: 0.4475339652042259\n",
      "Epoch: 321 - Loss: 0.4482606601413599\n",
      "Epoch: 322 - Loss: 0.4489889059764527\n",
      "Epoch: 323 - Loss: 0.4497187001934605\n",
      "Epoch: 324 - Loss: 0.4504500402601154\n",
      "Epoch: 325 - Loss: 0.45118292362811\n",
      "Epoch: 326 - Loss: 0.45191734773328124\n",
      "Epoch: 327 - Loss: 0.4526533099957958\n",
      "Epoch: 328 - Loss: 0.45339080782033336\n",
      "Epoch: 329 - Loss: 0.4541298385962718\n",
      "Epoch: 330 - Loss: 0.4548703996978714\n",
      "Epoch: 331 - Loss: 0.4556124884844584\n",
      "Epoch: 332 - Loss: 0.4563561023006097\n",
      "Epoch: 333 - Loss: 0.45710123847633655\n",
      "Epoch: 334 - Loss: 0.4578478943272682\n",
      "Epoch: 335 - Loss: 0.4585960671548357\n",
      "Epoch: 336 - Loss: 0.4593457542464553\n",
      "Epoch: 337 - Loss: 0.46009695287571106\n",
      "Epoch: 338 - Loss: 0.4608496603025387\n",
      "Epoch: 339 - Loss: 0.4616038737734076\n",
      "Epoch: 340 - Loss: 0.462359590521503\n",
      "Epoch: 341 - Loss: 0.4631168077669089\n",
      "Epoch: 342 - Loss: 0.4638755227167888\n",
      "Epoch: 343 - Loss: 0.46463573256556795\n",
      "Epoch: 344 - Loss: 0.4653974344951139\n",
      "Epoch: 345 - Loss: 0.46616062567491723\n",
      "Epoch: 346 - Loss: 0.4669253032622721\n",
      "Epoch: 347 - Loss: 0.46769146440245546\n",
      "Epoch: 348 - Loss: 0.46845910622890774\n",
      "Epoch: 349 - Loss: 0.46922822586340995\n",
      "Epoch: 350 - Loss: 0.4699988204162642\n",
      "Epoch: 351 - Loss: 0.47077088698647007\n",
      "Epoch: 352 - Loss: 0.47154442266190316\n",
      "Epoch: 353 - Loss: 0.4723194245194919\n",
      "Epoch: 354 - Loss: 0.4730958896253933\n",
      "Epoch: 355 - Loss: 0.47387381503517045\n",
      "Epoch: 356 - Loss: 0.4746531977939661\n",
      "Epoch: 357 - Loss: 0.4754340349366785\n",
      "Epoch: 358 - Loss: 0.47621632348813575\n",
      "Epoch: 359 - Loss: 0.4770000604632686\n",
      "Epoch: 360 - Loss: 0.4777852428672842\n",
      "Epoch: 361 - Loss: 0.47857186769583765\n",
      "Epoch: 362 - Loss: 0.4793599319352047\n",
      "Epoch: 363 - Loss: 0.48014943256245196\n",
      "Epoch: 364 - Loss: 0.4809403665456074\n",
      "Epoch: 365 - Loss: 0.4817327308438305\n",
      "Epoch: 366 - Loss: 0.4825265224075805\n",
      "Epoch: 367 - Loss: 0.4833217381787852\n",
      "Epoch: 368 - Loss: 0.4841183750910083\n",
      "Epoch: 369 - Loss: 0.4849164300696162\n",
      "Epoch: 370 - Loss: 0.48571590003194415\n",
      "Epoch: 371 - Loss: 0.4865167818874612\n",
      "Epoch: 372 - Loss: 0.48731907253793516\n",
      "Epoch: 373 - Loss: 0.4881227688775962\n",
      "Epoch: 374 - Loss: 0.48892786779329944\n",
      "Epoch: 375 - Loss: 0.48973436616468774\n",
      "Epoch: 376 - Loss: 0.4905422608643525\n",
      "Epoch: 377 - Loss: 0.4913515487579942\n",
      "Epoch: 378 - Loss: 0.4921622267045826\n",
      "Epoch: 379 - Loss: 0.49297429155651495\n",
      "Epoch: 380 - Loss: 0.49378774015977445\n",
      "Epoch: 381 - Loss: 0.49460256935408686\n",
      "Epoch: 382 - Loss: 0.4954187759730775\n",
      "Epoch: 383 - Loss: 0.49623635684442596\n",
      "Epoch: 384 - Loss: 0.497055308790021\n",
      "Epoch: 385 - Loss: 0.49787562862611406\n",
      "Epoch: 386 - Loss: 0.49869731316347216\n",
      "Epoch: 387 - Loss: 0.49952035920752913\n",
      "Epoch: 388 - Loss: 0.5003447635585373\n",
      "Epoch: 389 - Loss: 0.5011705230117173\n",
      "Epoch: 390 - Loss: 0.5019976343574065\n",
      "Epoch: 391 - Loss: 0.5028260943812081\n",
      "Epoch: 392 - Loss: 0.5036558998641373\n",
      "Epoch: 393 - Loss: 0.5044870475827689\n",
      "Epoch: 394 - Loss: 0.5053195343093811\n",
      "Epoch: 395 - Loss: 0.5061533568121008\n",
      "Epoch: 396 - Loss: 0.506988511855047\n",
      "Epoch: 397 - Loss: 0.5078249961984729\n",
      "Epoch: 398 - Loss: 0.5086628065989073\n",
      "Epoch: 399 - Loss: 0.5095019398092949\n",
      "Epoch: 400 - Loss: 0.5103423925791375\n",
      "Epoch: 401 - Loss: 0.5111841616546298\n",
      "Epoch: 402 - Loss: 0.5120272437787989\n",
      "Epoch: 403 - Loss: 0.5128716356916404\n",
      "Epoch: 404 - Loss: 0.5137173341302538\n",
      "Epoch: 405 - Loss: 0.5145643358289768\n",
      "Epoch: 406 - Loss: 0.5154126375195197\n",
      "Epoch: 407 - Loss: 0.5162622359310971\n",
      "Epoch: 408 - Loss: 0.5171131277905598\n",
      "Epoch: 409 - Loss: 0.5179653098225258\n",
      "Epoch: 410 - Loss: 0.5188187787495084\n",
      "Epoch: 411 - Loss: 0.519673531292047\n",
      "Epoch: 412 - Loss: 0.5205295641688324\n",
      "Epoch: 413 - Loss: 0.5213868740968349\n",
      "Epoch: 414 - Loss: 0.5222454577914278\n",
      "Epoch: 415 - Loss: 0.5231053119665151\n",
      "Epoch: 416 - Loss: 0.5239664333346514\n",
      "Epoch: 417 - Loss: 0.5248288186071666\n",
      "Epoch: 418 - Loss: 0.5256924644942863\n",
      "Epoch: 419 - Loss: 0.5265573677052526\n",
      "Epoch: 420 - Loss: 0.5274235249484436\n",
      "Epoch: 421 - Loss: 0.5282909329314909\n",
      "Epoch: 422 - Loss: 0.5291595883613978\n",
      "Epoch: 423 - Loss: 0.5300294879446547\n",
      "Epoch: 424 - Loss: 0.5309006283873553\n",
      "Epoch: 425 - Loss: 0.5317730063953102\n",
      "Epoch: 426 - Loss: 0.53264661867416\n",
      "Epoch: 427 - Loss: 0.5335214619294878\n",
      "Epoch: 428 - Loss: 0.5343975328669304\n",
      "Epoch: 429 - Loss: 0.5352748281922879\n",
      "Epoch: 430 - Loss: 0.5361533446116333\n",
      "Epoch: 431 - Loss: 0.5370330788314204\n",
      "Epoch: 432 - Loss: 0.5379140275585907\n",
      "Epoch: 433 - Loss: 0.5387961875006798\n",
      "Epoch: 434 - Loss: 0.5396795553659219\n",
      "Epoch: 435 - Loss: 0.5405641278633544\n",
      "Epoch: 436 - Loss: 0.54144990170292\n",
      "Epoch: 437 - Loss: 0.5423368735955698\n",
      "Epoch: 438 - Loss: 0.5432250402533634\n",
      "Epoch: 439 - Loss: 0.5441143983895691\n",
      "Epoch: 440 - Loss: 0.5450049447187627\n",
      "Epoch: 441 - Loss: 0.5458966759569261\n",
      "Epoch: 442 - Loss: 0.546789588821543\n",
      "Epoch: 443 - Loss: 0.547683680031696\n",
      "Epoch: 444 - Loss: 0.5485789463081607\n",
      "Epoch: 445 - Loss: 0.5494753843735005\n",
      "Epoch: 446 - Loss: 0.5503729909521584\n",
      "Epoch: 447 - Loss: 0.55127176277055\n",
      "Epoch: 448 - Loss: 0.5521716965571538\n",
      "Epoch: 449 - Loss: 0.5530727890426015\n",
      "Epoch: 450 - Loss: 0.5539750369597668\n",
      "Epoch: 451 - Loss: 0.5548784370438538\n",
      "Epoch: 452 - Loss: 0.555782986032483\n",
      "Epoch: 453 - Loss: 0.5566886806657789\n",
      "Epoch: 454 - Loss: 0.5575955176864544\n",
      "Epoch: 455 - Loss: 0.5585034938398942\n",
      "Epoch: 456 - Loss: 0.5594126058742388\n",
      "Epoch: 457 - Loss: 0.5603228505404667\n",
      "Epoch: 458 - Loss: 0.5612342245924757\n",
      "Epoch: 459 - Loss: 0.5621467247871631\n",
      "Epoch: 460 - Loss: 0.563060347884504\n",
      "Epoch: 461 - Loss: 0.5639750906476322\n",
      "Epoch: 462 - Loss: 0.5648909498429152\n",
      "Epoch: 463 - Loss: 0.5658079222400317\n",
      "Epoch: 464 - Loss: 0.5667260046120486\n",
      "Epoch: 465 - Loss: 0.5676451937354926\n",
      "Epoch: 466 - Loss: 0.5685654863904271\n",
      "Epoch: 467 - Loss: 0.5694868793605232\n",
      "Epoch: 468 - Loss: 0.5704093694331326\n",
      "Epoch: 469 - Loss: 0.5713329533993576\n",
      "Epoch: 470 - Loss: 0.5722576280541226\n",
      "Epoch: 471 - Loss: 0.5731833901962423\n",
      "Epoch: 472 - Loss: 0.5741102366284904\n",
      "Epoch: 473 - Loss: 0.5750381641576671\n",
      "Epoch: 474 - Loss: 0.5759671695946653\n",
      "Epoch: 475 - Loss: 0.5768972497545374\n",
      "Epoch: 476 - Loss: 0.5778284014565587\n",
      "Epoch: 477 - Loss: 0.578760621524292\n",
      "Epoch: 478 - Loss: 0.5796939067856512\n",
      "Epoch: 479 - Loss: 0.5806282540729626\n",
      "Epoch: 480 - Loss: 0.5815636602230269\n",
      "Epoch: 481 - Loss: 0.5825001220771789\n",
      "Epoch: 482 - Loss: 0.5834376364813489\n",
      "Epoch: 483 - Loss: 0.5843762002861196\n",
      "Epoch: 484 - Loss: 0.585315810346785\n",
      "Epoch: 485 - Loss: 0.5862564635234079\n",
      "Epoch: 486 - Loss: 0.5871981566808755\n",
      "Epoch: 487 - Loss: 0.5881408866889555\n",
      "Epoch: 488 - Loss: 0.5890846504223505\n",
      "Epoch: 489 - Loss: 0.5900294447607518\n",
      "Epoch: 490 - Loss: 0.5909752665888934\n",
      "Epoch: 491 - Loss: 0.5919221127966028\n",
      "Epoch: 492 - Loss: 0.592869980278854\n",
      "Epoch: 493 - Loss: 0.593818865935817\n",
      "Epoch: 494 - Loss: 0.5947687666729087\n",
      "Epoch: 495 - Loss: 0.5957196794008416\n",
      "Epoch: 496 - Loss: 0.5966716010356724\n",
      "Epoch: 497 - Loss: 0.5976245284988492\n",
      "Epoch: 498 - Loss: 0.5985784587172589\n",
      "Epoch: 499 - Loss: 0.599533388623273\n",
      "Epoch: 500 - Loss: 0.6004893151547933\n",
      "Epoch: 501 - Loss: 0.6014462352552958\n",
      "Epoch: 502 - Loss: 0.6024041458738758\n",
      "Epoch: 503 - Loss: 0.6033630439652898\n",
      "Epoch: 504 - Loss: 0.6043229264899985\n",
      "Epoch: 505 - Loss: 0.6052837904142087\n",
      "Epoch: 506 - Loss: 0.6062456327099137\n",
      "Epoch: 507 - Loss: 0.6072084503549345\n",
      "Epoch: 508 - Loss: 0.6081722403329581\n",
      "Epoch: 509 - Loss: 0.6091369996335778\n",
      "Epoch: 510 - Loss: 0.61010272525233\n",
      "Epoch: 511 - Loss: 0.6110694141907328\n",
      "Epoch: 512 - Loss: 0.612037063456322\n",
      "Epoch: 513 - Loss: 0.6130056700626882\n",
      "Epoch: 514 - Loss: 0.6139752310295109\n",
      "Epoch: 515 - Loss: 0.6149457433825944\n",
      "Epoch: 516 - Loss: 0.6159172041539016\n",
      "Epoch: 517 - Loss: 0.6168896103815873\n",
      "Epoch: 518 - Loss: 0.6178629591100313\n",
      "Epoch: 519 - Loss: 0.6188372473898703\n",
      "Epoch: 520 - Loss: 0.6198124722780297\n",
      "Epoch: 521 - Loss: 0.6207886308377544\n",
      "Epoch: 522 - Loss: 0.621765720138639\n",
      "Epoch: 523 - Loss: 0.6227437372566577\n",
      "Epoch: 524 - Loss: 0.623722679274193\n",
      "Epoch: 525 - Loss: 0.624702543280064\n",
      "Epoch: 526 - Loss: 0.6256833263695556\n",
      "Epoch: 527 - Loss: 0.6266650256444434\n",
      "Epoch: 528 - Loss: 0.6276476382130225\n",
      "Epoch: 529 - Loss: 0.6286311611901322\n",
      "Epoch: 530 - Loss: 0.6296155916971816\n",
      "Epoch: 531 - Loss: 0.6306009268621756\n",
      "Epoch: 532 - Loss: 0.6315871638197375\n",
      "Epoch: 533 - Loss: 0.632574299711134\n",
      "Epoch: 534 - Loss: 0.633562331684298\n",
      "Epoch: 535 - Loss: 0.634551256893851\n",
      "Epoch: 536 - Loss: 0.6355410725011256\n",
      "Epoch: 537 - Loss: 0.6365317756741862\n",
      "Epoch: 538 - Loss: 0.637523363587851\n",
      "Epoch: 539 - Loss: 0.6385158334237119\n",
      "Epoch: 540 - Loss: 0.6395091823701543\n",
      "Epoch: 541 - Loss: 0.6405034076223766\n",
      "Epoch: 542 - Loss: 0.6414985063824091\n",
      "Epoch: 543 - Loss: 0.6424944758591328\n",
      "Epoch: 544 - Loss: 0.643491313268296\n",
      "Epoch: 545 - Loss: 0.6444890158325329\n",
      "Epoch: 546 - Loss: 0.6454875807813797\n",
      "Epoch: 547 - Loss: 0.6464870053512911\n",
      "Epoch: 548 - Loss: 0.6474872867856564\n",
      "Epoch: 549 - Loss: 0.6484884223348143\n",
      "Epoch: 550 - Loss: 0.6494904092560684\n",
      "Epoch: 551 - Loss: 0.6504932448137013\n",
      "Epoch: 552 - Loss: 0.651496926278988\n",
      "Epoch: 553 - Loss: 0.6525014509302106\n",
      "Epoch: 554 - Loss: 0.6535068160526704\n",
      "Epoch: 555 - Loss: 0.6545130189387001\n",
      "Epoch: 556 - Loss: 0.6555200568876768\n",
      "Epoch: 557 - Loss: 0.6565279272060338\n",
      "Epoch: 558 - Loss: 0.6575366272072705\n",
      "Epoch: 559 - Loss: 0.6585461542119642\n",
      "Epoch: 560 - Loss: 0.6595565055477802\n",
      "Epoch: 561 - Loss: 0.6605676785494821\n",
      "Epoch: 562 - Loss: 0.6615796705589402\n",
      "Epoch: 563 - Loss: 0.6625924789251413\n",
      "Epoch: 564 - Loss: 0.6636061010041985\n",
      "Epoch: 565 - Loss: 0.6646205341593572\n",
      "Epoch: 566 - Loss: 0.665635775761004\n",
      "Epoch: 567 - Loss: 0.6666518231866746\n",
      "Epoch: 568 - Loss: 0.6676686738210605\n",
      "Epoch: 569 - Loss: 0.6686863250560156\n",
      "Epoch: 570 - Loss: 0.6697047742905621\n",
      "Epoch: 571 - Loss: 0.6707240189308973\n",
      "Epoch: 572 - Loss: 0.6717440563903981\n",
      "Epoch: 573 - Loss: 0.6727648840896274\n",
      "Epoch: 574 - Loss: 0.6737864994563372\n",
      "Epoch: 575 - Loss: 0.6748088999254751\n",
      "Epoch: 576 - Loss: 0.6758320829391861\n",
      "Epoch: 577 - Loss: 0.6768560459468186\n",
      "Epoch: 578 - Loss: 0.6778807864049256\n",
      "Epoch: 579 - Loss: 0.6789063017772691\n",
      "Epoch: 580 - Loss: 0.6799325895348228\n",
      "Epoch: 581 - Loss: 0.6809596471557733\n",
      "Epoch: 582 - Loss: 0.6819874721255232\n",
      "Epoch: 583 - Loss: 0.6830160619366925\n",
      "Epoch: 584 - Loss: 0.6840454140891198\n",
      "Epoch: 585 - Loss: 0.6850755260898626\n",
      "Epoch: 586 - Loss: 0.6861063954531998\n",
      "Epoch: 587 - Loss: 0.6871380197006307\n",
      "Epoch: 588 - Loss: 0.6881703963608751\n",
      "Epoch: 589 - Loss: 0.6892035229698733\n",
      "Epoch: 590 - Loss: 0.6902373970707865\n",
      "Epoch: 591 - Loss: 0.6912720162139939\n",
      "Epoch: 592 - Loss: 0.6923073779570941\n",
      "Epoch: 593 - Loss: 0.6933434798649015\n",
      "Epoch: 594 - Loss: 0.6943803195094458\n",
      "Epoch: 595 - Loss: 0.6954178944699698\n",
      "Epoch: 596 - Loss: 0.6964562023329269\n",
      "Epoch: 597 - Loss: 0.6974952406919789\n",
      "Epoch: 598 - Loss: 0.6985350071479929\n",
      "Epoch: 599 - Loss: 0.6995754993090385\n",
      "Epoch: 600 - Loss: 0.7006167147903841\n",
      "Epoch: 601 - Loss: 0.7016586512144939\n",
      "Epoch: 602 - Loss: 0.7027013062110234\n",
      "Epoch: 603 - Loss: 0.7037446774168156\n",
      "Epoch: 604 - Loss: 0.7047887624758965\n",
      "Epoch: 605 - Loss: 0.7058335590394709\n",
      "Epoch: 606 - Loss: 0.7068790647659169\n",
      "Epoch: 607 - Loss: 0.7079252773207816\n",
      "Epoch: 608 - Loss: 0.7089721943767748\n",
      "Epoch: 609 - Loss: 0.7100198136137645\n",
      "Epoch: 610 - Loss: 0.7110681327187703\n",
      "Epoch: 611 - Loss: 0.7121171493859579\n",
      "Epoch: 612 - Loss: 0.7131668613166324\n",
      "Epoch: 613 - Loss: 0.7142172662192322\n",
      "Epoch: 614 - Loss: 0.7152683618093222\n",
      "Epoch: 615 - Loss: 0.716320145809587\n",
      "Epoch: 616 - Loss: 0.7173726159498239\n",
      "Epoch: 617 - Loss: 0.7184257699669349\n",
      "Epoch: 618 - Loss: 0.7194796056049201\n",
      "Epoch: 619 - Loss: 0.7205341206148695\n",
      "Epoch: 620 - Loss: 0.7215893127549555\n",
      "Epoch: 621 - Loss: 0.7226451797904242\n",
      "Epoch: 622 - Loss: 0.7237017194935871\n",
      "Epoch: 623 - Loss: 0.7247589296438138\n",
      "Epoch: 624 - Loss: 0.7258168080275212\n",
      "Epoch: 625 - Loss: 0.7268753524381675\n",
      "Epoch: 626 - Loss: 0.7279345606762397\n",
      "Epoch: 627 - Loss: 0.728994430549248\n",
      "Epoch: 628 - Loss: 0.7300549598717132\n",
      "Epoch: 629 - Loss: 0.7311161464651595\n",
      "Epoch: 630 - Loss: 0.7321779881581038\n",
      "Epoch: 631 - Loss: 0.7332404827860453\n",
      "Epoch: 632 - Loss: 0.7343036281914563\n",
      "Epoch: 633 - Loss: 0.7353674222237723\n",
      "Epoch: 634 - Loss: 0.7364318627393798\n",
      "Epoch: 635 - Loss: 0.7374969476016079\n",
      "Epoch: 636 - Loss: 0.7385626746807166\n",
      "Epoch: 637 - Loss: 0.7396290418538852\n",
      "Epoch: 638 - Loss: 0.7406960470052035\n",
      "Epoch: 639 - Loss: 0.7417636880256583\n",
      "Epoch: 640 - Loss: 0.7428319628131232\n",
      "Epoch: 641 - Loss: 0.7439008692723478\n",
      "Epoch: 642 - Loss: 0.7449704053149446\n",
      "Epoch: 643 - Loss: 0.7460405688593787\n",
      "Epoch: 644 - Loss: 0.7471113578309554\n",
      "Epoch: 645 - Loss: 0.7481827701618083\n",
      "Epoch: 646 - Loss: 0.7492548037908872\n",
      "Epoch: 647 - Loss: 0.7503274566639454\n",
      "Epoch: 648 - Loss: 0.7514007267335288\n",
      "Epoch: 649 - Loss: 0.7524746119589616\n",
      "Epoch: 650 - Loss: 0.7535491103063353\n",
      "Epoch: 651 - Loss: 0.7546242197484948\n",
      "Epoch: 652 - Loss: 0.7556999382650263\n",
      "Epoch: 653 - Loss: 0.7567762638422445\n",
      "Epoch: 654 - Loss: 0.7578531944731788\n",
      "Epoch: 655 - Loss: 0.758930728157561\n",
      "Epoch: 656 - Loss: 0.7600088629018112\n",
      "Epoch: 657 - Loss: 0.7610875967190256\n",
      "Epoch: 658 - Loss: 0.7621669276289618\n",
      "Epoch: 659 - Loss: 0.7632468536580259\n",
      "Epoch: 660 - Loss: 0.7643273728392589\n",
      "Epoch: 661 - Loss: 0.7654084832123228\n",
      "Epoch: 662 - Loss: 0.7664901828234866\n",
      "Epoch: 663 - Loss: 0.7675724697256129\n",
      "Epoch: 664 - Loss: 0.7686553419781427\n",
      "Epoch: 665 - Loss: 0.769738797647083\n",
      "Epoch: 666 - Loss: 0.770822834804991\n",
      "Epoch: 667 - Loss: 0.7719074515309609\n",
      "Epoch: 668 - Loss: 0.7729926459106091\n",
      "Epoch: 669 - Loss: 0.7740784160360594\n",
      "Epoch: 670 - Loss: 0.7751647600059289\n",
      "Epoch: 671 - Loss: 0.7762516759253141\n",
      "Epoch: 672 - Loss: 0.7773391619057741\n",
      "Epoch: 673 - Loss: 0.7784272160653183\n",
      "Epoch: 674 - Loss: 0.77951583652839\n",
      "Epoch: 675 - Loss: 0.7806050214258519\n",
      "Epoch: 676 - Loss: 0.7816947688949714\n",
      "Epoch: 677 - Loss: 0.7827850770794049\n",
      "Epoch: 678 - Loss: 0.7838759441291842\n",
      "Epoch: 679 - Loss: 0.7849673682006986\n",
      "Epoch: 680 - Loss: 0.7860593474566833\n",
      "Epoch: 681 - Loss: 0.7871518800662002\n",
      "Epoch: 682 - Loss: 0.7882449642046258\n",
      "Epoch: 683 - Loss: 0.7893385980536343\n",
      "Epoch: 684 - Loss: 0.7904327798011822\n",
      "Epoch: 685 - Loss: 0.7915275076414926\n",
      "Epoch: 686 - Loss: 0.7926227797750406\n",
      "Epoch: 687 - Loss: 0.7937185944085363\n",
      "Epoch: 688 - Loss: 0.7948149497549104\n",
      "Epoch: 689 - Loss: 0.7959118440332978\n",
      "Epoch: 690 - Loss: 0.7970092754690221\n",
      "Epoch: 691 - Loss: 0.798107242293579\n",
      "Epoch: 692 - Loss: 0.7992057427446222\n",
      "Epoch: 693 - Loss: 0.8003047750659453\n",
      "Epoch: 694 - Loss: 0.8014043375074676\n",
      "Epoch: 695 - Loss: 0.8025044283252174\n",
      "Epoch: 696 - Loss: 0.8036050457813155\n",
      "Epoch: 697 - Loss: 0.8047061881439607\n",
      "Epoch: 698 - Loss: 0.8058078536874117\n",
      "Epoch: 699 - Loss: 0.8069100406919725\n",
      "Epoch: 700 - Loss: 0.8080127474439756\n",
      "Epoch: 701 - Loss: 0.8091159722357656\n",
      "Epoch: 702 - Loss: 0.8102197133656838\n",
      "Epoch: 703 - Loss: 0.8113239691380509\n",
      "Epoch: 704 - Loss: 0.8124287378631516\n",
      "Epoch: 705 - Loss: 0.8135340178572175\n",
      "Epoch: 706 - Loss: 0.8146398074424112\n",
      "Epoch: 707 - Loss: 0.8157461049468102\n",
      "Epoch: 708 - Loss: 0.8168529087043902\n",
      "Epoch: 709 - Loss: 0.8179602170550079\n",
      "Epoch: 710 - Loss: 0.8190680283443866\n",
      "Epoch: 711 - Loss: 0.8201763409240973\n",
      "Epoch: 712 - Loss: 0.8212851531515437\n",
      "Epoch: 713 - Loss: 0.8223944633899459\n",
      "Epoch: 714 - Loss: 0.8235042700083226\n",
      "Epoch: 715 - Loss: 0.824614571381476\n",
      "Epoch: 716 - Loss: 0.825725365889974\n",
      "Epoch: 717 - Loss: 0.8268366519201349\n",
      "Epoch: 718 - Loss: 0.8279484278640094\n",
      "Epoch: 719 - Loss: 0.8290606921193647\n",
      "Epoch: 720 - Loss: 0.8301734430896687\n",
      "Epoch: 721 - Loss: 0.8312866791840723\n",
      "Epoch: 722 - Loss: 0.8324003988173926\n",
      "Epoch: 723 - Loss: 0.8335146004100976\n",
      "Epoch: 724 - Loss: 0.8346292823882876\n",
      "Epoch: 725 - Loss: 0.8357444431836809\n",
      "Epoch: 726 - Loss: 0.8368600812335948\n",
      "Epoch: 727 - Loss: 0.8379761949809309\n",
      "Epoch: 728 - Loss: 0.8390927828741571\n",
      "Epoch: 729 - Loss: 0.8402098433672912\n",
      "Epoch: 730 - Loss: 0.8413273749198847\n",
      "Epoch: 731 - Loss: 0.8424453759970065\n",
      "Epoch: 732 - Loss: 0.8435638450692241\n",
      "Epoch: 733 - Loss: 0.8446827806125898\n",
      "Epoch: 734 - Loss: 0.8458021811086217\n",
      "Epoch: 735 - Loss: 0.8469220450442881\n",
      "Epoch: 736 - Loss: 0.8480423709119909\n",
      "Epoch: 737 - Loss: 0.8491631572095486\n",
      "Epoch: 738 - Loss: 0.8502844024401793\n",
      "Epoch: 739 - Loss: 0.8514061051124854\n",
      "Epoch: 740 - Loss: 0.852528263740435\n",
      "Epoch: 741 - Loss: 0.8536508768433465\n",
      "Epoch: 742 - Loss: 0.8547739429458715\n",
      "Epoch: 743 - Loss: 0.855897460577979\n",
      "Epoch: 744 - Loss: 0.8570214282749374\n",
      "Epoch: 745 - Loss: 0.858145844577299\n",
      "Epoch: 746 - Loss: 0.8592707080308825\n",
      "Epoch: 747 - Loss: 0.860396017186757\n",
      "Epoch: 748 - Loss: 0.861521770601225\n",
      "Epoch: 749 - Loss: 0.862647966835807\n",
      "Epoch: 750 - Loss: 0.8637746044572227\n",
      "Epoch: 751 - Loss: 0.8649016820373762\n",
      "Epoch: 752 - Loss: 0.8660291981533391\n",
      "Epoch: 753 - Loss: 0.8671571513873337\n",
      "Epoch: 754 - Loss: 0.8682855403267165\n",
      "Epoch: 755 - Loss: 0.8694143635639614\n",
      "Epoch: 756 - Loss: 0.8705436196966445\n",
      "Epoch: 757 - Loss: 0.8716733073274255\n",
      "Epoch: 758 - Loss: 0.8728034250640333\n",
      "Epoch: 759 - Loss: 0.873933971519248\n",
      "Epoch: 760 - Loss: 0.8750649453108856\n",
      "Epoch: 761 - Loss: 0.8761963450617812\n",
      "Epoch: 762 - Loss: 0.8773281693997717\n",
      "Epoch: 763 - Loss: 0.8784604169576811\n",
      "Epoch: 764 - Loss: 0.8795930863733027\n",
      "Epoch: 765 - Loss: 0.8807261762893838\n",
      "Epoch: 766 - Loss: 0.8818596853536083\n",
      "Epoch: 767 - Loss: 0.8829936122185812\n",
      "Epoch: 768 - Loss: 0.8841279555418123\n",
      "Epoch: 769 - Loss: 0.8852627139857002\n",
      "Epoch: 770 - Loss: 0.8863978862175147\n",
      "Epoch: 771 - Loss: 0.8875334709093818\n",
      "Epoch: 772 - Loss: 0.8886694667382673\n",
      "Epoch: 773 - Loss: 0.8898058723859607\n",
      "Epoch: 774 - Loss: 0.8909426865390586\n",
      "Epoch: 775 - Loss: 0.8920799078889493\n",
      "Epoch: 776 - Loss: 0.893217535131796\n",
      "Epoch: 777 - Loss: 0.8943555669685207\n",
      "Epoch: 778 - Loss: 0.8954940021047892\n",
      "Epoch: 779 - Loss: 0.8966328392509935\n",
      "Epoch: 780 - Loss: 0.8977720771222375\n",
      "Epoch: 781 - Loss: 0.8989117144383196\n",
      "Epoch: 782 - Loss: 0.9000517499237173\n",
      "Epoch: 783 - Loss: 0.9011921823075715\n",
      "Epoch: 784 - Loss: 0.9023330103236705\n",
      "Epoch: 785 - Loss: 0.9034742327104343\n",
      "Epoch: 786 - Loss: 0.9046158482108978\n",
      "Epoch: 787 - Loss: 0.9057578555726961\n",
      "Epoch: 788 - Loss: 0.9069002535480486\n",
      "Epoch: 789 - Loss: 0.9080430408937428\n",
      "Epoch: 790 - Loss: 0.9091862163711188\n",
      "Epoch: 791 - Loss: 0.9103297787460535\n",
      "Epoch: 792 - Loss: 0.9114737267889452\n",
      "Epoch: 793 - Loss: 0.9126180592746977\n",
      "Epoch: 794 - Loss: 0.913762774982705\n",
      "Epoch: 795 - Loss: 0.9149078726968349\n",
      "Epoch: 796 - Loss: 0.9160533512054152\n",
      "Epoch: 797 - Loss: 0.9171992093012161\n",
      "Epoch: 798 - Loss: 0.9183454457814364\n",
      "Epoch: 799 - Loss: 0.9194920594476869\n",
      "Epoch: 800 - Loss: 0.9206390491059759\n",
      "Epoch: 801 - Loss: 0.9217864135666927\n",
      "Epoch: 802 - Loss: 0.9229341516445941\n",
      "Epoch: 803 - Loss: 0.924082262158787\n",
      "Epoch: 804 - Loss: 0.9252307439327143\n",
      "Epoch: 805 - Loss: 0.9263795957941395\n",
      "Epoch: 806 - Loss: 0.9275288165751314\n",
      "Epoch: 807 - Loss: 0.9286784051120489\n",
      "Epoch: 808 - Loss: 0.9298283602455257\n",
      "Epoch: 809 - Loss: 0.930978680820456\n",
      "Epoch: 810 - Loss: 0.9321293656859778\n",
      "Epoch: 811 - Loss: 0.9332804136954597\n",
      "Epoch: 812 - Loss: 0.9344318237064846\n",
      "Epoch: 813 - Loss: 0.9355835945808351\n",
      "Epoch: 814 - Loss: 0.9367357251844789\n",
      "Epoch: 815 - Loss: 0.9378882143875537\n",
      "Epoch: 816 - Loss: 0.9390410610643519\n",
      "Epoch: 817 - Loss: 0.9401942640933065\n",
      "Epoch: 818 - Loss: 0.9413478223569752\n",
      "Epoch: 819 - Loss: 0.9425017347420276\n",
      "Epoch: 820 - Loss: 0.943656000139228\n",
      "Epoch: 821 - Loss: 0.9448106174434227\n",
      "Epoch: 822 - Loss: 0.9459655855535241\n",
      "Epoch: 823 - Loss: 0.9471209033724973\n",
      "Epoch: 824 - Loss: 0.9482765698073439\n",
      "Epoch: 825 - Loss: 0.9494325837690891\n",
      "Epoch: 826 - Loss: 0.9505889441727662\n",
      "Epoch: 827 - Loss: 0.9517456499374021\n",
      "Epoch: 828 - Loss: 0.9529026999860034\n",
      "Epoch: 829 - Loss: 0.9540600932455421\n",
      "Epoch: 830 - Loss: 0.9552178286469403\n",
      "Epoch: 831 - Loss: 0.9563759051250567\n",
      "Epoch: 832 - Loss: 0.9575343216186722\n",
      "Epoch: 833 - Loss: 0.9586930770704758\n",
      "Epoch: 834 - Loss: 0.9598521704270493\n",
      "Epoch: 835 - Loss: 0.9610116006388548\n",
      "Epoch: 836 - Loss: 0.9621713666602197\n",
      "Epoch: 837 - Loss: 0.9633314674493219\n",
      "Epoch: 838 - Loss: 0.9644919019681771\n",
      "Epoch: 839 - Loss: 0.9656526691826245\n",
      "Epoch: 840 - Loss: 0.9668137680623119\n",
      "Epoch: 841 - Loss: 0.9679751975806822\n",
      "Epoch: 842 - Loss: 0.9691369567149607\n",
      "Epoch: 843 - Loss: 0.9702990444461391\n",
      "Epoch: 844 - Loss: 0.9714614597589628\n",
      "Epoch: 845 - Loss: 0.9726242016419184\n",
      "Epoch: 846 - Loss: 0.9737872690872174\n",
      "Epoch: 847 - Loss: 0.9749506610907838\n",
      "Epoch: 848 - Loss: 0.9761143766522405\n",
      "Epoch: 849 - Loss: 0.9772784147748965\n",
      "Epoch: 850 - Loss: 0.9784427744657304\n",
      "Epoch: 851 - Loss: 0.9796074547353808\n",
      "Epoch: 852 - Loss: 0.9807724545981292\n",
      "Epoch: 853 - Loss: 0.9819377730718896\n",
      "Epoch: 854 - Loss: 0.983103409178192\n",
      "Epoch: 855 - Loss: 0.9842693619421714\n",
      "Epoch: 856 - Loss: 0.985435630392554\n",
      "Epoch: 857 - Loss: 0.9866022135616428\n",
      "Epoch: 858 - Loss: 0.987769110485305\n",
      "Epoch: 859 - Loss: 0.9889363202029597\n",
      "Epoch: 860 - Loss: 0.9901038417575625\n",
      "Epoch: 861 - Loss: 0.9912716741955947\n",
      "Epoch: 862 - Loss: 0.9924398165670484\n",
      "Epoch: 863 - Loss: 0.9936082679254143\n",
      "Epoch: 864 - Loss: 0.9947770273276689\n",
      "Epoch: 865 - Loss: 0.9959460938342609\n",
      "Epoch: 866 - Loss: 0.9971154665090977\n",
      "Epoch: 867 - Loss: 0.9982851444195344\n",
      "Epoch: 868 - Loss: 0.9994551266363586\n",
      "Epoch: 869 - Loss: 1.0006254122337794\n",
      "Epoch: 870 - Loss: 1.0017960002894137\n",
      "Epoch: 871 - Loss: 1.0029668898842732\n",
      "Epoch: 872 - Loss: 1.004138080102753\n",
      "Epoch: 873 - Loss: 1.005309570032617\n",
      "Epoch: 874 - Loss: 1.0064813587649866\n",
      "Epoch: 875 - Loss: 1.007653445394328\n",
      "Epoch: 876 - Loss: 1.0088258290184386\n",
      "Epoch: 877 - Loss: 1.0099985087384369\n",
      "Epoch: 878 - Loss: 1.0111714836587462\n",
      "Epoch: 879 - Loss: 1.012344752887086\n",
      "Epoch: 880 - Loss: 1.013518315534457\n",
      "Epoch: 881 - Loss: 1.0146921707151295\n",
      "Epoch: 882 - Loss: 1.0158663175466325\n",
      "Epoch: 883 - Loss: 1.0170407551497387\n",
      "Epoch: 884 - Loss: 1.018215482648454\n",
      "Epoch: 885 - Loss: 1.0193904991700056\n",
      "Epoch: 886 - Loss: 1.020565803844828\n",
      "Epoch: 887 - Loss: 1.0217413958065533\n",
      "Epoch: 888 - Loss: 1.022917274191997\n",
      "Epoch: 889 - Loss: 1.0240934381411475\n",
      "Epoch: 890 - Loss: 1.0252698867971521\n",
      "Epoch: 891 - Loss: 1.0264466193063078\n",
      "Epoch: 892 - Loss: 1.027623634818047\n",
      "Epoch: 893 - Loss: 1.0288009324849268\n",
      "Epoch: 894 - Loss: 1.0299785114626165\n",
      "Epoch: 895 - Loss: 1.0311563709098865\n",
      "Epoch: 896 - Loss: 1.0323345099885957\n",
      "Epoch: 897 - Loss: 1.0335129278636805\n",
      "Epoch: 898 - Loss: 1.0346916237031427\n",
      "Epoch: 899 - Loss: 1.0358705966780375\n",
      "Epoch: 900 - Loss: 1.0370498459624629\n",
      "Epoch: 901 - Loss: 1.038229370733547\n",
      "Epoch: 902 - Loss: 1.0394091701714374\n",
      "Epoch: 903 - Loss: 1.0405892434592887\n",
      "Epoch: 904 - Loss: 1.041769589783252\n",
      "Epoch: 905 - Loss: 1.042950208332463\n",
      "Epoch: 906 - Loss: 1.0441310982990308\n",
      "Epoch: 907 - Loss: 1.0453122588780253\n",
      "Epoch: 908 - Loss: 1.0464936892674692\n",
      "Epoch: 909 - Loss: 1.0476753886683217\n",
      "Epoch: 910 - Loss: 1.048857356284472\n",
      "Epoch: 911 - Loss: 1.050039591322726\n",
      "Epoch: 912 - Loss: 1.051222092992795\n",
      "Epoch: 913 - Loss: 1.0524048605072842\n",
      "Epoch: 914 - Loss: 1.0535878930816842\n",
      "Epoch: 915 - Loss: 1.054771189934355\n",
      "Epoch: 916 - Loss: 1.0559547502865216\n",
      "Epoch: 917 - Loss: 1.057138573362257\n",
      "Epoch: 918 - Loss: 1.058322658388474\n",
      "Epoch: 919 - Loss: 1.0595070045949153\n",
      "Epoch: 920 - Loss: 1.0606916112141405\n",
      "Epoch: 921 - Loss: 1.0618764774815157\n",
      "Epoch: 922 - Loss: 1.0630616026352042\n",
      "Epoch: 923 - Loss: 1.064246985916154\n",
      "Epoch: 924 - Loss: 1.065432626568089\n",
      "Epoch: 925 - Loss: 1.0666185238374954\n",
      "Epoch: 926 - Loss: 1.0678046769736138\n",
      "Epoch: 927 - Loss: 1.068991085228428\n",
      "Epoch: 928 - Loss: 1.0701777478566534\n",
      "Epoch: 929 - Loss: 1.0713646641157277\n",
      "Epoch: 930 - Loss: 1.0725518332657997\n",
      "Epoch: 931 - Loss: 1.0737392545697189\n",
      "Epoch: 932 - Loss: 1.0749269272930246\n",
      "Epoch: 933 - Loss: 1.0761148507039375\n",
      "Epoch: 934 - Loss: 1.0773030240733477\n",
      "Epoch: 935 - Loss: 1.0784914466748037\n",
      "Epoch: 936 - Loss: 1.0796801177845041\n",
      "Epoch: 937 - Loss: 1.080869036681287\n",
      "Epoch: 938 - Loss: 1.082058202646617\n",
      "Epoch: 939 - Loss: 1.0832476149645798\n",
      "Epoch: 940 - Loss: 1.0844372729218685\n",
      "Epoch: 941 - Loss: 1.0856271758077747\n",
      "Epoch: 942 - Loss: 1.0868173229141778\n",
      "Epoch: 943 - Loss: 1.0880077135355362\n",
      "Epoch: 944 - Loss: 1.0891983469688766\n",
      "Epoch: 945 - Loss: 1.0903892225137852\n",
      "Epoch: 946 - Loss: 1.091580339472394\n",
      "Epoch: 947 - Loss: 1.0927716971493764\n",
      "Epoch: 948 - Loss: 1.093963294851933\n",
      "Epoch: 949 - Loss: 1.095155131889785\n",
      "Epoch: 950 - Loss: 1.0963472075751608\n",
      "Epoch: 951 - Loss: 1.0975395212227912\n",
      "Epoch: 952 - Loss: 1.0987320721498937\n",
      "Epoch: 953 - Loss: 1.0999248596761688\n",
      "Epoch: 954 - Loss: 1.1011178831237858\n",
      "Epoch: 955 - Loss: 1.1023111418173763\n",
      "Epoch: 956 - Loss: 1.1035046350840227\n",
      "Epoch: 957 - Loss: 1.1046983622532496\n",
      "Epoch: 958 - Loss: 1.1058923226570143\n",
      "Epoch: 959 - Loss: 1.107086515629697\n",
      "Epoch: 960 - Loss: 1.1082809405080918\n",
      "Epoch: 961 - Loss: 1.1094755966313967\n",
      "Epoch: 962 - Loss: 1.1106704833412055\n",
      "Epoch: 963 - Loss: 1.1118655999814961\n",
      "Epoch: 964 - Loss: 1.113060945898625\n",
      "Epoch: 965 - Loss: 1.1142565204413144\n",
      "Epoch: 966 - Loss: 1.1154523229606448\n",
      "Epoch: 967 - Loss: 1.1166483528100455\n",
      "Epoch: 968 - Loss: 1.1178446093452852\n",
      "Epoch: 969 - Loss: 1.1190410919244647\n",
      "Epoch: 970 - Loss: 1.120237799908004\n",
      "Epoch: 971 - Loss: 1.1214347326586378\n",
      "Epoch: 972 - Loss: 1.1226318895414027\n",
      "Epoch: 973 - Loss: 1.12382926992363\n",
      "Epoch: 974 - Loss: 1.1250268731749384\n",
      "Epoch: 975 - Loss: 1.1262246986672209\n",
      "Epoch: 976 - Loss: 1.12742274577464\n",
      "Epoch: 977 - Loss: 1.1286210138736166\n",
      "Epoch: 978 - Loss: 1.1298195023428215\n",
      "Epoch: 979 - Loss: 1.131018210563168\n",
      "Epoch: 980 - Loss: 1.1322171379178017\n",
      "Epoch: 981 - Loss: 1.133416283792091\n",
      "Epoch: 982 - Loss: 1.1346156475736222\n",
      "Epoch: 983 - Loss: 1.135815228652186\n",
      "Epoch: 984 - Loss: 1.1370150264197727\n",
      "Epoch: 985 - Loss: 1.1382150402705609\n",
      "Epoch: 986 - Loss: 1.1394152696009126\n",
      "Epoch: 987 - Loss: 1.1406157138093589\n",
      "Epoch: 988 - Loss: 1.141816372296598\n",
      "Epoch: 989 - Loss: 1.1430172444654825\n",
      "Epoch: 990 - Loss: 1.1442183297210122\n",
      "Epoch: 991 - Loss: 1.1454196274703263\n",
      "Epoch: 992 - Loss: 1.1466211371226933\n",
      "Epoch: 993 - Loss: 1.1478228580895058\n",
      "Epoch: 994 - Loss: 1.149024789784268\n",
      "Epoch: 995 - Loss: 1.1502269316225928\n",
      "Epoch: 996 - Loss: 1.151429283022187\n",
      "Epoch: 997 - Loss: 1.1526318434028502\n",
      "Epoch: 998 - Loss: 1.15383461218646\n",
      "Epoch: 999 - Loss: 1.1550375887969697\n",
      "Epoch: 1000 - Loss: 1.1562407726603954\n"
     ]
    }
   ],
   "source": [
    "X, Y, prob  = model_LR.train(train_file, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1408,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model_LR.softmax(np.dot(X, model_LR.weights) + model_LR.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR_loaded = model_LR.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_weights': {'weights': array([[ 3.25415550e-03,  3.40083798e-03, -2.39145014e-04,\n",
       "           3.82157057e-03,  2.24468933e-03,  2.05746647e-03],\n",
       "         [ 3.89265284e-03,  3.91929004e-03, -6.27197695e-05,\n",
       "           2.66371856e-03,  1.98998596e-03,  1.48411806e-03],\n",
       "         [ 5.09642459e-03,  3.73958991e-03, -1.54583134e-04,\n",
       "           3.55029246e-03,  2.59382679e-03,  2.25011124e-03],\n",
       "         ...,\n",
       "         [ 1.09699452e-02,  9.36323969e-03, -4.38945047e-04,\n",
       "           9.87727667e-03,  5.92061110e-03,  6.04844393e-03],\n",
       "         [ 1.09704307e-02,  9.33492488e-03, -4.38859468e-04,\n",
       "           9.89194017e-03,  5.92081974e-03,  6.04865668e-03],\n",
       "         [ 1.09696609e-02,  9.33434809e-03, -4.38993752e-04,\n",
       "           9.86221733e-03,  5.93609747e-03,  6.08473742e-03]]),\n",
       "  'bias': array([ 0.0071371 ,  0.00863322, -0.00065039,  0.00850179,  0.00541161,\n",
       "          0.00575694]),\n",
       "  'Y_to_categorical': {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5'}},\n",
       " 'Feature': <__main__.Features_LR at 0x2d04890c0>}"
      ]
     },
     "execution_count": 1410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, prob = model_LR.classify(pred_file + \".txt\", model_LR_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.18802537e-04, 3.65680354e-04, 1.34041123e-06, 4.83723813e-04,\n",
       "        5.16377425e-05, 5.56870046e-05],\n",
       "       [2.36775021e-03, 1.09845466e-03, 1.01805903e-05, 1.38708610e-03,\n",
       "        2.14370899e-04, 2.28075746e-04],\n",
       "       [2.37192807e-03, 1.10075887e-03, 1.02260689e-05, 1.39341013e-03,\n",
       "        2.15000688e-04, 2.28815065e-04],\n",
       "       ...,\n",
       "       [7.58660477e-03, 4.26695414e-03, 1.27146955e-04, 5.08164719e-03,\n",
       "        1.25209013e-03, 1.31134259e-03],\n",
       "       [2.32929378e-03, 1.07673851e-03, 9.82390274e-06, 1.36080264e-03,\n",
       "        2.08984722e-04, 2.22684235e-04],\n",
       "       [2.33349421e-03, 1.07851072e-03, 9.85910030e-06, 1.36272409e-03,\n",
       "        2.09496960e-04, 2.23193498e-04]])"
      ]
     },
     "execution_count": 1419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0'}"
      ]
     },
     "execution_count": 1412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(train_file, sep='\\t', header=None, names=['text', 'true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true_label\n",
       "1    755\n",
       "3    755\n",
       "0    688\n",
       "5    522\n",
       "4    498\n",
       "2     53\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['true_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.07%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
