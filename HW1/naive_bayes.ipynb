{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NaiveBayes is a generative classifier based on the Naive assumption that features are independent from each other\n",
    "P(w1, w2, ..., wn|y) = P(w1|y) P(w2|y) ... P(wn|y)\n",
    "Thus argmax_{y} (P(y|w1,w2, ... wn)) can be modeled as argmax_{y} P(w1|y) P(w2|y) ... P(wn|y) P(y) using Bayes Rule\n",
    "and P(w1, w2, ... ,wn) is constant with respect to argmax_{y} \n",
    "Please refer to lecture notes Chapter 4 for more details\n",
    "\"\"\"\n",
    "from work.Features import Features_NB\n",
    "from work.Model import *\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "class NaiveBayes(Model):\n",
    "\n",
    "    def __init__(self, model_file, threshold=0):\n",
    "        super(NaiveBayes, self).__init__(model_file)\n",
    "        # self.count_word_label = None\n",
    "        # self.count_words_per_label = None\n",
    "        # self.count_label = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __count_frequency_word_label(self, sentences, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param sentences (list[list]): sentences tokenized\n",
    "        :param labels (list): list of labels\n",
    "        :return: count(c_j, w_i) refers to the count of word w_i in documents with label c_j\n",
    "                _sum_{i=1}^{V}{count(c_j, w_i)} sum of the counts of each word in our vocabulary in class c_j \n",
    "                 count(c_j) refers to the count of label c_j \n",
    "        \"\"\"\n",
    "        count_word_label = []\n",
    "        # count_words_per_label = defaultdict(int)\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            for token in sentence:\n",
    "                count_word_label.append((token, label))\n",
    "                # count_words_per_label[label] += 1\n",
    "            \n",
    "        # count_word_label = [(token, label) for sentence, label in zip(sentences, labels) for token in sentence]\n",
    "        count_label = Counter(labels)\n",
    "        count_word_label = Counter(count_word_label)\n",
    "\n",
    "        count_words_per_label = defaultdict(int)\n",
    "        for (word, label), count in count_word_label.items():\n",
    "            count_words_per_label[label] += count + 1 # Add laplace smoothing\n",
    "\n",
    "        return count_word_label, count_words_per_label, count_label\n",
    "    \n",
    "    def __compute_feature_weights(self, count_word_label, count_words_per_label, count_label, size_vocabulary, alpha=0):\n",
    "        \"\"\"\n",
    "        :param alpha (int): Hyperparemeter alpha for Laplace Smoothing\n",
    "        \"\"\"\n",
    "        feature_weights = defaultdict(dict)\n",
    "        for word, label in count_word_label.keys():\n",
    "            # Maximum Likelihood Estimates\n",
    "            # tmp = math.log((count_word_label[(word, label)] + alpha)/(size_vocabulary*alpha + count_words_per_label[label]))\n",
    "            tmp = math.log((count_word_label[(word, label)])/(count_words_per_label[label]))\n",
    "            feature_weights[label][word] = tmp\n",
    "\n",
    "        # Include Probability of each label: \n",
    "        total_documents = sum(count_label.values())\n",
    "        for label in count_label.keys():\n",
    "            probability_label_name = \"prob_mu\"\n",
    "            feature_weights[label][probability_label_name] = math.log(count_label[label]/total_documents)\n",
    "            probability_label_name = \"laplace_smoothing\" # Weights for token with count(y,j) = 0 \n",
    "            feature_weights[label][probability_label_name] = math.log(1/(count_words_per_label[label]))\n",
    "        return feature_weights\n",
    "\n",
    "    \n",
    "    def train(self, input_file):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "\n",
    "        # Instanciate Features_NB class:\n",
    "        #   - Create Vocabulary\n",
    "        features_naive_bayes = Features_NB(input_file, self.threshold)\n",
    "        print(len(features_naive_bayes.vocabulary))\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        labels = features_naive_bayes.labels\n",
    "        for sentence in features_naive_bayes.tokenized_text:\n",
    "            tmp = features_naive_bayes.replace_unknown_word_with_oov(sentence)\n",
    "            updated_text.append(tmp)            \n",
    "\n",
    "        # Compute Feature Weights\n",
    "        count_word_label, count_words_per_label, count_label = self.__count_frequency_word_label(updated_text, labels)\n",
    "        self.count_word_label = count_word_label\n",
    "        self.count_words_per_label = count_words_per_label\n",
    "        self.count_label = count_label\n",
    "        feature_weights = self.__compute_feature_weights(count_word_label, count_words_per_label, count_label, len(features_naive_bayes.vocabulary))\n",
    "\n",
    "        # Build Model\n",
    "        nb_model = {\n",
    "            \"feature_weights\": feature_weights,\n",
    "            \"Feature\": features_naive_bayes\n",
    "        }\n",
    "        \n",
    "        self.save_model(nb_model)\n",
    "    \n",
    "        \n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_file: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\" \n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_NB_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_NB_class.read_inference_file(input_file)\n",
    "        # # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # # token\n",
    "        # updated_text = []\n",
    "        # for sentence in Feature_NB_class.tokenized_text:\n",
    "        #     tmp = Feature_NB_class.replace_unknown_word_with_oov(sentence)\n",
    "        #     updated_text.append(tmp)   \n",
    "        # tokenized_text = updated_text\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # Choosing the label y which maximizes log p(x, y; μ, φ):\n",
    "        class_predictions_list = []\n",
    "        for sentence in tokenized_text:\n",
    "            sentence_features = Feature_NB_class.get_features(sentence, model)\n",
    "            # print(\"Sentence Characters: \", len(sentence_features.keys()))\n",
    "            class_predictions = defaultdict()\n",
    "\n",
    "            for label in Feature_NB_class.labelset:\n",
    "                # print(f\"##########   {label} ############\")\n",
    "                # print(label)\n",
    "                feature_weights_y = feature_weights[label]\n",
    "                # Compute Inner Product: feature_weights*feature_vector\n",
    "                # print(len(sentence_features))\n",
    "                total_sum = 0 \n",
    "                # print(feature_weights_y)\n",
    "                # print(\"Size match: @@@@@ \", len(sentence_features.keys() & feature_weights_y.keys()))\n",
    "                for key in sentence_features.keys():\n",
    "                    if key in feature_weights_y.keys():\n",
    "                        # print(key)\n",
    "                        # if key == \"prob_mu\":\n",
    "                        #     print(\"@@@@@@@ prob_mu @@\")\n",
    "                        # print(\"KEY: \", key)\n",
    "                        # print(\"sentence_features[key]: \", sentence_features[key])\n",
    "                        # print(\"feature_weights_y[key]: \", feature_weights_y[key])\n",
    "                        tmp = sentence_features[key] * feature_weights_y[key]\n",
    "                    else:\n",
    "                        # print(\"No MATCH: \", self.count_words_per_label[label])\n",
    "                        # print(sentence_features[key] * math.log(1/(self.count_words_per_label[label])))\n",
    "                        tmp = sentence_features[key] * feature_weights_y['laplace_smoothing']\n",
    "                    total_sum += tmp\n",
    "                # result = sum(sentence_features[key] * feature_weights_y[key] for key in sentence_features.keys() & feature_weights_y.keys())\n",
    "                # print(result)\n",
    "\n",
    "                class_predictions[label] = total_sum\n",
    "            # Find the class with the highest value\n",
    "            class_predictions_list.append(class_predictions)\n",
    "            class_with_highest_value = max(class_predictions, key=lambda k: class_predictions[k])\n",
    "            preds.append(class_with_highest_value)\n",
    "            # break\n",
    "\n",
    "        \n",
    "        return preds, class_predictions_list, sentence_features, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string \n",
    "import re\n",
    "\n",
    "# def tokenize(text):\n",
    "#     # TODO customize to your needs\n",
    "#     text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "#     # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "#     return re.sub('[^a-zA-Z]', ' ', text.lower()).split()\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    # 5) Remove Stop Words\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Naive Bayes Features #\n",
    "########################\n",
    "\n",
    "class Features_NB(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold):\n",
    "        super(Features_NB, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text, threshold)\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = [word for word, count in flattened_list_count.items() if count > threshold]\n",
    "\n",
    "        return flattened_list_count_filter\n",
    "\n",
    "    def replace_unknown_word_with_oov(self, tokenized_sentence):\n",
    "        \"\"\"Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        token\n",
    "        \"\"\"\n",
    "        updated_sentence = []\n",
    "        for word in tokenized_sentence:\n",
    "            if word not in self.vocabulary:\n",
    "                updated_sentence.append('OOV')\n",
    "            else:\n",
    "                updated_sentence.append(word)\n",
    "        return updated_sentence\n",
    "        \n",
    "    def get_features(self, tokenized, model):\n",
    "        \"\"\"Bag-of-words: return column vector of word counts, including OOV (Out-of-Vocabulary) token, if present.\n",
    "        Vector stores only non-zero values to improve performance\n",
    "        \"\"\"\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV\n",
    "        updated_text = model[\"Feature\"].replace_unknown_word_with_oov(tokenized)\n",
    "\n",
    "        bag_of_words = Counter(updated_text)\n",
    "        # Include OffsetFeature \"prob_mu\" to 1; which allows to include the probability of the label\n",
    "        # to the maximum likelihood estimation.\n",
    "\n",
    "        bag_of_words[\"prob_mu\"] = 1\n",
    "        return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #questions\n",
    "# train_file = \"work/datasets/questions/train.txt\"\n",
    "# pred_file = \"work/datasets/questions/val.test\"\n",
    "# pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "# model_file_name = \"nb.questions.model\"\n",
    "# model_nb = NaiveBayes(model_file_name, threshold=9)\n",
    "\n",
    "# odiya\n",
    "# train_file = \"work/datasets/odiya/train.txt\"\n",
    "# pred_file = \"work/datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "# model_file_name = \"nb.odiya.model\"\n",
    "# model_nb = NaiveBayes(model_file_name, threshold=2)\n",
    "\n",
    "#Products\n",
    "# train_file = \"work/datasets/products/train.txt\"\n",
    "# pred_file = \"work/datasets/products/val.test\"\n",
    "# pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "# model_file_name = \"nb.products.model\"\n",
    "# model_nb = NaiveBayes(model_file_name, threshold=2)\n",
    "# TIME Train =>  46.1s\n",
    "# INFERENCE => 10.1s\n",
    "\n",
    "# 4dim\n",
    "train_file = \"work/datasets/4dim/train.txt\"\n",
    "pred_file = \"work/datasets/4dim/val.test\"\n",
    "pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "model_file_name = \"nb.4dim.model\"\n",
    "model_nb = NaiveBayes(model_file_name, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8094\n"
     ]
    }
   ],
   "source": [
    "model = model_nb.train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = 0\n",
    "# for (word, label), count in model_nb.count_word_label.items():\n",
    "#     if label == 'pos.dec':\n",
    "#         total+=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_by_label = defaultdict(int)\n",
    "\n",
    "# for (word, label), count in model_nb.count_word_label.items():\n",
    "#     sum_by_label[label] += count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, label in model_nb.count_word_label.keys():\n",
    "#     if word == 'crime' and label == 'neg':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, class_predictions_list, sentence_features, sentence = model_nb.classify(pred_file + \".txt\", model_nb.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_dataset = pd.read_csv(\"work/datasets/questions/train.txt\", sep='\\t', header=None, names=['text', 'true_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01620299602568022"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53/true_dataset[\"true_label\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.88%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
