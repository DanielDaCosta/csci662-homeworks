{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NaiveBayes is a generative classifier based on the Naive assumption that features are independent from each other\n",
    "P(w1, w2, ..., wn|y) = P(w1|y) P(w2|y) ... P(wn|y)\n",
    "Thus argmax_{y} (P(y|w1,w2, ... wn)) can be modeled as argmax_{y} P(w1|y) P(w2|y) ... P(wn|y) P(y) using Bayes Rule\n",
    "and P(w1, w2, ... ,wn) is constant with respect to argmax_{y} \n",
    "Please refer to lecture notes Chapter 4 for more details\n",
    "\"\"\"\n",
    "from work.Features import Features_NB\n",
    "from work.Model import *\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "class NaiveBayes(Model):\n",
    "\n",
    "    def __init__(self, model_file, threshold):\n",
    "        super(NaiveBayes, self).__init__(model_file)\n",
    "        self.count_word_label = None\n",
    "        self.count_words_per_label = None\n",
    "        self.count_label = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __count_frequency_word_label(self, sentences, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param sentences (list[list]): sentences tokenized\n",
    "        :param labels (list): list of labels\n",
    "        :return: count(c_j, w_i) refers to the count of word w_i in documents with label c_j\n",
    "                _sum_{i=1}^{V}{count(c_j, w_i)} sum of the counts of each word in our vocabulary in class c_j \n",
    "                 count(c_j) refers to the count of label c_j \n",
    "        \"\"\"\n",
    "        count_word_label = []\n",
    "        count_words_per_label = defaultdict(int)\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            for token in sentence:\n",
    "                count_word_label.append((token, label))\n",
    "                count_words_per_label[label] += 1\n",
    "            \n",
    "        # count_word_label = [(token, label) for sentence, label in zip(sentences, labels) for token in sentence]\n",
    "        count_label = Counter(labels)\n",
    "        return Counter(count_word_label), count_words_per_label, count_label\n",
    "    \n",
    "    def __compute_feature_weights(self, count_word_label, count_words_per_label, count_label, size_vocabulary, alpha=1):\n",
    "        \"\"\"\n",
    "        :param alpha (int): Hyperparemeter alpha for Laplace Smoothing\n",
    "        \"\"\"\n",
    "        feature_weights = defaultdict(dict)\n",
    "        for word, label in count_word_label.keys():\n",
    "            # Maximum Likelihood Estimates\n",
    "            tmp = math.log((count_word_label[(word, label)] + alpha)/(size_vocabulary*alpha + count_words_per_label[label]))\n",
    "            feature_weights[label][word] = tmp\n",
    "\n",
    "        # Include Probability of each label: \n",
    "        total_documents = sum(count_label.values())\n",
    "        for label in count_label.keys():\n",
    "            probability_label_name = \"prob_mu\"\n",
    "            feature_weights[label][probability_label_name] = math.log(count_label[label]/total_documents)\n",
    "        return feature_weights\n",
    "\n",
    "    \n",
    "    def train(self, input_file):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "\n",
    "        # Instanciate Features_NB class:\n",
    "        #   - Create Vocabulary\n",
    "        features_naive_bayes = Features_NB(input_file, self.threshold)\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        labels = features_naive_bayes.labels\n",
    "        for sentence in features_naive_bayes.tokenized_text:\n",
    "            tmp = features_naive_bayes.replace_unknown_word_with_oov(sentence)\n",
    "            updated_text.append(tmp)            \n",
    "\n",
    "        # Compute Feature Weights\n",
    "        count_word_label, count_words_per_label, count_label = self.__count_frequency_word_label(updated_text, labels)\n",
    "        self.count_word_label = count_word_label\n",
    "        self.count_words_per_label = count_words_per_label\n",
    "        self.count_label = count_label\n",
    "        feature_weights = self.__compute_feature_weights(count_word_label, count_words_per_label, count_label, len(features_naive_bayes.vocabulary))\n",
    "\n",
    "        # Build Model\n",
    "        nb_model = {\n",
    "            \"feature_weights\": feature_weights,\n",
    "            \"Feature\": features_naive_bayes\n",
    "        }\n",
    "        \n",
    "        self.save_model(nb_model)\n",
    "    \n",
    "        \n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_file: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\" \n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_NB_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_NB_class.read_inference_file(input_file)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # Choosing the label y which maximizes log p(x, y; μ, φ):\n",
    "        for sentence in tokenized_text:\n",
    "            sentence_features = Feature_NB_class.get_features(sentence, model)\n",
    "            # print(\"Sentence Characters: \", len(sentence_features.keys()))\n",
    "            class_predictions = defaultdict()\n",
    "\n",
    "            for label in Feature_NB_class.labelset:\n",
    "                # print(label)\n",
    "                feature_weights_y = feature_weights[label]\n",
    "                # Compute Inner Product: feature_weights*feature_vector\n",
    "                # print(len(sentence_features))\n",
    "                total_sum = 0 \n",
    "                # print(\"Size match: @@@@@ \", len(sentence_features.keys() & feature_weights_y.keys()))\n",
    "                for key in sentence_features.keys():\n",
    "                    if key in feature_weights_y.keys():\n",
    "                        # print(key)\n",
    "                        total_sum += sentence_features[key] * feature_weights_y[key]\n",
    "                # result = sum(sentence_features[key] * feature_weights_y[key] for key in sentence_features.keys() & feature_weights_y.keys())\n",
    "                # print(result)\n",
    "\n",
    "                class_predictions[label] = total_sum\n",
    "            # Find the class with the highest value\n",
    "            class_with_highest_value = max(class_predictions, key=lambda k: class_predictions[k])\n",
    "            preds.append(class_with_highest_value)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string \n",
    "import re\n",
    "\n",
    "# def tokenize(text):\n",
    "#     # TODO customize to your needs\n",
    "#     text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "#     # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "#     return re.sub('[^a-zA-Z]', ' ', text.lower()).split()\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    return text.split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Naive Bayes Features #\n",
    "########################\n",
    "\n",
    "class Features_NB(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold):\n",
    "        super(Features_NB, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text, threshold)\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = [word for word, count in flattened_list_count.items() if count > threshold]\n",
    "\n",
    "        return flattened_list_count_filter\n",
    "\n",
    "    def replace_unknown_word_with_oov(self, tokenized_sentence):\n",
    "        \"\"\"Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        token\n",
    "        \"\"\"\n",
    "        updated_sentence = []\n",
    "        for word in tokenized_sentence:\n",
    "            if word not in self.vocabulary:\n",
    "                updated_sentence.append('OOV')\n",
    "            else:\n",
    "                updated_sentence.append(word)\n",
    "        return updated_sentence\n",
    "        \n",
    "    def get_features(self, tokenized, model):\n",
    "        \"\"\"Bag-of-words: return column vector of word counts, including OOV (Out-of-Vocabulary) token, if present.\n",
    "        Vector stores only non-zero values to improve performance\n",
    "        \"\"\"\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV\n",
    "        updated_text = model[\"Feature\"].replace_unknown_word_with_oov(tokenized)\n",
    "\n",
    "        bag_of_words = Counter(updated_text)\n",
    "        # Include OffsetFeature \"prob_mu\" to 1; which allows to include the probability of the label\n",
    "        # to the maximum likelihood estimation.\n",
    "\n",
    "        bag_of_words[\"prob_mu\"] = 1\n",
    "        return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "train_file = \"work/datasets/questions/train.txt\"\n",
    "pred_file = \"work/datasets/questions/val.test\"\n",
    "pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "model_file_name = \"nb.questions.model\"\n",
    "model_nb = NaiveBayes(model_file_name, threshold=0)\n",
    "\n",
    "# # odiya\n",
    "# train_file = \"work/datasets/odiya/train.txt\"\n",
    "# pred_file = \"work/datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "# model_file_name = \"logreg.odiya.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.000001, epochs=1000, threshold=10, max_features=1000)\n",
    "\n",
    "\n",
    "# 4dim\n",
    "# train_file = \"work/datasets/4dim/train.txt\"\n",
    "# pred_file = \"work/datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "# model_file_name = \"logreg.4dim.model\"\n",
    "# model_LR = LogisticRegression(model_file_name, learning_rate=0.2, epochs=200, threshold=1, max_features=100)\n",
    "\n",
    "\n",
    "# Products\n",
    "# train_file = \"work/datasets/products/train.txt\"\n",
    "# pred_file = \"work/datasets/products/val.test\"\n",
    "# pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "# model_file_name = \"nb.products.model\"\n",
    "\n",
    "# 4dim\n",
    "# train_file = \"work/datasets/4dim/train.txt\"\n",
    "# pred_file = \"work/datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "# model_file_name = \"nb.4dim.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_nb.classify(pred_file + \".txt\", model_nb.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 5.87%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
