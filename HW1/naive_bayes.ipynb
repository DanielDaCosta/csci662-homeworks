{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NaiveBayes is a generative classifier based on the Naive assumption that features are independent from each other\n",
    "P(w1, w2, ..., wn|y) = P(w1|y) P(w2|y) ... P(wn|y)\n",
    "Thus argmax_{y} (P(y|w1,w2, ... wn)) can be modeled as argmax_{y} P(w1|y) P(w2|y) ... P(wn|y) P(y) using Bayes Rule\n",
    "and P(w1, w2, ... ,wn) is constant with respect to argmax_{y} \n",
    "Please refer to lecture notes Chapter 4 for more details\n",
    "\"\"\"\n",
    "from work.Features import Features_NB\n",
    "from work.Model import *\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "class NaiveBayes(Model):\n",
    "\n",
    "    def __init__(self, model_file):\n",
    "        super(NaiveBayes, self).__init__(model_file)\n",
    "\n",
    "    def __count_frequency_word_label(self, sentences, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param sentences (list[list]): sentences tokenized\n",
    "        :param labels (list): list of labels\n",
    "        :return: count(c_j, w_i) refers to the count of word w_i in documents with label c_j\n",
    "                _sum_{i=1}^{V}{count(c_j, w_i)} sum of the counts of each word in our vocabulary in class c_j \n",
    "                 count(c_j) refers to the count of label c_j \n",
    "        \"\"\"\n",
    "        count_word_label = []\n",
    "        count_words_per_label = defaultdict(int)\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            for token in sentence:\n",
    "                count_word_label.append((token, label))\n",
    "                count_words_per_label[label] += 1\n",
    "            \n",
    "        # count_word_label = [(token, label) for sentence, label in zip(sentences, labels) for token in sentence]\n",
    "        count_label = Counter(labels)\n",
    "        return Counter(count_word_label), count_words_per_label, count_label\n",
    "    \n",
    "    def __compute_feature_weights(self, count_word_label, count_words_per_label, count_label, size_vocabulary, alpha=1):\n",
    "        \"\"\"\n",
    "        :param alpha (int): Hyperparemeter alpha for Laplace Smooting\n",
    "        \"\"\"\n",
    "        feature_weights = defaultdict(dict)\n",
    "        for word, label in count_word_label.keys():\n",
    "            # Maximum Likelihood Estimates\n",
    "            tmp = np.log((count_word_label[(word, label)] + alpha)/(size_vocabulary*alpha + count_words_per_label[label]))\n",
    "            feature_weights[label][word] = tmp\n",
    "\n",
    "        # Include Probability of each label: \n",
    "        total_documents = sum(count_label.values())\n",
    "        for label in count_label.keys():\n",
    "            probability_label_name = \"prob_mu\"\n",
    "            feature_weights[label][probability_label_name] = np.log(count_label[label]/total_documents)\n",
    "        return feature_weights\n",
    "\n",
    "    \n",
    "    def train(self, input_file):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "\n",
    "        # Instanciate Features_NB class:\n",
    "        #   - Create Vocabulary\n",
    "        features_naive_bayes = Features_NB(input_file)\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        # token\n",
    "        updated_text = []\n",
    "        labels = features_naive_bayes.labels\n",
    "        for sentence in features_naive_bayes.tokenized_text:\n",
    "            tmp = features_naive_bayes.replace_unknown_word_with_oov(sentence)\n",
    "            updated_text.append(tmp)            \n",
    "\n",
    "        # Compute Feature Weights\n",
    "        count_word_label, count_words_per_label, count_label = self.__count_frequency_word_label(updated_text, labels)\n",
    "        feature_weights = self.__compute_feature_weights(count_word_label, count_words_per_label, count_label, len(features_naive_bayes.vocabulary))\n",
    "\n",
    "        # Build Model\n",
    "        nb_model = {\n",
    "            \"feature_weights\": feature_weights,\n",
    "            \"Feature\": features_naive_bayes\n",
    "        }\n",
    "        \n",
    "        self.save_model(nb_model)\n",
    "    \n",
    "        \n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_file: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\" \n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_NB_class = model[\"Feature\"]\n",
    "\n",
    "        # Read Input File\n",
    "        tokenized_text = Feature_NB_class.read_inference_file(input_file)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # Choosing the label y which maximizes log p(x, y; μ, φ):\n",
    "        for sentence in tokenized_text:\n",
    "            sentence_features = Feature_NB_class.get_features(sentence, model)\n",
    "            # print(\"Sentence Characters: \", len(sentence_features.keys()))\n",
    "            class_predictions = defaultdict()\n",
    "\n",
    "            for label in Feature_NB_class.labelset:\n",
    "                # print(label)\n",
    "                feature_weights_y = feature_weights[label]\n",
    "                # Compute Inner Product: feature_weights*feature_vector\n",
    "                # print(len(sentence_features))\n",
    "                total_sum = 0 \n",
    "                # print(\"Size match: @@@@@ \", len(sentence_features.keys() & feature_weights_y.keys()))\n",
    "                for key in sentence_features.keys():\n",
    "                    if key in feature_weights_y.keys():\n",
    "                        # print(key)\n",
    "                        total_sum += sentence_features[key] * feature_weights_y[key]\n",
    "                # result = sum(sentence_features[key] * feature_weights_y[key] for key in sentence_features.keys() & feature_weights_y.keys())\n",
    "                # print(result)\n",
    "\n",
    "                class_predictions[label] = total_sum\n",
    "            # Find the class with the highest value\n",
    "            class_with_highest_value = max(class_predictions, key=lambda k: class_predictions[k])\n",
    "            preds.append(class_with_highest_value)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Basic feature extractor\n",
    "\"\"\"\n",
    "from operator import methodcaller\n",
    "import string \n",
    "\n",
    "def tokenize(text):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    return text.split()\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Naive Bayes Features #\n",
    "########################\n",
    "\n",
    "class Features_NB(Features):\n",
    "\n",
    "    def __init__(self, model_file):\n",
    "        super(Features_NB, self).__init__(model_file)\n",
    "        self.vocabulary = self.create_vocabulary(self.tokenized_text)\n",
    "\n",
    "    def read_inference_file(self, input_file):\n",
    "        \"\"\"Read inference file that is in the form: <text> i.e. a line\n",
    "        of text that does not contain a tab.\n",
    "        \"\"\"\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "\n",
    "        tokenized_text = [tokenize(text) for text in texts]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def create_vocabulary(self, tokenized_text, threshold=0):\n",
    "        \"\"\"Creat vocabulary from training set, considering only words that have an occurence > threshold.\n",
    "        \"\"\"\n",
    "        # Append everything together in a dictionary\n",
    "        flattened_list = [item for sublist in tokenized_text for item in sublist]\n",
    "        flattened_list_count = Counter(flattened_list)\n",
    "\n",
    "        # Considering only words that have an occurence > threshold.\n",
    "        flattened_list_count_filter = [word for word, count in flattened_list_count.items() if count > threshold]\n",
    "\n",
    "        return flattened_list_count_filter\n",
    "\n",
    "    def replace_unknown_word_with_oov(self, tokenized_sentence):\n",
    "        \"\"\"Replace words that are not in vocabulary with OOV (Out-of-Vocabulary)\n",
    "        token\n",
    "        \"\"\"\n",
    "        updated_sentence = []\n",
    "        for word in tokenized_sentence:\n",
    "            if word not in self.vocabulary:\n",
    "                updated_sentence.append('OOV')\n",
    "            else:\n",
    "                updated_sentence.append(word)\n",
    "        return updated_sentence\n",
    "        \n",
    "    def get_features(self, tokenized, model):\n",
    "        \"\"\"Bag-of-words: return column vector of word counts, including OOV (Out-of-Vocabulary) token, if present.\n",
    "        Vector stores only non-zero values to improve performance\n",
    "        \"\"\"\n",
    "\n",
    "        # Replace words that are not in vocabulary with OOV\n",
    "        updated_text = model[\"Feature\"].replace_unknown_word_with_oov(tokenized)\n",
    "\n",
    "        bag_of_words = Counter(updated_text)\n",
    "        # Include OffsetFeature \"prob_mu\" to 1; which allows to include the probability of the label\n",
    "        # to the maximum likelihood estimation.\n",
    "\n",
    "        bag_of_words[\"prob_mu\"] = 1\n",
    "        return bag_of_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = \"work/datasets/4dim.train.txt\"\n",
    "# pred_file = \"work/datasets/4dim.train.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim.train.txt\"\n",
    "# model_file_name = \"nb.4dim.model\"\n",
    "train_file = \"work/datasets/4dim.train.txt\"\n",
    "pred_file = \"work/datasets/4dim.test\"\n",
    "pred_true_labels = \"work/datasets/4dim.val.txt\"\n",
    "model_file_name = \"nb.4dim.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = Features_NB(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = NaiveBayes(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos.dec\n",
      "neg.dec\n",
      "pos.tru\n",
      "neg.tru\n",
      "pos.dec\n",
      "neg.dec\n",
      "pos.tru\n",
      "neg.tru\n",
      "pos.dec\n",
      "neg.dec\n",
      "pos.tru\n",
      "neg.tru\n",
      "pos.dec\n",
      "neg.dec\n",
      "pos.tru\n",
      "neg.tru\n",
      "pos.dec\n",
      "neg.dec\n",
      "pos.tru\n",
      "neg.tru\n"
     ]
    }
   ],
   "source": [
    "preds = model_nb.classify(pred_file + \".txt\", model_nb.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work/datasets/4dim.train.test'"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 2.56%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_labels.merge(pred, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you enter the lobby at the Millennium Kni...</td>\n",
       "      <td>pos.dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stayed at the Chicago Hilton for three nights ...</td>\n",
       "      <td>pos.tru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My stay at the Homewood Suites in downtown Chi...</td>\n",
       "      <td>neg.dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The James Chicago Hotel is located right in th...</td>\n",
       "      <td>pos.dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hotel Allegro in Chicago is understated lu...</td>\n",
       "      <td>pos.dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>Problems started when I booked the InterContin...</td>\n",
       "      <td>neg.dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>I recently had the opportunity to stay at the ...</td>\n",
       "      <td>neg.dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>Though grand and having a brand, this hotel se...</td>\n",
       "      <td>neg.dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>The Swissotel is totally understaffed and lack...</td>\n",
       "      <td>neg.tru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>I stayed at the Sheraton Chicago Hotel and Tow...</td>\n",
       "      <td>pos.dec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1560 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text true_label\n",
       "0     When you enter the lobby at the Millennium Kni...    pos.dec\n",
       "1     Stayed at the Chicago Hilton for three nights ...    pos.tru\n",
       "2     My stay at the Homewood Suites in downtown Chi...    neg.dec\n",
       "3     The James Chicago Hotel is located right in th...    pos.dec\n",
       "4     The Hotel Allegro in Chicago is understated lu...    pos.dec\n",
       "...                                                 ...        ...\n",
       "1555  Problems started when I booked the InterContin...    neg.dec\n",
       "1556  I recently had the opportunity to stay at the ...    neg.dec\n",
       "1557  Though grand and having a brand, this hotel se...    neg.dec\n",
       "1558  The Swissotel is totally understaffed and lack...    neg.tru\n",
       "1559  I stayed at the Sheraton Chicago Hotel and Tow...    pos.dec\n",
       "\n",
       "[1560 rows x 2 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
