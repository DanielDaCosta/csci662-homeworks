{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import methodcaller\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "def expand_contradictions(text):\n",
    "\n",
    "    contraction_mapping = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "\n",
    "    pattern = re.compile(r\"\\b(?:\" + \"|\".join(re.escape(contraction) for contraction in contraction_mapping.keys()) + r\")\\b\")\n",
    "    text = pattern.sub(lambda x: contraction_mapping[x.group()], text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_digits_and_words_digits(text):\n",
    "    # Define a regular expression pattern to match words containing digits\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    text_without_words_with_digits = re.sub(pattern, '', text)\n",
    "\n",
    "    return text_without_words_with_digits\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                  'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "                  'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "                  'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                  'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "                  'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "                  'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                  'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                  \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                  'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    text_clean = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_clean.append(word)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def tokenize(text, stop_words=True, split=True):\n",
    "    # TODO customize to your needs\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    # re.sub('[^a-zA-Z]', '', dataset['Text'][i])\n",
    "\n",
    "    # Text preprocessing techniques:\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2) Expand Contradictions\n",
    "    text = expand_contradictions(text)\n",
    "\n",
    "    # 3) Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "\n",
    "    # 4) Remove digits and words with digits\n",
    "    text = remove_digits_and_words_digits(text)\n",
    "\n",
    "    if split:\n",
    "        text = text.split()\n",
    "\n",
    "    # # 5) Remove Stop Words\n",
    "    # if stop_words:\n",
    "    # text = remove_stop_words(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, self.labels = map(list, zip(*data_split))\n",
    "\n",
    "        self.tokenized_text = [tokenize(text) for text in texts]\n",
    "\n",
    "        self.labelset = list(set(self.labels))\n",
    "\n",
    "    @classmethod \n",
    "    def get_features(cls, tokenized, model):\n",
    "        # TODO: implement this method by implementing different classes for different features \n",
    "        # Hint: try simple general lexical features first before moving to more resource intensive or dataset specific features \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Naive Bayes Features #\n",
    "########################\n",
    "\n",
    "class Features_NB_TF_IDF(Features):\n",
    "\n",
    "    def __init__(self, model_file, threshold):\n",
    "        super(Features_NB_TF_IDF, self).__init__(model_file)\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def get_features(self, input_data: pd.DataFrame):\n",
    "        \"\"\"Compute TF-IDF for input dataframe\n",
    "        \"\"\"\n",
    "        # loading CountVectorizer\n",
    "        tf_vectorizer = TfidfVectorizer(\n",
    "            min_df=self.threshold\n",
    "            )\n",
    "        \n",
    "        # Text Preprocessing\n",
    "        input_data.apply(lambda x: tokenize(x, split=False))\n",
    "\n",
    "        X_train_tf = tf_vectorizer.fit_transform(input_data)\n",
    "\n",
    "        self.tfidf_vectorizer = tf_vectorizer\n",
    "        return X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NaiveBayes is a generative classifier based on the Naive assumption that features are independent from each other\n",
    "P(w1, w2, ..., wn|y) = P(w1|y) P(w2|y) ... P(wn|y)\n",
    "Thus argmax_{y} (P(y|w1,w2, ... wn)) can be modeled as argmax_{y} P(w1|y) P(w2|y) ... P(wn|y) P(y) using Bayes Rule\n",
    "and P(w1, w2, ... ,wn) is constant with respect to argmax_{y} \n",
    "Please refer to lecture notes Chapter 4 for more details\n",
    "\"\"\"\n",
    "from work.Features import Features_NB\n",
    "from work.Model import *\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "class NaiveBayes_TF_IDF(Model):\n",
    "\n",
    "    def __init__(self, model_file, threshold=None):\n",
    "        super(NaiveBayes_TF_IDF, self).__init__(model_file)\n",
    "        self.threshold = threshold # Minimum number of occurences of word\n",
    "    \n",
    "    def train(self, input_file):\n",
    "        \"\"\"\n",
    "        This method is used to train your models and generated for a given input_file a trained model\n",
    "        :param input_file: path to training file with a text and a label per each line\n",
    "        :return: model: trained model \n",
    "        \"\"\"\n",
    "        # Read dataset\n",
    "\n",
    "        # train_dataset = pd.read_csv(input_file, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        data_split = map(methodcaller(\"rsplit\", \"\\t\", 1), data)\n",
    "        texts, labels = map(list, zip(*data_split))\n",
    "        train_dataset = pd.DataFrame({'text': texts, 'true_label': labels})\n",
    "\n",
    "        \n",
    "        X_train = train_dataset['text']\n",
    "        y_train = train_dataset['true_label']\n",
    "\n",
    "\n",
    "        # Instanciate Features_NB_TF_IDF class:\n",
    "        #   - Create TF-IDF Matrix\n",
    "        features_naive_bayes = Features_NB_TF_IDF(input_file, self.threshold)\n",
    "        \n",
    "        X_train_tf = features_naive_bayes.get_features(X_train)\n",
    "\n",
    "\n",
    "        # Train Model\n",
    "        naive_bayes = MultinomialNB()\n",
    "        naive_bayes.fit(X_train_tf, y_train)\n",
    "\n",
    "        # Build Model\n",
    "        nb_model = {\n",
    "            \"NaiveBayes\": naive_bayes,\n",
    "            \"feature_weights\": features_naive_bayes.tfidf_vectorizer,\n",
    "            \"Feature\": features_naive_bayes\n",
    "        }\n",
    "        \n",
    "        self.save_model(nb_model)\n",
    "    \n",
    "        \n",
    "\n",
    "    def classify(self, input_file, model):\n",
    "        \"\"\"\n",
    "        This method will be called by us for the validation stage and or you can call it for evaluating your code \n",
    "        on your own splits on top of the training sets seen to you\n",
    "        :param input_file: path to input file with a text per line without labels\n",
    "        :param model: the pretrained model\n",
    "        :return: predictions list\n",
    "        \"\"\" \n",
    "        feature_weights = model[\"feature_weights\"]\n",
    "        Feature_NB_TF_IDF_class = model[\"Feature\"]\n",
    "        NaiveBayes = model[\"NaiveBayes\"]\n",
    "\n",
    "        # Read dataset\n",
    "        # test_dataset = pd.read_csv(input_file, sep='\\t', header=None, names=['text'])\n",
    "\n",
    "        with open(input_file) as file:\n",
    "            data = file.read().splitlines()\n",
    "\n",
    "        texts = data\n",
    "        test_dataset = pd.DataFrame({'text': data})\n",
    "        \n",
    "        X_test = test_dataset['text'].values.astype('U')\n",
    "\n",
    "        # return X_test\n",
    "\n",
    "        X_test_tfidf = feature_weights.transform(X_test)\n",
    "\n",
    "        # Make Prediction\n",
    "        y_pred = NaiveBayes.predict(X_test_tfidf)\n",
    "\n",
    "        # Convert to string for saving\n",
    "        preds = [str(num) for num in y_pred]\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions\n",
    "# train_file = \"work/datasets/questions/train.txt\"\n",
    "# pred_file = \"work/datasets/questions/val.test\"\n",
    "# pred_true_labels = \"work/datasets/questions/val.txt\"\n",
    "# model_file_name = \"naivebayes_tfidf.questions.model\"\n",
    "# model_nb = NaiveBayes_TF_IDF(model_file_name, threshold=9)\n",
    "\n",
    "# # odiya\n",
    "# train_file = \"work/datasets/odiya/train.txt\"\n",
    "# pred_file = \"work/datasets/odiya/val.test\"\n",
    "# pred_true_labels = \"work/datasets/odiya/val.txt\"\n",
    "# model_file_name = \"naivebayes_tfidf.odiya.model\"\n",
    "# model_nb = NaiveBayes_TF_IDF(model_file_name, threshold=2)\n",
    "\n",
    "#Products\n",
    "train_file = \"work/datasets/products/train.txt\"\n",
    "pred_file = \"work/datasets/products/val.test\"\n",
    "pred_true_labels = \"work/datasets/products/val.txt\"\n",
    "model_file_name = \"naivebayes_tfidf.products.model\"\n",
    "model_nb = NaiveBayes_TF_IDF(model_file_name, threshold=2)\n",
    "\n",
    "# # 4dim\n",
    "# train_file = \"work/datasets/4dim/train.txt\"\n",
    "# pred_file = \"work/datasets/4dim/val.test\"\n",
    "# pred_true_labels = \"work/datasets/4dim/val.txt\"\n",
    "# model_file_name = \"naivebayes_tfidf.4dim.model\"\n",
    "# model_nb = NaiveBayes_TF_IDF(model_file_name, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_nb.classify(pred_file + \".txt\", model_nb.load_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the predictions: one label prediction per line\n",
    "with open(pred_file + \".pred.txt\", \"w\") as file:\n",
    "    for pred in preds:\n",
    "        file.write(pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv(pred_true_labels, sep='\\t', header=None, names=['text', 'true_label'])\n",
    "pred_dataset = pd.read_csv(pred_file + \".pred.txt\", sep='\\t', header=None, names=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.35%\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns have the same name; adjust as needed\n",
    "column_name = 'true_label'  # Change to the actual column name\n",
    "pred_column_name = 'pred'  # Change to the actual predicted column name\n",
    "\n",
    "# Merge the two DataFrames on a common index or key if available\n",
    "merged_df = true_dataset.merge(pred_dataset, left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the accuracy by comparing the two columns\n",
    "accuracy = (merged_df[column_name] == merged_df[pred_column_name]).mean()\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
